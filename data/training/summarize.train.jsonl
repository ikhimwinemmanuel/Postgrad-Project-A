{"prompt": "Title: VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction\n\nIntroduction API Agents represent a paradigm shift in intelli- gent interaction by combining large language mod- els‚Äô cognitive capabilities with external tools‚Äô ex- ecution capabilities (Durante et al., 2024; Zhang et al., 2025; Qu et al., 2025, inter alia). These agents distinguish themselves through autonomous decision-making, sophisticated reasoning abilities, and seamless tool interaction, transcending tradi- *Equal contribution. 1https://github.com/OpenMOSS/VehicleWorld User: The weather is hot, turn on the air conditioner, then play my music collection. Device Music Seat Airconditioner Door Radio Steeringwheel Direct Function Call Assistant: I will first check what functional modules the system has. search_module() User: {\"music\", \"airconditioner\"...} Assistant: I will check what APIs are available for the music and airconditioner module. search_api([ \"music\", \"airconditioner\" ]) User: def Environment.to_dict(): \"\"\" Get temperature information\"\"\" def airconditioner.switch(switch:bool, temp: int): \"\"\" Turn on or off the in-car air conditioner\"\"\" def music.view_favorite_play_list(): \"\"\"View my favorite music list\"\"\"\" def music.add_current_play_List(music: Str): \"\"\"Add and play the current playlist\"\"\" ... Assistant: I will first check the current ambient temperature. Environment.to_dict() State-based Function Call (ours) sound_channel: None temperature: 27 is_playing: False current_play_list: [] temperature: 27 is_on: False Assistant: I will observe the environment and then generate state transition code to solve this problem. Environment.sound_channel = music Environment.temperature = 24 airconditioner.is_on = True Environment.temperature = 24 music.is_playing = True music.current_play_list = music.favorite.play_list Assistant: Now I will call the API to solve your problem. airconditioner.switch(True, 24) play_list = music.view_favorite_play_list() music.add_current_play_List(play_list[0]) User: temperature: 27 ... sound_channel: music temperature: 24 is_playing: True current_play_list: [\"music01\"...] temperature: 24 is_on: False Figure 1: Direct Function Call (FC) versus State-based Function Call (SFC) tional dialogue systems‚Äô limitations (Xi et al., 2025; Jin et al., 2024; Cao et al., 2024). Intelligent vehicle cockpits present a uniquely challenging domain for API Agents, integrating numerous tightly-coupled subsystems from enter- tainment and navigation to vehicle diagnostics and environmental controls. Within this environment, agents must orchestrate diverse systems while al- lowing drivers to maintain focus on the road (Ma et al., 2024a). Despite their importance to modern vehicle systems, the field lacks a comprehensive evaluation framework for these cockpit agents, pre- arXiv:2509.06736v1 [cs.AI] 8 Sep 2025 venting systematic assessment of their performance across implementations (Khiabani et al., 2025). Figure 1 illustrates the challenges with a com- mon request: ‚ÄúThe weather is hot, turn on the air conditioner, then play my music collection.‚Äù This seemingly simple instruction requires coordination across multiple subsystems. Traditional Function Call (FC) approach operates statelessly, sequen- tially exploring available modules and APIs, ne- cessitating multiple exploratory calls to build envi- ronmental awareness before execution. As noted by Guo et al. (2024), this approach becomes prob- lematic when API calls fail, as agents struggle to recover without a macroscopic understanding of the global state. Additionally, agents can only dis- cern execution results through limited API return information, which may lead to incorrect conclu- sions about task success or failure. To address these limitations, we developed Vehi- cleWorld, a virtual intelligent cockpit environment supporting 30 modules, 250 APIs, and 680 prop- erties. All APIs are executable code implementa- tions, with each corresponding to a module instance method implemented through attribute state modifi- cations. Based on our comprehensive environment construction, we discovered that state information significantly enhances agent call accuracy. We propose State-based Function Call (SFC), which explicitly constructs state transition processes by maintaining awareness of the system‚Äôs current state and directly implementing necessary transitions to achieve target states. Our contributions include: ‚Ä¢ The first comprehensive environment for the automotive domain that provides real-time state information during model execution. ‚Ä¢ A novel State-based Function Call (SFC) ap- proach specifically engineered for our Vehi- cleWorld environment. ‚Ä¢ Experiments demonstrate that SFC exhibits significant improvements in execution accu- racy and latency reduction compared to FC. 2 Related Work Tool-Utility Agent Recent studies have demon- strated that integrating tool-using capabilities sig- nificantly enhances the adaptability and effective- ness of agents in complex environments (Mialon et al., 2023; Schick et al., 2023). Tools expa", "response": "Intelligent vehicle cockpits present unique challenges for API Agents, requiring coordination across tightly-coupled subsystems that exceed typical task environments' complexity. Traditional Function Calling (FC) approaches operate statelessly, requiring multiple exploratory calls to build environmental awareness before execution, leading to inefficiency and limited error recovery. We introduce VehicleWorld, the first comprehensive environment for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties with fully executable implementations that provide real-time state information during agent execution. This environment enables precise evaluation of vehicle agent behaviors across diverse, challenging scenarios. Through systematic analysis, we discovered that direct state prediction outperforms function calling for environmental control. Building on this insight, we propose State-based Function Call (SFC), a novel approach that maintains explicit system state awareness and implements direct state transitions to achieve target conditions. Experimental results demonstrate that SFC significantly outperforms traditional FC approaches, achieving superior execution accuracy and reduced latency. We have made all implementation code publicly available on Github https://github.com/OpenMOSS/VehicleWorld."}
{"prompt": "Title: Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation\n\nIntroduction Function representation and approximation are fundamental problems in Machine Learning and scientific computing, underpinning a wide range of applications such as physical simulation, data modeling, and predictive analysis. A foundational result in this area is the Universal Approximation Theorem, which established that artificial neural networks (ANNs) are universal approximators, capable of approximating any continuous function over compact domains [15]. The theoretical study of ANNs as universal approximators has since evolved significantly, with further work demonstrating their capacity to approximate continuous and smooth functions effectively [10, 31, 16, 27]. More recent research ‚àóCorresponding author. arXiv:2509.06694v1 [cs.LG] 8 Sep 2025 BNNs and LWPE Loss: A Green Geometric and Topological Framework for Function Approximation has explored optimal approximation properties of sparsely connected deep networks [2] and the expressive power of ReLU-based architectures for function approximation [18, 23]. Although deep neural networks (DNNs) are powerful and are indeed known to be universal approximators, they typically require highly overparameterized architectures, large datasets, and significant computational resources for training. This often results in architectures (models) that are opaque, costly and energy-intensive. Such limitations are particularly problematic in settings with limited resources, such as embedded systems and edge computing, where lightweight and interpretable models are preferable. The growing tension between performance and interpretability has motivated the development of more structured, explainable, and computationally efficient models. This aligns with the emerging paradigm of Green Artificial Intelligence (Green AI) [3], which advocates for energy-efficient and sustainable machine learning systems. In this context, geometric and topological tools have gained increasing attention for their ability to structure information and capture global patterns that go beyond local pointwise accuracy. On the geometric side, barycentric coordinates offer a classical yet underutilized approach for function approximation. These coordinates allow points to be expressed as convex combinations of the vertices of a simplex, enabling a natural mechanism for piecewise linear interpolation. Historically applied in computer graphics and geometric design [30], barycentric interpolation has recently been extended to function approximation [22]. While [22] employs barycentric subdivisions, which lead to an exponential growth in the size of the triangulations, our approach does not rely on such subdivisions. These approaches demonstrate that fixed geometric structures can yield accurate and efficient representations, especially for continuous piecewise linear functions. Nested barycentric coordinate systems (NBCS) have been proposed to represent complex function spaces in a composable and interpretable manner [12]. Similarly, variational barycentric interpolation has been explored for mesh-based learning and shape optimization [7]. Despite their potential, geometric techniques, like barycentric coordinates, are rarely integrated into trainable models for function approximation, despite remaining decoupled from neural network architectures. On the topological side, topological methods have also been integrated into analysis and machine learning. Persistent homology, a core method in topological data analysis (TDA), and its associated summaries, like persistent entropy, have proven valuable for extracting global, deformation-invariant features from data. Initially used for feature extraction, TDA provides stable and noise-robust descriptors based on the birth and death of topological features such as connected components, loops, and voids, that can be used as input data for machine learning models across diverse tasks, such as characterizing idiotypic immune networks [25], analyzing similarities in piecewise linear functions and time series [26], and defining safety regions for robotic navigation [29]. More recently, the field of Topological Deep Learning (TDL) has emerged, incorporating persistent homology into the training of neural networks through differentiable topological layers and loss functions [33]. For instance, persistent homology and associated descriptors have been used to inject shape-aware constraints into neural networks, yielding improvements in tasks like anatomical segmentation [19], point cloud reconstruction [14], shape classification [24], and time series forecasting [17]. Recent advances in differentiable topological optimization [4, 32] have further improved the trainability and robustness of topological loss functions, making them viable components for neural network training pipelines. These topological losses aim to preserve global structural properties in learned functions, which classical loss functions like MSE tend to overlook. However, challenges persist,", "response": "While it is well-established that artificial neural networks are \\emph{universal approximators} for continuous functions on compact domains, many modern approaches rely on deep or overparameterized architectures that incur high computational costs. In this paper, a new type of \\emph{small shallow} neural network, called the \\emph{Barycentric Neural Network} ($\\BNN$), is proposed, which leverages a fixed set of \\emph{base points} and their \\emph{barycentric coordinates} to define both its structure and its parameters. We demonstrate that our $\\BNN$ enables the exact representation of \\emph{continuous piecewise linear functions} ($\\CPLF$s), ensuring strict continuity across segments. Since any continuous function over a compact domain can be approximated arbitrarily well by $\\CPLF$s, the $\\BNN$ naturally emerges as a flexible and interpretable tool for \\emph{function approximation}. Beyond the use of this representation, the main contribution of the paper is the introduction of a new variant of \\emph{persistent entropy}, a topological feature that is stable and scale invariant, called the \\emph{length-weighted persistent entropy} ($\\LWPE$), which is weighted by the lifetime of topological features. Our framework, which combines the $\\BNN$ with a loss function based on our $\\LWPE$, aims to provide flexible and geometrically interpretable approximations of nonlinear continuous functions in resource-constrained settings, such as those with limited base points for $\\BNN$ design and few training epochs. Instead of optimizing internal weights, our approach directly \\emph{optimizes the base points that define the $\\BNN$}. Experimental results show that our approach achieves \\emph{superior and faster approximation performance} compared to classical loss functions such as MSE, RMSE, MAE, and log-cosh."}
{"prompt": "Title: floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL\n\n1. Introduction Velocity ùíóùúΩùíï, ùíõ ùíî, ùíÇ) Numerical integration of the flow t ùëß‡¨¥‚àºùëàùëõùëñùëì[ùëô, ùë¢] ùëß‡ØÑ‚àº Œ¥(ùëÑùë†, ùëé) ‚Ä¶ ùë† ùëé ùë° ùëß‡Øß‡¨æ‡¨µ‚àà‚Ñù Categorical representation of ùëß‡Øß Q-function Q-value sample ùëÑùë†, ùëé, ùëß‡¨¥= ùëß‡ØÑ K rounds of integration ‚Ñù ‚Ñù ‚Ñù ‚Ñù Fourier Embed Figure 1: floq architecture. Our approach models the Q- function via a velocity field in a flow-matching generative model. Over multiple calls, this velocity field converts a randomly sam- pled input ùëß(0) into a sample from the Dirac-delta distribution centered at the Q-value. We prescribe how this sample can be trained via a flow-matching loss. Doing this enables us to scale computation by running numerical integration, with multiple calls to the velocity field. To train floq, we utilize a categorical representation of input ùëßùë°[13]. A key principle that has enabled effective model scaling in various areas of machine learning is the use of iterative computation: producing com- plex output functions by composing a sequence of small and simple operations. For example, transformers [72] are able to generate coherent text and long reasoning chains by predicting the next token and by composing multiple atomic reasoning strategies [19] respectively. Similarly, diffusion models [29, 68] and flow-matching tech- niques [2, 47] learn to synthesize realistic images by progressively denoising small perturbations. The wide application of these models suggests that iterative computation is a powerful tool for modeling complex functions with deep networks. Motivated by these empirical results, in this pa- per, we ask: can iterative computation also im- prove value estimation in reinforcement learn- ing (RL)? Specifically, we are interested in im- proving the estimation of the Q-value function. While Q-functions map state-action inputs to a scalar value, they are known to be highly complex and difficult to fit accurately (e.g., [10]). Standard Corresponding author(s): bbagrawa@andrew.cmu.edu arXiv:2509.06863v1 [cs.LG] 8 Sep 2025 floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL temporal-difference (TD) learning used to train Q-functions struggles to leverage capacity of deep net- works [5, 21, 38, 39, 50], often resulting in poor generalization. These problems are further exacerbated in the offline RL problem setting [36, 45], where we must learn entirely from static datasets. This motivates exploring architectures that spend compute iteratively to estimate value functions, potentially yielding more accurate Q-values, and as a result, better downstream policies. A natural starting point to use iterative computation for value-based RL is to represent the Q-function with ResNets [27], where stacking residual blocks iteratively computes the Q-value. Recent work in TD-learning has obtained modest gains with ResNets [13, 40, 41, 53], but these methods deliberately need to operate with normalization and regularizers to enable stable training [5, 40, 42, 43, 53]. Despite improvements, these approaches lack one ingredient that makes iterative computation effective in transformers or diffusion models: supervision at every step of the iterative computation process. Just as next-token prediction supervises each generated token, and diffusion supervises each denoising step, we hypothesize that stepwise loss supervision applied to TD learning might lead to improvements. With this observation, to effectively leverage iterative computation with dense supervision, we design a novel architecture for representing Q-functions. Instead of using a single monolithic network, we represent the Q-function as a velocity field over a scalar value (Figure 1). Our approach, floq (flow-matching Q- functions) samples a scalar uniformly distributed noise variable and maps it to the Q-value by numerically integrating the predictions of this velocity field. We train this velocity field with a linear flow-matching objective [2, 47] adapted from generative modeling. Unlike standard flow-matching, which supervises the velocity with stationary targets, we train the velocity to match the evolving TD-targets. At each step, we minimize the deviation between the current Q-value estimate and the corresponding TD-target. We introduce several design choices that stabilize training and help the architecture scale capacity effectively. First, we appropriately set the support of the initial noise to be as wide as possible. Second, we use a categorical input representation to handle the non-stationarity of inputs during training. Finally, we apply a tailored Fourier-basis embedding of time so that the velocity predictions vary meaningfully across integration steps. These design choices are crucial for learning a good floq critic. We use floq to represent the Q-function for a number of complex RL [36, 45] tasks from the OGBench [57] benchmark, previously studied by Park et al. [59]. In aggregate, we find that floq outperforms offline RL algorithms that represent Q-functions using a monolithic network by nea", "response": "A hallmark of modern large-scale machine learning techniques is the use of training objectives that provide dense supervision to intermediate computations, such as teacher forcing the next token in language models or denoising step-by-step in diffusion models. This enables models to learn complex functions in a generalizable manner. Motivated by this observation, we investigate the benefits of iterative computation for temporal difference (TD) methods in reinforcement learning (RL). Typically they represent value functions in a monolithic fashion, without iterative compute. We introduce floq (flow-matching Q-functions), an approach that parameterizes the Q-function using a velocity field and trains it using techniques from flow-matching, typically used in generative modeling. This velocity field underneath the flow is trained using a TD-learning objective, which bootstraps from values produced by a target velocity field, computed by running multiple steps of numerical integration. Crucially, floq allows for more fine-grained control and scaling of the Q-function capacity than monolithic architectures, by appropriately setting the number of integration steps. Across a suite of challenging offline RL benchmarks and online fine-tuning tasks, floq improves performance by nearly 1.8x. floq scales capacity far better than standard TD-learning architectures, highlighting the potential of iterative computation for value learning."}
{"prompt": "Title: AI for Scientific Discovery is a Social Problem\n\nIntroduction Artificial intelligence promises to accelerate scientific discovery, yet its benefits remain concentrated rather than democratized. While breakthrough applications like AlphaFold [48] and the Materials Project [43] demonstrate AI‚Äôs transformative potential, current development remains largely inaccessi- ble to most researchers, requiring computational resources, specialized datasets, and interdisciplinary expertise that few institutions can match [70]. This concentration limits research diversity and undermines the collaborative foundations essential for robust scientific progress [25]. The challenge extends beyond resource scarcity to a fundamental mismatch between AI paradigms and scientific needs. Most AI development optimizes for predictive accuracy on large, homogeneous datasets, while science demands understanding from high-dimensional, low-sample-size data with complex relationships [9]. When AI tools prioritize automation over augmentation, they risk short- circuiting the human understanding essential for scientific progress [53], creating a production- progress paradox where increased output does not translate to deeper insight [49]. This paper examines four critical barriers preventing AI democratization in science: community dys- function that undermines collaboration, misaligned research priorities targeting narrow applications over upstream computational bottlenecks, data fragmentation due to incompatible standards [37], and infrastructure inequities concentrating power within privileged institutions [4]. While these manifest as technical challenges, their root causes are fundamentally social and institutional: including under- valuing data contributions, educational gaps preventing cross-disciplinary engagement, and absent community consensus on shared priorities. We then propose corresponding solutions: strengthen- ing cross-disciplinary collaboration and education, structuring upstream challenges through shared benchmarks, standardizing scientific data practices, and building sustainable community-owned infrastructure. Our analysis demonstrates that democratizing AI for science requires treating it as a collective social project where equitable participation is a prerequisite for technical progress. Preprint. arXiv:2509.06580v1 [cs.LG] 8 Sep 2025 2 Barriers to Scientific Achievement 2.1 Barrier One: Community Dysfunction The dysfunction within the AI for science community manifests through harmful narratives, mis- aligned incentives, and communication breakdowns that prevent effective collaboration. These reflect deeper cultural issues about how the community values different types of contributions, frames the purpose of AI in science, and structures interactions between disciplines. Figure 1: Figure from [83] contrasts the true Newto- nian forces (left) and the predicted forces (right) learned by a transformer-based foundation model with high ac- curacy in predicting planetary trajectories. Though it performs well on the task it was fine-tuned for, it has not learned an inductive bias toward true Newtonian me- chanics. General purpose models do not necessarily aid in specific scientific understanding. The AI Scientist Myth It has become popular to suggest that AGI is imminent and, that once it arrives, will solve the vast majority of our scientific problems [41]. Many efforts in the ma- chine learning for science community have also focused on the development of so-called ‚ÄúAI sci- entists‚Äù, which have had results ranging from disappointing to actively harmful [26]. While these systems may eventually provide valuable support as co-pilot tools, particularly in retriev- ing accurate information and assisting with rou- tine tasks, they are often framed as a future re- placement for the human scientist. This narrative is ultimately counterproductive to genuine scientific progress [14]. First, there is little evidence that AGI is imminent, let alone consensus on what the term means, and wait- ing for it risks delaying advances that could be achieved with current methods. Second, it de- values the expertise and contributions of human scientists, whose knowledge and creativity remain essential to discovery [44]. Third, it oversimplifies the inherent complexity of scientific practice, which depends not only on predictive accuracy but also on careful validation, contextualization, and theoretical integration. Finally, it obscures the central purpose of science, which is not only to produce solutions but also to cultivate human understanding [23]. Rewarding Long-Lasting Impact Another counterproductive tendency within the current research ecosystem is the undervaluing of contributions to data and infrastructure in hiring, publicity, and tenure evaluations [33]. High-quality datasets, particularly those with well-curated metadata, often have far greater long-term impact than individual model contributions. Most models are rapidly superseded by marginally improved variants, and their infl", "response": "Artificial intelligence promises to accelerate scientific discovery, yet its benefits remain unevenly distributed. While technical obstacles such as scarce data, fragmented standards, and unequal access to computation are significant, we argue that the primary barriers are social and institutional. Narratives that defer progress to speculative \"AI scientists,\" the undervaluing of data and infrastructure contributions, misaligned incentives, and gaps between domain experts and machine learning researchers all constrain impact. We highlight four interconnected challenges: community dysfunction, research priorities misaligned with upstream needs, data fragmentation, and infrastructure inequities. We argue that their roots lie in cultural and organizational practices. Addressing them requires not only technical innovation but also intentional community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress."}
{"prompt": "Title: Disentangling Interaction and Bias Effects in Opinion Dynamics of Large Language Models\n\nIntroduction Large Language Models (LLMs) demonstrate impressive ca- pabilities in mimicking human behavior, generating text that is increasingly indistinguishable from human-produced lan- guage [1‚Äì3]. Because of their ability to accurately portray syntax, semantics and contextual relationships in language, LLMs have recently been adopted as powerful tools for agent- based simulations [4‚Äì6], especially of social processes such as opinion dynamics [7‚Äì12]. Traditional agent-based models for opinion dynamics typically represent opinions as discrete states, continuous variables or higher dimensional vectors, with explicit mathematical update rules governing the interaction between agents [13‚Äì16]. While often highly interpretable in nature, these simplified models fail to capture the rich, nuanced, and context-sensitive nature of real human communication [17, 18]. In contrast, LLMs directly utilize natural language, enabling more detailed and context- sensitive interactions [19, 20]. However, this greater realism comes at the cost of reduced interpretability. Since the factors driving opinion change are encoded in the model‚Äôs training weights rather than in explicit rules, they are less transparent. Recent studies have shown that the factors behind LLM opin- ion dynamics include certain biases‚Äîsystematic tendencies inherited from their training data‚Äîthat can drive conversa- tion outcomes away from the outcomes otherwise expected from human discussants [21‚Äì24]. Particularly, LLMs have been shown to exhibit what we term a topic bias, which makes them converge towards consensus reflecting opinions about the discussion topic embedded within their training data [7]. Additionally, an acquiescence or agreement bias has previously been observed for some LLMs, which makes agents more likely to answer ‚Äúagree‚Äù to a question irrespective of its content [25]. Furthermore, LLMs have in some instances been observed to express an anchoring bias [26], making them overvalue the first opinion expressed in a discussion. While some of these biases can be related to human biases, others are attributed only to LLM-specific behavior, potentially masking the gen- uine interaction of different agents. This confusion motivates the need for a systematic framework to disentangle how much and how each of these factors contributes to the overall opinion dynamics observed in discussions between LLM agents. A framework that allows to quantify how the individual fac- tors contribute to the overall opinion shifts can be found in Bayesian modeling. Bayesian models have in previous work been used to model opinion changes and decision making in human subjects [27‚Äì29], allowing for capturing the effect and relevance of different factors in intuitive terms and providing tools to compare between models. On a broader level, Bayesian inference has been proposed as a core computational principle of human cognition, providing a principled mechanism for up- dating beliefs under uncertainty [30]. These properties make Bayesian modeling a suitable framework for constructing and evaluating factors of the influence-response function of LLM agents, that describes mathematically how an agent shifts their belief after one round of discussing with an other agent. In this work, we introduce a Bayesian framework for modeling how the observed opinion shifts in discussions between LLM agents are influenced by the interaction with the interlocutor and by different bias effects. We find that different models are dominated by different biases, with a more capable LLM putting greater emphasis on the interaction with the other agent. Furthermore, we propose to quantify the opinion of an LLM agent on a two-dimensional scale, with the expectation value of the LLM‚Äôs response distribution to a query representing its stance on a topic, and the Shannon entropy quantifying its uncertainty in this position. This uncertainty is found to be predictive of the variance of the subsequent opinion shift. To address the issue of biases dominating the opinion dynamics in LLMs, we further explored the effectiveness of fine-tuning as a means to reinforce the persistence of the prompted initial opinion of an agent, thereby enhancing realism of simulated conversations. We demonstrate that fine-tuning LLMs to align with specific initial opinions shifts their bias towards the fine- tuned opinion and shows a trend towards a stronger influence of interaction in the overall dynamics. By explicitly modeling the factors governing LLM interactions, our work offers human-interpretable metrics suitable for quan- tifying bias strength between different LLMs or influence of fine-tuning. The flexible nature of the Bayesian framework allows to easily construct models including different influence factors, and provides rigorous methods for cross-study com- parison. In future work, the Bayesian models may serve as computationally efficient proxies for simulation of artificial agents in large social networks. 2", "response": "Large Language Models are increasingly used to simulate human opinion dynamics, yet the effect of genuine interaction is often obscured by systematic biases. We present a Bayesian framework to disentangle and quantify three such biases: (i) a topic bias toward prior opinions in the training data; (ii) an agreement bias favoring agreement irrespective of the question; and (iii) an anchoring bias toward the initiating agent's stance. Applying this framework to multi-step dialogues reveals that opinion trajectories tend to quickly converge to a shared attractor, with the influence of the interaction fading over time, and the impact of biases differing between LLMs. In addition, we fine-tune an LLM on different sets of strongly opinionated statements (incl. misinformation) and demonstrate that the opinion attractor shifts correspondingly. Exposing stark differences between LLMs and providing quantitative tools to compare them to human subjects in the future, our approach highlights both chances and pitfalls in using LLMs as proxies for human behavior."}
{"prompt": "Title: From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers\n\nIntroduction Generative AI systems are becoming always more capable, demonstrating remarkable performance on a wide range of tasks, across diverse data modalities. Yet, transformer models can at times confidently misstate facts, invent details in generated content, and engage in confabulated reasoning. These model behaviors occur even when accurate information for the task is abundantly present in the models‚Äô training data. These plausible-seeming yet incorrect outputs are often referred to as ‚Äúhallucinations‚Äù, and range from simple factual mistakes to potentially harmful and deceptive or mis-aligned behaviors [1][2][3][4][5]. While it may be trivial to quantify the error rate of a transformer model using various performance benchmarks, working towards a truly mechanistic understanding of model hallucinations, errors, and inherent biases will be unavoidable as we entrust AI systems with increasingly sensitive and consequential real-world tasks. For the goal of dissecting the units of meaning on which transformer models internally operate, sparse autoencoders (SAEs) have imposed themselves as a widely adopted tool in the mechanistic interpretability community [6][7]. This class of autoencoders now provides the means for disentan- Preprint. Under review. arXiv:2509.06938v1 [cs.LG] 8 Sep 2025 gling the opaque visceral workings of large language models (LLMs) into human understandable components. Such SAEs learn a mapping function from intermediate transformer model activations to a sparse overcomplete representational space, which has an interpretable basis by nature, that can be used to faithfully reconstruct the latent activations. Recent work has shown that human interpretable features of consequence can be identified in transformer models at scale, from toy models to commercial-scale LLMs using SAEs [8]. Much of this interpretability research is rooted in the ‚Äúlinear representation hypothesis‚Äù. It posits that semantic concepts are represented as single-dimensional linear directions in a representation space derived from transformer layer activations [9][10]. While there is some evidence that certain features may be inherently multi-dimensional in SAE embedding activations [11], there is a growing body of research underscoring the validity and utility of these linearly decomposable single-dimensional features [12][13][14][10]. However, a highly important open question remains: to what extent do these learned representations in transformer models reflect properties of the input data itself, as opposed to inherent biases or structural priors that emerged during model training? How do these internal representations behave when the transformer model is facing uncertainty due to confusing, ambiguous, or noisy inputs? Using SAEs as a tool to probe the emergent conceptual landscape of transformer internals, we show that pre-trained transformers impose coherent, steerable conceptual structure even on pure noise or randomly shuffled inputs, with the repertoire of activated concepts expanding as input structure degrades, across both images and text. Crucially, we show that patterns of concept activation elicited by the input prompt reliably predict hallucinations in the transformer‚Äôs generated output, providing a practical signal for anticipating unfaithful generations and directly linking high-level inferred concepts to model errors. Taken together, our exploratory and interventional experiments yield three key contributions: 1. First, common pre-trained transformer models exhibit a strong form of input-insensitive inductive bias: these systems tend to impose semantic structure on inputs, tying them into learned conceptual webs, even if the model inputs are ambiguous or lack any coherent meaning. 2. Second, this skewing as information trickles through transformer processing layers exac- erbates as input uncertainty increases: the more randomness we experimentally introduce in the input observations, the more persistently the transformer model is drawn towards operating on semantic units that align with familiar internal model representations. We show a demonstrable expansion of semantic concepts that are triggered, localized especially to the middle layers of the transformer model, as the coherence of input information degrades. 3. Third, the constellation of concept activations in a transformer model‚Äôs intermediate process- ing representations can be used to reliably predict the tendency of hallucinated or unfaithful output. This insight suggests an automatically measurable link between spurious internal feature recruitment of the input and the fidelity of the output produced by these increasingly popular deep learning systems. 2 Preliminaries In this section, we introduce our experimental framework to probe the emergence of semantic structure in transformer models, from layer to layer, by means of SAE training and concept evaluation. To address our research aims, we employ SAEs trained on transformer activations", "response": "As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk."}
{"prompt": "Title: Topological Regularization for Force Prediction in Active Particle Suspension with EGNN and Persistent Homology\n\nIntroduction Self propelled active particle suspensions ‚Äî which encompasses, e.g., bacteria, algae or synthetic colloidal rotors that eat power rather than momentum [1][2][3]‚Äîdisplay a wide variety of non-equilibrium phenomena that emerge from a coupling among an individual's propulsion, inter-particle interactions and hydrodynamics. Although continuum models built on both kinetic theory and coarse grained hydrodynamics have been used to describe large scale instabilities and pattern formation in bacterial turbulence [4][5][6], they typically have insufficient resolution to resolve pairwise forces [24], as well as the emergent network topology on the particle scale [7][8][9]. They are often used for coupling between fluid and particle computations in high Reynolds number flows, which requires unstructured meshes when doing so [7][8][9]; however extracting the forces directly from these simulations as system size grows quickly becomes computationally infeasible. Recent advances in machine learning may allow us to infer effective interaction rules for active matter. Other geometric deep learning frameworks, such as graph neural networks (GNNs) [13][12][34], have been developed in order to learn relational inductive biases from material physical systems, and group equivariant architectures enforce the learned predictions to be consistent w.r.t. Euclidean symmetries [10][11][33] in parallel to Physics-informed neural networks (PINNs) [14‚Äì17] incorporate governing equations into the loss function to enforce that the learned solution satisfies the relevant physical consistency terms (for example, Navier‚ÄìStokes residuals). Yet neither standard GNNs nor PINNs enforce topological properties of the inferred interaction networks leading to spurious loops, disconnected components and artefacts appear in the resulting force graph. Topological Data Analysis (TDA) gives a theoretical basis for quantifying the shape of data and using it to enforce qualitative constraints in networks. Persistent Homology can be seen as a multiscale summary of topological features over all connected components and holes by tracking the birth and death points of connected components or loops under scale sweep [18][19][20][21]. Rapid computation of persistence diagrams for large datasets is now accessible by a number of software libraries [22,23]. Hypotheses were formulated by researchers and Persistence-based loss terms have been adapted to neural networks to, for example, disallow unwanted structures in image segmentation [29] or to characterize protein compressibility in biomolecular systems [24]. Furthermore, TDA has been used to discover coherent structures in fluid and shear flows [27, 28]. TDA allows for the discovery of coherent structures in turbulent flows. In addition, differentiable topology modules supports end-to-end topological summarization and back- propagates through it [31], and topological graph neural networks build on the relational model by integrating homological features [32]. Even with these developments, there is no graph learning framework that ties equivariance, physics-informed constraints and topological regularization directly for predicting force networks in active matter. Here, we fill this gap by introducing a full, multi-step pipeline for topologically regularized force forecasting in active colloidal suspensions as our work. To begin we use a high-resolution fluid velocity and stress fields in periodic boundary conditions using a lattice-Boltzmann solver [7][8][9]. We then secondly E(2) equivariant/agnostic-GNN with particle positions/orientations to predict approximate pairwise forces by harnessing recent progresses in improving group- equivariant message passing [10][11][12] (LR-2203) and33traditional34. Finally, a PINN corrects these estimate using Fourier feature maps: deep residulas networks and balances the physical consistency as well as Navier Stokes residual [14][15][16][17]. Next, we incorporate topological regularization as a loss term in which we develop based on persistent homology to penalize unrealistically tangled or spurious connections in the force graph [18][19][20][21], exploiting efficient persistent homology toolchains [22][23]. We also aggregate persistence- based loss functions from segmentation [29] and biomolecular topology [24], together with differentiable topology layers [31] and topological GNN components [32] to enforce global network consistency. By integrating these we capture fine-scale hydrodynamic interaction, and emergent global topology to provide a data-driven, scalable model of complex system of active matter systems. Methodology This section is the description of framework to perform computational simulation of active particles in a fluid, as well as learning their dominating interaction by integraci√≥n. We simulate 2D fluid and a swarm of self propelled particles in turn, by sequentially: coupling a lattice-", "response": "Capturing the dynamics of active particles, i.e., small self-propelled agents that both deform and are deformed by a fluid in which they move is a formidable problem as it requires coupling fine scale hydrodynamics with large scale collective effects. So we present a multi-scale framework that combines the three learning-driven tools to learn in concert within one pipeline. We use high-resolution Lattice Boltzmann snapshots of fluid velocity and particle stresses in a periodic box as input to the learning pipeline. the second step takes the morphology and positions orientations of particles to predict pairwise interaction forces between them with a E(2)-equivariant graph neural network that necessarily respect flat symmetries. Then, a physics-informed neural network further updates these local estimates by summing over them with a stress data using Fourier feature mappings and residual blocks that is additionally regularized with a topological term (introduced by persistent homology) to penalize unrealistically tangled or spurious connections. In concert, these stages deliver an holistic highly-data driven full force network prediction empathizing on the physical underpinnings together with emerging multi-scale structure typical for active matter."}
{"prompt": "Title: CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning\n\nINTRODUCTION In recent years, multimodal large language models (MLLMs) have been pro- pelling artificial intelligence from purely textual understanding toward compre- hensive understanding and reasoning across ‚Äúall modalities,‚Äù including vision, speech, and video. In 2023, OpenAI released GPT-4, which for the first time in- troduced visual input capabilities, enabling LLMs to parse images and perform reasoning [1]. Since then, numerous researchers have begun developing and ap- plying multimodal large models [2], [3]. Against this backdrop, tasks such as video question answering impose higher demands on dynamic-scene comprehen- sion and temporal reasoning[4], motivating research to unify, align, and fuse vision, language, and audio in order to enhance cross-modal understanding and reasoning [5]. Consequently, multimodal large models have expanded from im- ages to video and audio, evolving toward a general ‚Äúaudiovisual omnipotent‚Äù architecture that strengthens world representations while broadening human‚Äì computer interaction [6]. However, existing multimodal models still have shortcomings in complex reason- ing scenarios. Even with massive parameters and multimodal pre-training, they continue to face challenges such as insuÔ¨Äicient global contextual understanding and ‚Äúshortcut‚Äù reasoning in complex tasks [7]. Models often over-rely on local or single-modality cues while overlooking critical cross-modal information, leading to outputs that deviate from human intent [8]. On the other hand, even when employing prompting strategies such as Chain-of-Thought, current multimodal large models remain notably limited on multi-step cross-modal reasoning tasks [9]. Constructing reasoning chains via reinforcement learning may cause mod- els to acquire ‚Äúshortcut‚Äù strategies, thereby reducing generalization [10]. These phenomena indicate that relying solely on the model‚Äôs inherent reasoning ability and simple prompts is still insuÔ¨Äicient to align the model with human intent. To address these issues, researchers have begun incorporating ‚Äúintent‚Äù as a mediating variable connecting user queries and cross-modal evidence into the multimodal reasoning loop: one line of work introduces explicit intent labels or scene purposes in multimodal question answering and video understanding to constrain candidate answers and the scope of reasoning [7]; another employs in- struction tuning and templated prompts to explicitly declare desired behaviors on the input side, thereby implicitly aligning reasoning goals [11]; still others construct ‚Äúintent-conditioned‚Äù retrieval‚Äìreasoning pipelines or agents so that ev- idence selection and reasoning steps are driven by the current intent [12] ; there are also methods that adopt text-guided multimodal fusion to assist multimodal intent understanding [13]. Meanwhile, attempts to exploit the temporal and event structures of audio‚Äìvideo to infer latent intent and filter irrelevant cues are increasing [14], [15]. However, these methods have certain limitations: they 2 rely on dense annotation and task-specific training, making zero-shot transfer diÔ¨Äicult [7], [16], or they treat intent as static labels or prompt fragments with- out externalizing it into generable, assessable, and selectable strategies, thereby failing to stably suppress ‚Äúshortcut‚Äù reasoning and local biases [8], [11]. Recently, studies have begun to explore multimodal reasoning that simulates hu- man ‚Äúsketches of thought.‚Äù For example, the Sketch-of-Thought framework [17] introduces a human-like cognitive reasoning paradigm that maintains reasoning accuracy while reducing verbose intermediate reasoning; the Machine Mental Imagery approach [18] enables models to incorporate latent visual ‚Äúimagina- tion‚Äù representations during reasoning, simulating the human process of using mental imagery to aid reasoning, thereby improving reasoning eÔ¨Äiciency and un- derstanding of complex scenes. Inspired by such research, we propose a human- like cognitive intent-sketch reasoning component that modularly enhances multi- modal intent recognition and reasoning performance. The core idea is to decom- pose and recombine the context of complex problems and to embed human cogni- tive processes into model reasoning through explicit strategy-planning prompts (i.e., the ‚Äúintent sketch‚Äù mechanism), thereby strengthening the model‚Äôs under- standing of multimodal information and latent intent. Specifically, we design a plug-and-play three-module pipeline: Intent Perceiver, Strategy Generator, and Strategy Selector. The Intent Perceiver module analyzes video scenes and gener- ates textual intent; the Strategy Generator module produces multiple candidate reasoning strategies based on the context; and the Strategy Selector module eval- uates these candidate strategies and selects the optimal one. Finally, a large language model (serving as the reasoning engine) combines the selected strategy with the context and produces an answer by injecting prompts. This compone", "response": "Targeting the issues of \"shortcuts\" and insufficient contextual understanding in complex cross-modal reasoning of multimodal large models, this paper proposes a zero-shot multimodal reasoning component guided by human-like cognitive strategies centered on an \"intent sketch\". The component comprises a plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and Strategy Selector-that explicitly constructs a \"understand-plan-select\" cognitive process. By generating and filtering \"intent sketch\" strategies to guide the final reasoning, it requires no parameter fine-tuning and achieves cross-model transfer solely through in-context engineering. Information-theoretic analysis shows that this process can reduce conditional entropy and improve information utilization efficiency, thereby suppressing unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and Daily-Omni validate the method's generality and robust gains; compared with their respective baselines, the complete \"three-module\" scheme yields consistent improvements across different reasoning engines and pipeline combinations, with gains up to approximately 9.51 percentage points, demonstrating the practical value and portability of the \"intent sketch\" reasoning component in zero-shot scenarios."}
{"prompt": "Title: QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients\n\nIntroduction Photoplethysmogram (PPG) and the electrocardiogram (ECG), have been extensively shown to be valuable for clin- ical diagnosis and prediction (Almarshad et al. 2022) (Bal- aji et al. 2002). However,in the context of clinical settings, such as Intensive Care Unit (ICU) or Operating Room (OR), the high incidence of poor, incomplete, and inconsistent sig- nal quality can compromise the monitoring tasks, such as the prompt identification of life-threatening events. These physiological data are highly susceptible to factors such as patient movements, inadequate electrode-skin contact, as Copyright ¬© 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. well as instrumental noise and artifacts (Gambarotta et al. 2016). The consequences of such low-quality signals con- sist mainly in triggering false alarms that cause alarm fa- tigue of clinical staff (Drew et al. 2014), affecting the accu- racy of automated diagnostic systems (van der Bijl, Elgendi, and Menon 2022) as well as compromising the continuous monitoring of these patients. To solve this problem many methods have been pro- posed and they are mainly base on feature engineering cou- pled with traditional machine learning, such as length trans- forms (Zong, Moody, and Jiang 2003) or peak energy de- tection (Oster et al. 2013), but the reliance on domain- specific prior knowledge restrict their generalizability. In re- sponse to these limitations, end-to-end deep learning models were introduced to automatically learn feature hierarchies and increase performance (Rim et al. 2020). Recently, con- trastive learning (Zhou et al. 2022) and stable diffusion (Wu et al. 2023) have been proposed to address false arrhyth- mia alarms. However, the supervised learning paradigm re- quires vast quantities of meticulously labeled data to guide model training. The labeling process needs expert clinicians making it expensive and it is time-consuming. Furthermore, these models are typically designed for a single task. If we change the downstream task, the model needs to be re- designed and trained, demonstrating a fundamental lack of cross-task transferability (see Appendix A for further details on the state-of-the-art literature). To overcome the limitations of label dependency and poor transferability, we introduce a new paradigm: a multimodal physiological signal foundation model designed to build a general-purpose understanding of signal quality (quali- tyFM). The objective of this work is to construct a Qual- ityFM from PPG and ECG recordings and to adapt it to various downstream tasks with the final goal to address challenges arising from poor signal quality and clinical needs. We pre-train our model using a tailored-made self- supervised learning strategy on a large-scale dataset consist- ing of 21 million waveforms of 30 second length for a total of 179,757 hours. The model input consist of paired physio- logical signals of differing quality close in time and a dual- track architecture was adopted to generate the corresponding feature representations, which serve for the pairs of differ- ent quality signals. We adopt a self-distillation strategy to use the encoder of high-quality signals as a teacher model arXiv:2509.06516v1 [cs.LG] 8 Sep 2025 Figure 1: Pipeline of QualityFM pretraining and downstream tasks. Stage 1 (Pretraining): a teacher encoder updated via Exponential Moving Average (EMA) guides a student encoder using pairs of high- and low-quality signals, while a decoder reconstructs spectral features. Stage 2 (Downstream Tasks): the pretrained teacher encoder is adapted for downstream tasks, including false arrhythmia alarm detection, Atrial Fibrillation (AF) identification, and arterial blood pressure (BP) estimation. to guide the encoder of low-quality signals. The teacher model parameters are smoothly updated using an Exponen- tial Moving Average (EMA) strategy to ensure filter noises. Furthermore, we employ a window-based Transformer to address the excessive computational complexity of self- attention in long sequential signals using a windowed sparse attention mechanism. This ensures computational feasibility while effectively modeling the essential local characteristics of the signals. Finally, we design a Composite Loss Function with Di- rect and Indirect Supervision. A decoder, sharing weights with the encoder, aims at reconstructing the amplitude and phase of signal spectrum from the features. The loss is then calculated between the reconstructed spectra and the origi- nal spectra of the signal pairs. To demonstrate its efficacy and practical value, we pre- train three models with varying parameter counts, from 9.6 M to 319 M, and validate their performance through transfer learning on three distinct ICU medical tasks. To the best of our knowledge, qualityFM represents the largest foundation model for addressing quality challenges of ECG and PPG signals in critically ill patients. Methodolo", "response": "Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in intesive care unit (ICU) and operating room (OR). However, the high incidence of poor, incomplete, and inconsistent signal quality, can lead to false alarms or diagnostic inaccuracies. The methods explored so far suffer from limited generalizability, reliance on extensive labeled data, and poor cross-task transferability. To overcome these challenges, we introduce QualityFM, a novel multimodal foundation model for these physiological signals, designed to acquire a general-purpose understanding of signal quality. Our model is pre-trained on an large-scale dataset comprising over 21 million 30-second waveforms and 179,757 hours of data. Our approach involves a dual-track architecture that processes paired physiological signals of differing quality, leveraging a self-distillation strategy where an encoder for high-quality signals is used to guide the training of an encoder for low-quality signals. To efficiently handle long sequential signals and capture essential local quasi-periodic patterns, we integrate a windowed sparse attention mechanism within our Transformer-based model. Furthermore, a composite loss function, which combines direct distillation loss on encoder outputs with indirect reconstruction loss based on power and phase spectra, ensures the preservation of frequency-domain characteristics of the signals. We pre-train three models with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy and practical value through transfer learning on three distinct clinical tasks: false alarm of ventricular tachycardia detection, the identification of atrial fibrillation and the estimation of arterial blood pressure (ABP) from PPG and ECG signals."}
{"prompt": "Title: RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms\n\nI. INTRODUCTION Reinforcement Learning (RL) is a powerful framework for autonomous decision-making, enabling significant suc- cess in various domains from video games [1] to robotics control [2]. However, despite significant progress in simu- lated environments, applying RL algorithms to real-world robotic systems remains highly challenging due to practical constraints such as sample inefficiency, high computational demands, and inference delays [3]. To train an RL agent directly on a physical robot, it must learn from limited data while ensuring real-time inference at the control frequency of the system. In simulated environments, these constraints are often overlooked since agents can generate unlimited data and the environment can wait for computations to finish. In contrast, real-world robotic applications impose strict time and data constraints, making direct learning on hardware significantly more difficult. This work aims to enable RL deployment on real robots systems while addressing two fundamental constraints: (i) a limited budget on training data, as real-world data collection is costly and time-consuming, and (ii) real-time execution constraints, where the limited processing power of embedded hardware introduces inference delays that must be managed to maintain synchronization with the system‚Äôs operating frequency. Model-free RL (MFRL) methods show strong perfor- mance in simulated robotic tasks [2], but they are notoriously sample-inefficient, requiring millions of interactions to learn effective policies. This makes real-world deployment imprac- Authors are with 1Sorbonne Universit¬¥e, CNRS, ISIR, F-75005 Paris, France. Emails: {elasri, laiche, rambour, sigaud, thome}@isir.upmc.fr 2 Institut Universitaire de France (IUF) (a) Collect trajectories with d-step MPC (b) Concatenate augmented states to restore the original MDP Fig. 1: In the case of inference delay, the agent requires d timesteps to compute the next action, causing it to miss d‚àí1 environment steps. The d-step MPC framework allows the agent to compensate for this delay by planning a sequence of d actions instead of a single action, and to restore the Markov property by augmenting the current state with the d missed states and the d future actions. tical, as data collection on physical robots is always time consuming, often expensive, and even sometimes risky [3]. An alternative approach, model-based RL (MBRL) [4], has gained attention for its ability to learn system dynamics from limited data and use predictive models for planning. Unlike model-free methods, MBRL builds an internal representation of the environment, allowing the agent to simulate potential future states and make more informed decisions. A common strategy in MBRL is Model Predictive Control (MPC), which optimizes control actions over a finite horizon by leveraging a learned forward model to predict future states [5], [6]. Since MPC requires solving an optimization problem at each decision step, many approaches rely on the Cross-Entropy Method (CEM) [7], a gradient-free stochastic optimization technique, to efficiently search for optimal sequences of actions, particularly in complex and nonlinear dynamics. While MBRL significantly improves sample efficiency, it comes at the cost of high computational demands, often requiring considerable inference time to compute actions. As a result, the agent may need multiple control timesteps to compute an action, misaligning its decision-making cycle arXiv:2509.06714v1 [cs.LG] 8 Sep 2025 with the system‚Äôs control loop. This results in execution gaps where no new control commands are issued, potentially degrading performance and stability. We refer to this problem as inference delay, a fundamental challenge in deploying MBRL for real-time control. It has recently been shown that inference delays could be drastically reduced by combining MPC with model-free RL [6] and that an even better compromise between inference time, sample efficiency, and performance could be found by also leveraging prior knowledge about the dynamics of the controlled system [8]. However, while these approaches reduce inference delays, they do not eliminate them entirely. A large inference time persists, especially in real-world scenarios in which embedded computational resources are limited, making inference delay a persistent challenge for real-world robotic applications where real-time control is critical. Unlike transmission delays, which simply create a lag between action selection and execution while maintaining the control frequency of the system, inference delay arises from the agent‚Äôs decision-making process, disrupting synchro- nization between its decision-making cycle and the control frequency of the system. To address the challenge of inference delay in real-world robotics, we present the following key contributions: 1) We introduce a delay-MDP problem to formally ac- count for inference delay and propose a d-step MPC framework for MBRL under in", "response": "Learning a controller directly on the robot requires extreme sample efficiency. Model-based reinforcement learning (RL) methods are the most sample efficient, but they often suffer from a too long inference time to meet the robot control frequency requirements. In this paper, we address the sample efficiency and inference time challenges with two contributions. First, we define a general framework to deal with inference delays where the slow inference robot controller provides a sequence of actions to feed the control-hungry robotic platform without execution gaps. Then, we compare several RL algorithms in the light of this framework and propose RT-HCP, an algorithm that offers an excellent trade-off between performance, sample efficiency and inference time. We validate the superiority of RT-HCP with experiments where we learn a controller directly on a simple but high frequency FURUTA pendulum platform. Code: github.com/elasriz/RTHCP"}
{"prompt": "Title: Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers\n\nIntroduction Medical image segmentation is a fundamental task in computer-aided diagnosis, guiding clinicians in identifying and delineating critical anatomical or pathological structures Ron- neberger et al. (2015). Conventional convolutional neural network (CNN) architectures, most notably the U-Net family Ronneberger et al. (2015), have achieved remarkable success in this domain. However, the limited receptive field of standard convolutions often restricts global context modeling, which can be critical for segmenting complex or subtle regions in medical images Gang (2025); Li et al. (2021). In parallel, Transformers Vaswani et al. (2017)Dosovitskiy et al. (2020) have recently shown promise in vision tasks, capturing both short- and long-range dependencies by virtue of the self-attention mechanism Schlemper et al. (2019). Merging these two paradigms, hybrid U-Net-like models with Transformer encoders, such as TransUNet Chen et al. (2021) and Swin-Unet Cao et al. (2021), are an emerging trend for high-accuracy medical segmentation tasks. Self-supervised learning (SSL) further addresses a key obstacle in medical imaging: the shortage of large-scale labeled data Jin et al. (2023). By leveraging redundancy and data augmentations, SSL methods can pretrain encoders to extract robust, generalizable features without extensive annotations. Among the various SSL approaches, Barlow Twins Zbontar et al. (2021) has gained attention for its less complex and effective redundancy reduction objective. A prior work integrated Barlow Twins into a U-Net backbone, demonstrating improved encoder representations Chaitanya et al. (2020); Punn1 and Agarwal1 (2021). Nevertheless, this line of research remains underexplored, particularly with Transformer- based encoders. In this paper, we introduce Barlow-Swin, a novel hybrid end-to-end lightweight architecture that benefits from a Swin Transformer-like encoder Liu et al. (2021) with a U-Net-like de- coder. Encoder is first pretrained via Barlow Twins on the same medical imaging datasets. By preserving high-level global context from the shifted window-based self-attention mech- anism Li et al. (2018) and fusing it with low-level spatial details through skip connections, our method aims to produce high-fidelity segmentation masks. Barlow Twins pretraining is used to enhance representation quality without requiring extensive labeled sets. In contrast to deeper Swin Transformer architectures like SwinUnet and TransUnet Cao et al. (2021) Chen et al. (2021) , we opt for a shallower configuration to ensure computational efficiency, making it more attractive for real-time or resource-constrained environments. We evaluate Barlow-Swin on four public medical image datasets (BCCD, BUSIS, ISIC2016, and a retinal dataset), each covering a diverse set of segmentation challenges. Our exper- iments compare the proposed architecture with classical U-Net baselines, the BT-UNet Punn1 and Agarwal1 (2021), as well as other modern designs such as YOLOv8 Ultralytics (2023), Segment Anything (SAM) Kirillov et al. (2023), and HoverNet Graham et al. (2019). Barlow-Swin achieves competitive or superior performance in terms of Dice coefficient and other metrics, underscoring the benefit of self-supervised Swin encoders for medical image segmentation. Qualitative visualizations are also provided, illustrating that Barlow-Swin can more accurately capture boundary details relative to purely convolutional architectures. The remainder of this paper is organized as follows. Section 2 surveys the literature on U-Net variants, vision transformers in medical applications, and self-supervised learning. Section 3 outlines the Barlow-Swin architecture and SSL pretraining strategy. Section 4 describes the datasets, training protocols, and experimental results. Section 5 presents a discussion of our findings. Finally, Section 6 concludes the paper. 2 2 Related Works Medical image segmentation has a long history, with classical methods preceding the advent of deep neural networks. Early approaches included thresholding, region growing, active contours (snakes) Kass et al. (1988), and atlas-based segmentation Ashburner and Friston (2000)Pham et al. (2000), which relied heavily on handcrafted features and prior anatomical knowledge. While effective in certain scenarios, these methods often struggled with variability in anatomy, noise, and imaging artifacts. The field underwent a paradigm shift with the introduction of deep learning, particularly fully convolutional networks (FCNs) and the U-Net family Ronneberger et al. (2015); Cicek et al. (2016). U-Net and its variants, such as U-Net++ Zhou et al. (2018), ResUNet Isensee et al. (2021), and UNet 3+ Huang et al. (2020), utilize a symmetric encoder-decoder archi- tecture with skip connections, achieving state-of-the-art performance on a wide range of segmentation tasks across modalities including MRI Menze et al. (2015), CT Bilic et al. (2019), and retinal fundus imaging Staal", "response": "Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: https://github.com/mkianih/Barlow-Swin."}
{"prompt": "Title: Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in Panoramic Radiographs using Federated, Centralized and Local Learning\n\nIntroduction Artificial intelligence (AI) techniques are currently transforming and innovating diverse fields such as healthcare, finance, and logistics [1, 2]. In particular in the medical domain, AI has demonstrated considerable potential in improving diagnostic accuracy and patient treatment outcomes [3‚Äì5]. However, despite these benefits, especially for automating repetitive tasks, the integration of AI technology into clinical practice remains limited by challenges related to data collection and sharing [6‚Äì8]. The most straightforward method for training an AI model is to use locally available data, a paradigm referred to as Local Learning (LL). Yet, to achieve an optimal and generalized model that is applicable across various hospitals, centers, and clinics (hereafter referred to as ‚Äùclients‚Äù), data must ideally be aggregated in a centralized manner, which enables centralized learning (CL) [9]. This approach, however, is often restricted by regulatory, user-preference, and data volume constraints, as well as ethical and legal privacy concerns (e.g., compliance with the General Data Protection Regulation (GDPR)) [10]. Therefore, a novel approach is required that can balance high-performance model training with strict adherence to data privacy regulations. Federated Learning (FL) offers such a solution by enabling multiple clients to collaboratively train a shared model while keeping the data on-site. Unlike a centralized approach, which would require both data and the associated computational ¬© The Author 2025. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com 1 arXiv:2509.06553v1 [eess.IV] 8 Sep 2025 2 Johan Andreas Balle Rubak et al. load to be gathered in one location, FL distributes the training workload across each data holder, making it especially attractive for medical imaging. FL allows institutions to benefit from diverse and extensive datasets without compromising patient privacy or data security [11‚Äì13]. Nevertheless, FL introduces new challenges: each client must possess adequate computational resources, a requirement that may entail significant initial economic investment, and issues related to data heterogeneity and inconsistent labeling may further complicate model performance [14]. Understanding these factors is essential to evaluate FL‚Äôs integration into medical applications and to identify elements that may either facilitate or hinder its clinical adoption. The primary objective of this study is to investigate FL‚Äôs potential to enhance segmentation of dental anatomical structures in panoramic radiographs, while addressing some of the specific challenges associated with its implementation. To this end, we: (1) establish an FL pipeline for simulating a central server with distinct client nodes housing local data repositories; (2) Compare the performance of FL models with those trained via CL or LL; and (3) Assess the sensitivity of FL models to the variations in image quality across clients and labeling inconsistencies in order to mimic real-world scenario. Our hypothesis is that FL can significantly improve performance compared with LL and even rival the effectiveness of CL. Data and Methods Study Design Panoramic radiography provides a comprehensive two- dimensional overview of the dentition and surrounding maxillofacial structures, playing a crucial role in diagnostic assessment and treatment planning [15]. While manual delineation of individual teeth is labor-intensive and subject to inter- and intra-observer variability, automated segmentation solutions provide a more standard and reliable alternative [16, 17]. This study aims to investigate the potential of FL for training neural networks to segment dental anatomical structures (i.e., teeth) in comparison to the more conventional training paradigms, namely LL and CL. To this end, we evaluate the trained models through both local validation data and centralized test data. The experiments were repeated under four conditions: (1) baseline models with unaltered data, (2) models affected by label manipulation for one client (3) models affected by image quality degradation for one client, and (4) models with one faulty client excluded. This will be discussed further in more details under the Training procedure. Data The dataset used in this study is the open-source A dual labeled dataset [18] (the version of the dataset was downloaded the 23rd December, 2024), that comprises of 2066 panoramic radiographs collected from three hospitals (China-Japan Union Hospital of Jilin University, Wuxi Stomatology Hospital, and People‚Äôs Hospital of Zhengzhou) and three dental clinics between January 1, 2015, and December 31, 2023. Imaging equipment was sourced from various manufacturers, including Orthophos XG (Dentsply Sirona, Germany), Planmeca ProMax (Planmeca, Finland), and Bondream 1020 (Bondream, China). The study population (989 males and 1011 females) consisted of individu", "response": "Objectives: Federated learning (FL) may mitigate privacy constraints, heterogeneous data quality, and inconsistent labeling in dental diagnostic AI. We compared FL with centralized (CL) and local learning (LL) for tooth segmentation in panoramic radiographs across multiple data corruption scenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six institutions across four settings: baseline (unaltered data); label manipulation (dilated/missing annotations); image-quality manipulation (additive Gaussian noise); and exclusion of a faulty client with corrupted data. FL was implemented via the Flower AI framework. Per-client training- and validation-loss trajectories were monitored for anomaly detection and a set of metrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set. From these metrics significance results were reported through Wilcoxon signed-rank test. CL and LL served as comparators. Results: Baseline: FL achieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at 0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777). Label manipulation: FL maintained the best median Dice score at 0.94884 (ASSD: 1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD: 1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL scored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD: 1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD: 1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring reliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and outperforms LL across corruption scenarios while preserving privacy. Per-client loss trajectories provide an effective anomaly-detection mechanism and support FL as a practical, privacy-preserving approach for scalable clinical AI deployment."}
{"prompt": "Title: An AI system to help scientists write expert-level empirical software\n\nIntroduction Scientists need diverse information to advance their scientific agendas. Some are simple questions for which perfunctory answers can be fulfilled by a search engine. However, performing computational experiments often demands deeper information. For example, one of the authors‚Äô research involves deforestation analyses, assessing land cover change1 using global spatially-resolved measurements, past and present. This is carried out using a satellite-based deforestation detector, built with code to answer a scientific question. A deforestation detector is one of many thousands of examples of empirical software in science. We use the term empirical software to mean software that is designed to maximize a definable or measurable quality score, typically a fit to existing observations. If a task can be solved with empirical software, we call this a scorable task. We have two hypotheses about the scorable tasks and empirical software in science. First, scorable tasks are ubiquitous in science. Almost every sub-field of science, applied mathematics, and engineering now relies on software. In the combined experience of the authors, we have found that much of this software is empirical software solving a scorable task. Often such empirical software is at the heart of *Equal contribution in alphabetical order. ** Carried out as part of a student researchership at Google Research. ‚Ä° To whom correspondence should be addressed: shibl@google.com, mbrenner@google.com arXiv:2509.06503v1 [cs.AI] 8 Sep 2025 An AI system to help scientists write expert-level empirical software a scientist‚Äôs work. Empirical software has recently enabled a number of Nobel Prizes in Chemistry: in 1998 for Density Functional Theory2,3, in 2013 for molecular dynamics simulation4 and in 2024 for protein structure prediction5,6. Empirical software underlies our ability to create models of complex systems, ranging from parameterizations of a vertical column of the earth‚Äôs atmosphere for weather modeling7, to the parameterization of stress response in a turbulent fluid flow8, to the prediction of social systems9‚Äì11. Second, empirical software for science is slow and difficult to create. Domain-specific empirical software requires tedious work, often over many years. When empirical software is used to test complex hypotheses, it becomes ever more difficult to write purely from first principles. There usually is no systematic search for alternative approaches. Design choices are often governed by intuition or expediency, rather than exhaustive experimentation. Creating the software is so time-consuming that it severely limits the possibilities that can be productively explored. This paper presents an AI-based system that systematically and automatically creates empirical software to solve scorable tasks. Our method is based on an LLM that rewrites software to attempt to improve its quality score. The system creates a number of software candidate solutions, and uses Tree Search12,13 to decide which candidates merit further exploration (Fig. 1a). While there are many ways of designing a code mutation system14‚Äì18, we developed and refined the method by designing and competing against a benchmark of basic Kaggle competitions (Fig. 1b), described below. We augment code mutation with research ideas, obtained from a range of sources from highly cited papers, to specialized textbooks, to results of search engines (Fig. 1c). In practice, these ideas can be injected either directly by the user or automatically using a search engine to access research in the literature. The LLM uses this injected guidance in writing code. We find that our method can be applied to a wide variety of scorable tasks from across science, producing software that outperforms the state-of-the-art produced by scientists. This superhuman performance arises because of the ability to exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying needle-in-the-haystack high quality solutions. Results Overview of Scorable Tasks We develop our method on a benchmark of Kaggle playground competitions, and test it by selecting scorable tasks based on scientific or engineering problems. We selected these problems using two criteria: first, we chose tasks which have had slow recent progress, but yet are important to a set of scientists; second, we chose tasks which would be useful to the scientific agenda of at least one co-author. These scorable tasks are listed below. scRNA-seq batch integration:19 By removing confounding factors, we can enable large-scale multi-lab transcriptomic data integration, such as the Human Cell Atlas20. This is a difficult problem because it requires distinguishing subtle biological signals from noise in high-dimensional sparse datasets. CDC COVID Forecasting:21 By predicting COVID cases several weeks in advance, we can inform public health policy and resource allocations. The challenge in this task arises from predicting non-", "response": "The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. Our method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress."}
{"prompt": "Title: SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion\n\nIntroduction Knowledge graphs (KGs) encode real-world facts as structured triples (h, r, t), where h and t de- note the head and tail entities, respectively, and r is the relation connecting them. As a backbone for structured knowledge representation, KGs em- power a variety of downstream applications such as question answering (Saxena et al., 2020), recom- mendation (Wang et al., 2019), and commonsense **Corresponding author. reasoning (Lin et al., 2019). However, real-world KGs are often incomplete, which motivates the task of knowledge graph completion (KGC), i.e., predicting missing entities or relations. Traditional KGC methods such as TransE (Bor- des et al., 2013), DistMult (Yang et al., 2015), and RotatE (Sun et al., 2019) learn low-dimensional embeddings for entities and relations, and rank candidate triples based on geometric scoring func- tions. While these models perform well in dense regions of the KG, they often underperform on long-tail entities with sparse local neighborhoods. To mitigate this, some extensions incorporate tex- tual features (Wang et al., 2021b) or graph-aware context (Vashishth et al., 2020), but still struggle with generalization and semantic discrimination. Recent advances in large language models (LLMs) have introduced a new paradigm for knowl- edge graph completion (KGC), where pretrained models leverage semantic priors to generate miss- ing entities from textualized queries (Lewis et al., 2020; Raffel et al., 2020; Xie et al., 2022; Sax- ena et al., 2022). To improve grounding, sev- eral strategies incorporate KG-derived signals into prompts. Instruction tuning (Liu et al., 2024) en- codes relation semantics and output formats into natural language templates, while structural aug- mentation (Liu et al., 2025; Wei et al., 2024; Yang et al., 2025) use local subgraphs or structure-aware demonstrations to better align with graph context. Despite these strategies having shown promising improvements in generation controllability and KG- awareness, persistent limitations emerge, as illus- trated in Figure 1, when examining the link predic- tion query (?, born_in, Salzburg): ‚Ä¢ Challenge 1: Structural Sparsity ‚Äî KG- augmented LLMs rely on local subgraph con- text for grounding, yet many entities are poorly connected. In this case, sparse links around the gold entity ‚ÄúWolfgang Amadeus arXiv:2509.06531v1 [cs.CL] 8 Sep 2025 Wolfgang Amadeus Mozart Salzburg Composer occupation born_in Query: (?, born_in, Salzburg) The answer is : Vienna Philharmonic (a) LLM with Noisy or Sparse KG (b) LLM with Ambiguous Structure Structural sparsity ‚Üíhallucinated answer Semantically similar but Structural inconsistent ‚Üímisprediction (c) SLiNT(Our method) Structure-aware contrastive disambiguation ‚Üícorrect prediction The answer is :Joseph Haydn The answer is : Wolfgang Amadeus Mozart Wolfgang Amadeus Mozart Austria born_in Rohrau located_in Salzburg located_in Joseph Haydn Contrastive learning Structure- aware evidence Pseudo-neighbors Figure 1: Motivating example for SLiNT. Given query (?, born_in, Salzburg), (a) LLMs hallucinate due to sparse KG; (b) Semantic similarity overrides structural correctness, causing misprediction; (c) SLiNT disam- biguates candidates via contrastive reasoning and struc- ture injection. Mozart‚Äù offer little structural support, thereby causing the model to hallucinate plausible but unsupported answers such as ‚ÄúVienna Phil- harmonic.‚Äù This reflects a critical failure: the generation collapses when the KG lacks suffi- cient structural cues. ‚Ä¢ Challenge 2: Semantic Ambiguity ‚Äî Even when structurally valid entities like ‚ÄúWolf- gang Amadeus Mozart‚Äù are retrieved, mod- els may make wrong predictions by select- ing semantically similar but incorrect alter- natives such as ‚ÄúJoseph Haydn.‚Äù This con- fusion arises because current LLMs favor surface-level similarity over structural align- ment, lacking mechanisms to resolve fine- grained, relation-specific conflicts in entity semantics. To tackle the aforementioned challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a uni- fied generative framework that explicitly integrates structural context and fine-grained supervision into a frozen LLM backbone with lightweight LoRA- based adaptation (Hu et al., 2022). To address Challenge 1, SLiNT introduces Structure-Guided Neighborhood Enhancement (SGNE), which re- trieves top-ks pseudo-neighbors from pretrained KG embeddings and fuses them with attention to construct richer contextual representations for sparsely connected entities. To mitigate Chal- lenge 2, we develop Dynamic Hard Contrastive Learning (DHCL), which synthesizes interpo- lated hard positives and negatives based on se- mantic proximity and structural signals, encour- aging the model to distinguish structurally coher- ent answers from semantically similar but mislead- ing distractors. To bridge the gap between struc- tural representations and language generation, we fu", "response": "Link prediction in knowledge graphs requires integrating structural information and semantic context to infer missing entities. While large language models offer strong generative reasoning capabilities, their limited exploitation of structural signals often results in structural sparsity and semantic ambiguity, especially under incomplete or zero-shot settings. To address these challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive Learning (DHCL) introduces fine-grained supervision by interpolating hard positives and negatives to resolve entity-level ambiguity; and Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware intervention while preserving the core LLM parameters. Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion."}
{"prompt": "Title: AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification\n\nIntroduction Class imbalance refers to the phenomenon where the distribution of classes in a dataset is biased towards specific classes (Leevy et al., 2018; Aguiar et al., 2024). It is identified as a challenging problem in data mining and machine learning, as machine learning and deep learning classifiers tend to perform suboptimally under imbalanced data, specifically towards minority class instances (Altalhan et al., 2025; Ghosh et al., 2024). This problem cannot be ignored as many real-world and engineering datasets naturally have imbalanced class distributions due to their real-world characteristics (Kishanthan and Hevapathige, 2025; Chen et al., 2024). Researchers have explored various mechanisms to alleviate class imbalance issues for decades, lead- ing to the development of diverse techniques to improve classifier performance on minority classes. These techniques include data resampling, where sampling is performed to balance class distributions (Carvalho et al., 2025; Zhao et al., 2024), algorithm-level approaches that modify the learning model to be more sensitive to minority classes (Araf et al., 2024; Farhadpour et al., 2024), and hybrid approaches that combine both techniques (Shin et al., 2024; Goswami and Singh, 2024). Among these methods, data resampling is commonly preferred due to its model-agnostic nature, simplicity, and effectiveness in practice. Specifically, data oversampling, which increases the number of minority samples, has been empirically shown to work better for real-world datasets. Unlike undersampling, which reduces the total amount of training data by discarding instances from the majority class, oversampling increases ‚àóCorresponding author 1 arXiv:2509.06875v1 [cs.LG] 8 Sep 2025 the dataset size by maintaining all original instances and adding more examples. This approach allows the model to learn more accurate decision boundaries with enhanced generalizability. Additionally, compared to hybrid sampling methods, oversampling is simpler and easier to interpret, as it avoids the complexities and additional hyperparameters associated with balancing multiple sampling techniques. Commonly used oversampling techniques include diverse methods such as SMOTE Chawla et al. (2002), SVMSMOTE Tang et al. (2008), BorderlineSMOTE Han et al. (2005), and ADASYN He et al. (2008), which focus on generating synthetic minority data instances by fusing the existing minority data. Nonetheless, these existing oversampling methods suffer from several limitations. ‚Ä¢ Feature Independence Assumption: Traditional oversampling methods treat features inde- pendently during synthetic sample generation, potentially breaking important feature correlations and semantic relationships that exist within the original data. ‚Ä¢ Lack of Similarity-Based Generation Control: Most existing oversampling approaches em- ploy simple interpolation between samples without adequately considering whether the selected samples are sufficiently similar for meaningful synthetic generation, which can result in unrealistic synthetic instances. ‚Ä¢ Deterministic Generation Processes: The generation processes in these methods tend to be relatively deterministic, following predictable patterns that may limit the diversity of the generated samples and reduce the robustness of the approach. ‚Ä¢ Limited Diversity Control Mechanisms: Traditional approaches generally lack sophisticated mechanisms for controlling diversity in synthetic samples, which can lead to overfitting to the training data and reduced generalization capability of the resulting models. In this work, we aim to address these limitations. We propose a novel oversampling approach that reconceptualises data instances as autonomous agents capable of complex interactions during syn- thetic sample generation. We observe that modelling data instances as agents enables the capture of sophisticated relationships and dependencies that traditional methods overlook. Specifically, we adopt Axelrod‚Äôs cultural dissemination model (Axelrod, 1997) as the theoretical foundation for our method, as it offers several key advantages: it naturally handles multi-dimensional feature interactions through the concept of cultural traits, incorporates similarity-based exchange mechanisms that ensure mean- ingful interactions between compatible instances, introduces probabilistic elements that add controlled randomness to the generation process, and provides explicit parameters for managing diversity and interaction patterns. Axelrod‚Äôs model is particularly well-suited for oversampling because it was orig- inally designed to explain how similar entities influence each other while maintaining diversity, which directly parallels the challenge of generating realistic synthetic samples that are similar to existing minority instances while avoiding exact replication. By leveraging this agent-based cultural exchange paradigm, our approach systematically addresses each of the identified gaps while providing a t", "response": "Class imbalance in machine learning poses a significant challenge, as skewed datasets often hinder performance on minority classes. Traditional oversampling techniques, which are commonly used to alleviate class imbalance, have several drawbacks: they treat features independently, lack similarity-based controls, limit sample diversity, and fail to manage synthetic variety effectively. To overcome these issues, we introduce AxelSMOTE, an innovative agent-based approach that views data instances as autonomous agents engaging in complex interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE implements four key innovations: (1) trait-based feature grouping to preserve correlations; (2) a similarity-based probabilistic exchange mechanism for meaningful interactions; (3) Beta distribution blending for realistic interpolation; and (4) controlled diversity injection to avoid overfitting. Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms state-of-the-art sampling methods while maintaining computational efficiency."}
{"prompt": "Title: H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers\n\nINTRODUCTION 3D Human pose estimation (HPE) from videos has numerous applications, such as action recognition [1], [2], [3], human-robot interaction [4], [5], [6], and computer animation [7], [8]. Current video-based 3D HPE methods mainly follow the pipeline of 2D-to-3D pose lifting [9], [10], [11], [12]. This two-stage pipeline first utilizes an off-the-shelf 2D HPE model to detect 2D body joints for each video frame and then employs a separate lifting model to estimate 3D pose sequences from the detected 2D poses. Recently, transformer-based architectures [13], [14], [15], [16], [17] have shown state-of-the-art (SOTA) performance in the field of video-based 3D HPE, thanks to their competency in modeling the long-range dependencies among video frames. These video pose transformers (VPTs) typically regard each video frame as a pose token and utilize extremely long video sequences to achieve superior HPE performance (e.g., 81 frames in [13], 243 frames in [15], [16], [18], or 351 frames in [14], [19], [20]). However, these methods inevitably suffer from high computational costs since the complexity of the self-attention in VPT grows quadratically with respect to the number of tokens (i.e., frames), hindering the deployment ‚Ä¢ Wenhao Li is with the 1State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, China, and also with the 2College of Computing and Data Science, Nanyang Technological University, Singapore. E-mail: wenhao.li@ntu.edu.sg. ‚Ä¢ Mengyuan Liu and Hong Liu are with the 1State Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School, China. E-mail: {liumengyuan, hongliu}@pku.edu.cn. ‚Ä¢ Pichao Wang is with 3Amazon AGI, USA. The work does not relate to author‚Äôs position at Amazon. E-mail: pichaowang@gmail.com. ‚Ä¢ Shijian Lu is with the 2College of Computing and Data Science, Nanyang Technological University, Singapore. E-mail: Shijian.Lu@ntu.edu.sg. ‚Ä¢ Nicu Sebe is with the 4University of Trento, Italy. E-mail: nicu- lae.sebe@unitn.it. ‚Ä¢ ‚àóCorresponding Authors: Mengyuan Liu, Hong Liu. of these heavy VPTs in many real-world applications. Two factors are crucial for achieving efficient VPTs. First, directly reducing the frame number can boost VPTs‚Äô efficiency, but it results in a small temporal receptive field that hinders the model from capturing sufficient spatio-temporal information in pose estimation [22], [23]. Hence, it is essential to design an efficient solution while maintaining a large tem- poral receptive field for accurate estimation. Second, adjacent frames in a video sequence contain redundant information due to the similarity of nearby poses (50 Hz cameras used in Human3.6M [24]). Moreover, recent studies [25], [26] found that many tokens tend to be similar in the deep transformer blocks. Hence, using full-length pose tokens in these blocks tends to introduce redundant calculations but contributes little to the pose estimation. Based on these observations, we propose to prune pose tokens in the deep transformer blocks to improve the efficiency of VPTs. Although token pruning can reduce the number of tokens and improve efficiency, it also makes it difficult to estimate the consecutive 3D pose of all frames, as each token corresponds to one frame in existing VPTs [14], [15], [16]. Additionally, for efficient inference, a real-world 3D HPE system should be able to estimate the 3D poses of all frames at once in an input video. Therefore, it is necessary to recover the full-length tokens to estimate 3D poses for all frames so that the model can achieve fast inference and be compatible with existing VPT frameworks. Driven by this analysis, we present a novel hierarchical pruning-and-recovering framework for efficient transformer- based 3D HPE from videos. Different from existing VPTs [13], [14], [15], [16] that maintain the full-length sequence across all blocks, our method begins with progressively pruning the pose tokens of redundant frames and ends with arXiv:2509.06956v1 [cs.CV] 8 Sep 2025 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 Input Pose Tokens Output Pose Tokens (b) HoT (a) VPT Token Recovering Token Pruning Token Pruning Output Pose Tokens Input Pose Tokens Token Recovering Token Pruning Input Pose Tokens Output Pose Tokens (c) H2OT Fig. 1: Illustration of different VTP frameworks. (a) Existing VPTs [13], [14], [15], [16] follow a ‚Äúrectangle‚Äù paradigm that retains the full-length sequence across all blocks, which incurs expensive and redundant computational costs. (b) HoT [21] follows an ‚Äúhourglass‚Äù paradigm that prunes the pose tokens and recovers the full-length tokens, which keeps a few tokens in the intermediate transformer blocks and thus improves the model efficiency. (c) Our H2OT extends HoT [21] by introducing a hierarchical pruning design, forming an ‚Äútrophy-shaped (pyramidal)‚Äù paradigm. The gray squares represent the pruned tokens. recovering the full-length tokens", "response": "Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT."}
{"prompt": "Title: BEAM: Brainwave Empathy Assessment Model for Early Childhood\n\nI. INTRODUCTION Empathy plays a crucial role in facilitating the transmis- sion of emotional states [1], underpinning caregiving and prosocial behaviors, whereas its absence relates to severe social-emotional dysfunctions, such as antisocial personality disorder [2]. Empathy can be divided into two key compo- nents: cognitive empathy, also known as Theory of Mind (ToM), which refers to the ability to understand and adopt another‚Äôs perspective, and emotional empathy (EM), which involves the ability to share the emotional states of others in both valence and intensity [3]. While empathy-related abilities begin to emerge in infancy, their full development This work was supported by the STI 2030-Major Projects (No. 2022ZD0209000), the Shanghai Pilot Program for Basic Research - Chinese Academy of Sciences, Shanghai Branch (No. JCYJ-SHFY-2022-014), the Key Program of Xiamen Medical and Health (Grant No. 3502Z20234013), and the Grand Strategic Project of the National Social Science Fund of China (No. 23&ZD319). The computation in this work was also supported by the HPC Platform of ShanghaiTech University. 1School of Biomedical Engineering, ShanghaiTech University, Shanghai, China. xiechen2023@shanghaitech.edu.cn 2Ministry of Brain Functional Genomics (MOE&STCSM), School of Psychology and Cognitive Science, East China Normal University, Shang- hai, China. 3Children‚Äôs Hospital of Fudan University (Xiamen Branch), Xiamen Children‚Äôs Hospital, Fujian Key Laboratory of Neonatal Diseases, Xiamen, Fujian, China. 4State Key Laboratory of Advanced Medical Materials and Devices, ShanghaiTech University, Shanghai 201210, China. zhanghan2@shanghaitech.edu.cn 5Shanghai Clinical Research and Trial Center, Shanghai 201210, China. relies on continuing support through early emotional inter- actions with caregivers [4]. In general, assessing empathy in children is critical for understanding key stages of social- emotional development and provides a scientific basis for early interventions to foster healthy emotional and social behaviors. Recent cognitive neuroscience research has increasingly emphasized the study of empathy development in children [5]. However, accurately assessing and predicting empathy levels in children remains a significant challenge. Existing assessments are primarily based on subjective methods, such as self-reports [6], parent surveys [5], and manual annota- tions [7], which lack objectivity and real-time monitoring capabilities. Moreover, current research often treats empathy as a single-dimensional construct, neglecting comprehensive modeling of cognitive and emotional components. While EEG has been employed in empathy prediction studies, these efforts are predominantly focused on adults and are limited to extracting static asymmetry features [8], thereby overlooking critical dynamic information and spatial patterns. To address these challenges, we propose a robust deep learning framework that uses EEG signals to objectively and efficiently assess children‚Äôs empathy levels. Based on evidence that video stimuli evoke ToM and EM in chil- dren aged 3‚Äì12 years [9], and that EEG signals reflect neural correlates of empathetic prosocial behaviors [10], our approach models both dimensions of empathy. EEG signals from ToM and EM events are processed through Large Brain Model (LaBraM) encoder to extract spatial and dynamic temporal representations at global and local levels. ToM and EM, as complementary components of empathy, are further integrated using a feature fusion module that preserves shared features while capturing component-specific characteristics. Additionally, a contrastive learning module is applied to improve cross-subject consistency and enhance prediction performance. The key contributions of this study are summarized as follows: 1) Children Empathy Prediction: To our best knowl- edge, we unprecedentedly leverage EEG signals to predict children‚Äôs willingness to help, a novel classi- fication approach provides new insight into empathy development and may support early interventions in evaluating children‚Äôs prosocial behaviors. 2) Multi-view Framework with Contrastive Learning: We propose a multi-view framework that integrates ToM and EM dimensions from EEG signals through feature fusion for a robust empathy representation. arXiv:2509.06620v1 [cs.LG] 8 Sep 2025 The encoder captures temporal dynamics and preserves spatial information for high-quality feature extraction. Additionally, contrastive learning further enhances per- formance by reducing cross-subject variability. 3) Comprehensive validation: We comprehensively val- idate our proposed method, BEAM, on the ongo- ing Chinese Baby Connectome Project (CBCP) and achieve superior performance compared to other state- of-the-art (SOTA) methods, demonstrating its effec- tiveness in empathy prediction for young children. II. METHOD A. Datasets We validate the proposed method on the CBCP dataset, which includes EEG data from 57 typically developing ch", "response": "Empathy in young children is crucial for their social and emotional development, yet predicting it remains challenging. Traditional methods often only rely on self-reports or observer-based labeling, which are susceptible to bias and fail to objectively capture the process of empathy formation. EEG offers an objective alternative; however, current approaches primarily extract static patterns, neglecting temporal dynamics. To overcome these limitations, we propose a novel deep learning framework, the Brainwave Empathy Assessment Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM leverages multi-view EEG signals to capture both cognitive and emotional dimensions of empathy. The framework comprises three key components: 1) a LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a feature fusion module to integrate complementary information from multi-view signals, and 3) a contrastive learning module to enhance class separation. Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across multiple metrics, demonstrating its potential for objective empathy assessment and providing a preliminary insight into early interventions in children's prosocial development."}
{"prompt": "Title: Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection\n\nIntroduction Event-based anomaly detection focuses on identifying irregularities in events1, a type of machine-generated data that captures system activity in a predefined schema (e.g., time, actor, operation) [1, 5, 17]. These events are ubiquitous in cloud platforms and applications, where they serve as rich sources of data for understanding system behavior. Unlike unstructured logs [3, 14], which contain free-text messages, events such as the one given in Figure 1 are suitable for systematic mining and modeling. Recent studies [7‚Äì9, 13, 15, 29] report that despite interest in log- and event-based anomaly detection, several key challenges remain unresolved and block adoption. Because unsupervised ap- proaches are less accurate than supervised approaches [7, 8] and real-world labeled anomaly data is scarce, users must choose be- tween generating labeled data for their unique system and relying on less powerful unsupervised techniques. In addition to accuracy, existing systems are not interpretable [13] or efficient [7] enough for wide use. If an event is identified as an anomaly, it is crucial 1https://opentelemetry.io/docs/specs/semconv/general/events/ { \"actor\": { \"id\": \"AttrService -InstanceRole -BTDN\", ... }, \"api\": { \"operation\": \"GetInstanceStatus\", \"request.data\": {\"instanceID\": \"i-12345\" , \"asnDesc \": \"AMAZON -AES\"} } } Figure 1: Example of an event in OCSF schema [24]. that this information is available to the user immediately, along with additional context to support further diagnosis and mitigation. Some systems generate billions of events per day, requiring fast and incremental detection techniques. While deep learning approaches fit the requirements on accu- racy [13], they are not interpretable and are computationally ex- pensive. Deep learning approaches take an all-in view on anomaly detection where all of their processing is encoded in an uninter- pretable statistical model. This gives them flexibility because they can be trained on almost any input, but the statistical models might be hiding many sub-problems in anomaly detection that can be tackled with efficient and interpretable algorithmic approaches. Our work identifies event categorization as a sub-problem that can be solved in an efficient and interpretable manner. In event categoriza- tion, the goal is to dynamically generate a set of general equivalence classes that covers the normal events in a system. Our approach is unconventional in that it solves the event categorization problem first and then uses the equivalence classes to detect anomalies: any event that does not fall into a known class is anomalous. We show that the resulting anomaly detection approach is more accurate than existing deep-learning methods. The key to building these equivalence classes is generalizing correctly over entities in the system. For example, imagine a sys- tem where three usernames appear in events, System-Read-QCHXY, System-Read-RBLEA and System-Admin-DFGRD. The two System-Read users can be observed to behave in the same way in event data. If these two users fall into different equivalence classes, a third System-Read user that is created later will always appear as anoma- lous, even if it behaves the same as the other read accounts. The arXiv:2509.06911v1 [cs.SE] 8 Sep 2025 Conference‚Äô17, July 2017, Washington, DC, USA Margarida Ferreira, Victor Nicolet, Luan Pham, Joey Dodds, Daniel Kroening, Ines Lynce, and Ruben Martins pattern we want is System-Read-*. On the other hand, if we gener- alize more, we run into another problem. The pattern System-*-* is too general, because it also captures our admin user, which can be observed behaving in ways that the read users don‚Äôt. We present HyGLAD, which synthesizes regular expression- based equivalence classes from a baseline of normal system behavior recorded in events. A key benefit of our regex-based equivalence model is that the model itself can be directly understood by humans. These benefits carry through to the anomaly detection method, which amounts to matching a few regexes per event (27 MB/s on a single CPU core), and where each anomaly is easily explained by comparing it to our known equivalence classes. Our equivalence classes are a set of filters consisting of regular expressions, where each filter matches events. The regexes in the filters are a generalization of string values that appear in the events: they match at least those values and may additionally match other similar-looking values representing entities that behave similarly. Generalizing solely based on the values themselves would introduce rules that accept potentially anomalous behavior. The key insight of our approach is that events record the underlying relations between entities in a system, and these relations can safely guide the gener- alization over the observed values. In other words, we combine and generalize entities that we observe acting in the same way with respect to other entities. We use this information to", "response": "We propose HyGLAD, a novel algorithm that automatically builds a set of interpretable patterns that model event data. These patterns can then be used to detect event-based anomalies in a stationary system, where any deviation from past behavior may indicate malicious activity. The algorithm infers equivalence classes of entities with similar behavior observed from the events, and then builds regular expressions that capture the values of those entities. As opposed to deep-learning approaches, the regular expressions are directly interpretable, which also translates to interpretable anomalies. We evaluate HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five datasets from real-world systems. The experimental results show that on average HyGLAD outperforms existing deep-learning methods while being an order of magnitude more efficient in training and inference (single CPU vs GPU). Precision improved by 1.2x and recall by 1.3x compared to the second-best baseline."}
{"prompt": "Title: Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents\n\nIntroduction The research paper is the traditional unit of scientific communication. It remains the norm for documenting methods, results, and insights, and is the primary way research is shared with the broader community. However, papers are fundamentally passive objects: a reader must discover the paper (not an easy task given the flood of publications), parse its contributions, and manu- ally determine how to apply them to their own work. In particular, when a paper describes a new computational method, significant technical barriers often remain before the method can be Email: {jcmiao, jamesz}@stanford.edu 1 arXiv:2509.06917v1 [cs.AI] 8 Sep 2025 used on new data [1]. A reader might need to locate the corresponding code repository, install de- pendencies, configure environments, and interpret the correct inputs and outputs [2]. Even with well-maintained repositories, this process is often non-trivial. For instance, consider AlphaGenome, which provides a powerful framework for genome-scale foundation modeling [3]. Despite its utility, this system requires substantial technical expertise to set up and deploy, limiting accessibility for biologists who could otherwise benefit. Using Al- phaGenome in code involves installing the environment, importing multiple modules, creating client objects with API keys, and constructing inputs such as chromosomes, variant objects, and selecting desired output modalities. Users must understand the API hierarchy and parameter semantics, which imposes a learning curve for biologists unfamiliar with these abstractions. This illustrates a broader challenge: research outputs are passively siloed behind technical barriers. Paper2Agent re-imagines research dissemination by turning static papers into active AI agents. Each agent serves as an interactive expert on the corresponding paper, capable of demonstrating, applying, and adapting its methods to new projects. Identify the codebase Remote server A Paper2Agent automatically converts papers into AI agents <Paper>_mcp.py Tool 1 Tool K MCP resources Manuscript MCP prompts MCP tools Instructions for scientific task 1 ‚Ä¶ ‚Ä¶ Input: papers Code repository link Supplements and data Instructions for reproducing figure Instructions for scientific task 2 Connect to any agent or LLM without setup <Paper> Agent User: Apply this paper‚Äôs method to my dataset Paper2MCP Chat with interactive and reliable paper agents Enabling scientific discovery through paper agents ‚Ä¶ B Paper2Agent workflow Input Paper MCP servers Codebase Environment agent Extraction agent Test Remote server Output Paper MCP server python file Configured environment Agent: Absolutely! The analysis results are presented below: Connect with AI agent Paper Agent Implemented tools Testing agent Refine Figure 1: Overview of the Paper2Agent. (A) Paper2Agent turns research papers into interactive AI agents by building remote MCP servers with tools, resources, and prompts. Connecting an AI agent to the server creates a paper-specific agent for diverse tasks. (B) Workflow of Paper2Agent. It starts with codebase extraction and automated environment setup for reproducibility. Core analytical features are wrapped as MCP tools, then validated through iterative testing. The resulting MCP server is deployed remotely and integrated with an AI agent, enabling natural-language interaction with the paper‚Äôs methods and analyses. 2 AI agents are autonomous systems that can reason about tasks and act to achieve goals by leveraging external tools and resources [4]. Modern AI agents are typically powered by large language models (LLMs) connected to external tools or APIs. They can perform reasoning, invoke specialized models, and adapt based on feedback [5]. Agents differ from static models in that they are interactive and adaptive. Rather than returning fixed outputs, they can take multi-step actions, integrate context, and support iterative human‚ÄìAI collaboration. Importantly, because agents are built on top of LLMs, users can interact with agents through human language, substantially reducing usage barriers for scientists. Recent advances highlight the promise of agents for accelerating discovery. For example, the Virtual Lab framework organizes teams of AI scientist agents that collaboratively design and exe- cute research projects across biology and chemistry [6]. Similarly, Google‚Äôs AI co-scientist serves as a virtual collaborator, assisting with hypothesis generation and research proposal development [7]. Sakana AI‚Äôs co-scientist aims for automation of the research lifecycle‚Äîfrom ideation to pub- lication [8]. FutureHouse provides an AI scientist platform designed for diverse scientific tasks [9]. Alongside these general-purpose platforms, specialized agents are also emerging for specific domains [10]. For example, CellVoyager introduces an agentic system for autonomous analysis of single-cell omics data [11]. Biomni is an AI agent for diverse biological tasks [12]. These s", "response": "We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists."}
{"prompt": "Title: Interleaving Reasoning for Better Text-to-Image Generation\n\nINTRODUCTION Unified multimodal understanding and generation models consolidate image/text understanding and image synthesis capabilities within a single foundation model, and have recently emerged as a focal point of interest in the research community (Sun et al., 2024; Team, 2024; Tong et al., 2024; Wu et al., 2025a; Xie et al., 2024; Chen et al., 2025a; Xiao et al., 2025b;a; Liao et al., 2025; Xie et al., 2025b; Wu et al., 2025b; Deng et al., 2025). Representative efforts in this line of research, exemplified by GPT-4o (OpenAI, 2025c), seamlessly integrate comprehension and generation capabilities. This enables a pronounced performance gap relative to existing unified models, particularly in instruction- following for image generation and in the preservation of visual details (Deng et al., 2025). Motivated by recent advances in text-based reasoning, notably test-time scaling techniques for (Multimodal) Large Language Models ((M)LLMs) (Jaech et al., 2024; Guo et al., 2025; Huang et al., 2025; Chen et al., 2025b), a growing body of work has explored whether incorporating such text-based reasoning processes can yield improvements in the fidelity and overall quality of image generation (Fang et al., 2025; Xiao et al., 2025b; Deng et al., 2025; Jiang et al., 2025). These works underscore this perspective and seek to exploit large-scale interleaved text‚Äìimage corpora to learn subtle cross-modal interaction patterns, thus enabling seamless knowledge transfer between the understanding and generation stages of the model. However, in Text-to-Image (T2I) tasks, they employ only a single textual segment as auxiliary supervision in T2I generation, with the objective of producing outputs that more faithfully adhere to the original prompt. Recently, some works in (M)LLM field have focused on the interleaving reasoning, i.e., multi-turn interactions and exhibits sophisticated reasoning dynamics (Huang, 2025), while this reasoning modality has empirical demonstrated superior accuracy in addressing complex problems (OpenAI, 2025b;a). This observation motivates the exploration: Whether interleaving reasoning can further enhance T2I generation quality? As shown in Fig 1, in generating a prompt-aligned image, the model is typically able to produce content that is broadly correct in terms of semantics, yet it remains challenging to attain superior visual quality and fine-grained fidelity (e.g., in rendering textures, shadow realism, and delicate 3 Preprint structures such as fingers). The idea is intuitive: if high-quality image synthesis is considered a hard problem, adopting a multi-step reasoning strategy to tackle it is both reasonable and necessary. To solve this, a straightforward idea is to have the model first produce a text-based thinking process and generate an image based on that reasoning. Then, building on the initial image, the model reflects on how to improve its quality, and produces an improved image through reflection. We denote this process as Interleaving Reasoning Generation (IRG). Thus, we argue two points, 1) an additional text-based reasoning process can serve as auxiliary supervision for image generation, thereby alleviating the difficulty of direct generation, and 2) producing one image that simultaneously attains high visual quality and precise instruction following is non-trivial, whereas a multi-turn generation strategy can incrementally refine the output toward the desired goal. While these positions are aligned with prior reflection-based T2I generation approaches, the key distinction lies in their goals: prior methods (Zhuo et al., 2025; Wu et al., 2025b; Chern et al., 2025) generally employ reflection to rectify major semantic or structural errors in the generated content, with some adopting non-end-to-end frameworks, whereas our approach focuses on leveraging reflection to refine fine- grained details and improve overall visual quality in an end-to-end manner, with the main subject matter established during the initial generation. Specifically, through IRG, we not only enhance the semantic correctness of the generated content, but also focus on improving the quality, fine-grained details, and aesthetic aspects of the generated images. Based on the insights, we firstly select a unified multimodal understanding and generation model as the base model, given its capability to produce interleaved text‚Äìimage outputs. To facilitate interleaving reasoning-based generation with reflection, we propose the Interleaving Reasoning Generation Learning (IRGL) paradigm and formulate the objective as two sub-goals. The first is to strengthen the model‚Äôs initial thinking and generation stage, which establishes the core content and base quality of the generated image. The second is to equip the model with the ability to produce detailed, high-quality text-based reflections, and to generate enhanced images that faithfully implement those reflections. In particular, we propose the IRGL-300K dataset, which refines", "response": "Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation ."}
{"prompt": "Title: Signal-Based Malware Classification Using 1D CNNs\n\n1 Introduction Malware detection remains a critical challenge in cyber-security. While signature based classification is effective for known malware, it relies on handcrafted features which are time-consuming and require expert knowledge to develop (Votipka et al. 2020; Mohaisen et al. 2014). Furthermore, small changes to a malware‚Äôs source code can greatly alter the compiled binary, allowing signature based detection to be readily evaded (Sathyanarayan et al. 2008). While dynamic analysis provides a more in- depth inspection, its resource intensive nature limits its applicability at scale, posing significant challenges for cybersecurity defences that require rapid and accurate malware assessments. Recent advances in machine learning have offered promising avenues for enhanc- ing malware classification. Specifically, the transformation of malware binaries into visual image representations, known as byteplots, has emerged as a novel method that leverages computer vision techniques to classify malware samples based on the byteplot‚Äôs textural information (Conti et al. 2008; Nataraj et al. 2011a). This approach boasts numerous advantages, removing the need to develop hand-crafted features and malware signatures, moreover the malware images are quick to generate allowing these methods to be deployed at scale. Significantly, this approach has enabled deep learning classification models which have been shown to be robust to both polymor- phic malware variants and obfuscation techniques such as section reordering and file packing (Nataraj et al. 2011a,b; A. et al. 2023). However, this approach is not without its limitations. The process of converting malware files into 2D images introduces significant information loss due to quantisation noise when rounding to integer pixel values and, more significantly, the artificial introduction of 2D spatial dependencies, which can distort the original binary structure and degrade the performance of downstream classification models. Addressing these limitations, this work proposes an innovative approach that resizes the malware binaries into 1D signals instead of 2D images. This method not only maintains the integrity of the original data structure, but also retains more of the binary‚Äôs information, improving the signal-to-noise ratio of the resized signal. It is shown that existing 2D models can be adapted to operate on 1D data for improved performance with no additional parameters or computation. Furthermore, a bespoke Convolutional Neural Network (CNN) architecture is proposed to classify the 1D signal data. This architecture is adapted from the preactivation ResNet architecture (He et al. 2016), however, also incorporates squeeze-and-excitation (SE) layers (Hu et al. 2018) and the GELU activation function (Hendrycks and Gimpel 2023). While the proposed pipeline can be applied to arbitrary file binaries, this work focuses on the classification of Android DEX files due to the availability of a large scale dataset for training and evaluation, and the substantial adoption of third party appli- cations in the Android ecosystem. Despite this, the proposed signal-based classification method is experimentally shown to be more effective than the equivalent image- based approaches on Windows‚Äô EXE binaries, making it an attractive replacement to image-based models. Comprehensive evaluation on the MalNet dataset (Nataraj et al. 2011a) demon- strates that the proposed approach not only surpasses 2D image-based models but 2 also achieves state-of-the-art performance in binary, type, and family level malware classification. This work paves the way for new malware classification systems which rely on 1D signal-based representations. The core contributions of this work can be summarised as follows: 1. A resizing approach is proposed which keeps the file binaries in a 1D format, improving the signal-to-noise ratio when compared to resizing as a 2D image. Additionally, by saving the signals in a floating point format the quantisation error is avoided. 2. A procedure to adapt arbitrary 2D CNNs to operate on the 1D signal representations is proposed. The 1D models were compared to their equivalent 2D models in a malware classification task on the MalNet and the Microsoft malware classification datasets. It was found that for an equal number of parameters and compute the 1D models outperform their 2D counterparts. 3. A bespoke 1D CNN model is proposed to classify the malware signals. The model is based on the preactivation ResNet architecture with a GELU activation function and SE layers. The proposed model outperforms state-of-the-art approaches in binary, type, and family classification on the MalNet dataset. This work is organised as follows: related works are outlined in Section 2, includ- ing previous approaches to malware classification, byte plot images, and previous approaches utilising 1D convolutions for malware classification. Section 3 provides overview and explanation of the propos", "response": "Malware classification is a contemporary and ongoing challenge in cyber-security: modern obfuscation techniques are able to evade traditional static analysis, while dynamic analysis is too resource intensive to be deployed at a large scale. One prominent line of research addresses these limitations by converting malware binaries into 2D images by heuristically reshaping them into a 2D grid before resizing using Lanczos resampling. These images can then be classified based on their textural information using computer vision approaches. While this approach can detect obfuscated malware more effectively than static analysis, the process of converting files into 2D images results in significant information loss due to both quantisation noise, caused by rounding to integer pixel values, and the introduction of 2D dependencies which do not exist in the original data. This loss of signal limits the classification performance of the downstream model. This work addresses these weaknesses by instead resizing the files into 1D signals which avoids the need for heuristic reshaping, and additionally these signals do not suffer from quantisation noise due to being stored in a floating-point format. It is shown that existing 2D CNN architectures can be readily adapted to classify these 1D signals for improved performance. Furthermore, a bespoke 1D convolutional neural network, based on the ResNet architecture and squeeze-and-excitation layers, was developed to classify these signals and evaluated on the MalNet dataset. It was found to achieve state-of-the-art performance on binary, type, and family level classification with F1 scores of 0.874, 0.503, and 0.507, respectively, paving the way for future models to operate on the proposed signal modality."}
{"prompt": "Title: Reinforcement Learning Foundations for Deep Research Systems: A Survey\n\nIntroduction 1 2 Data Synthesis & Curation 4 2.1 How RL Training Data Differs from SFT/DPO . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Constructing Complex Queries and Curating Data . . . . . . . . . . . . . . . . . . . . 5 2.3 Classification of Query Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3 RL Methods for Agentic Research 8 3.1 Training Regime and Optimization Structure . . . . . . . . . . . . . . . . . . . . . . . 8 3.1.1 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Reward Design and Credit Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2.1 Outcome-level Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2.2 Step-level Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2.3 Credit Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.3 Multimodal Research Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.3.1 Multimodal Action‚ÄìObservation Interface . . . . . . . . . . . . . . . . . . . . 15 3.3.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4 Agentic RL Training Frameworks 16 4.1 Bottlenecks & challenges in agentic RL training . . . . . . . . . . . . . . . . . . . . . 16 4.2 What the frameworks contribute (methods & features) . . . . . . . . . . . . . . . . . . 17 4.3 How to choose (pragmatic guidance) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 5 Agent Architecture & Coordination 19 5.1 Open-Source Architectures & Coordination . . . . . . . . . . . . . . . . . . . . . . . . 19 5.2 Academic Architectures & Coordination . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.3 RL for Multi-Agent Coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 6 Evaluations 23 6.1 QA and VQA Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 6.2 Long-form Text Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 6.3 Domain-Grounded Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 6.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 7 Conclusion 27 1 INTRODUCTION The rapid emergence of deep research systems (e.g., OpenAI (2025); Google (2025); Perplexity Team (2025)), agentic AI models capable of tackling complex, multi-step information-seeking tasks, marks a significant shift in how AI approaches reasoning, execution, and synthesis. In this survey, we focus on information-seeking use cases, as most existing research and products center on this application. We define deep research systems as agentic AI researchers that autonomously plan and carry out multi-step investigations across the open web and user-provided files, iteratively searching, reading, and reasoning as new evidence appears, and ultimately producing either a concise answer for an objective question or a well-structured, citation-backed report for a subjective open question. A trend in both academia (Li et al., 2025f; Jin et al., 2025b; Wan et al., 2025) and indus- try (ByteDance & contributors, 2025; LangChain & contributors, 2025; MiroMindAI & contrib- utors, 2025) is to move from monolithic agents to hierarchical agent architectures for deep research. Figure 1 mirrors this architecture: a Planner performs step-by-step decomposition and reflection; a Coordinator handles assignment, delegation, aggregation, and verification; and a pool of Executors (i.e., specialized agents and tools) executes grounded actions over the web and files. This separation of concerns decouples strategic planning from execution details, enabling parallelization, plug-and- play expertise (e.g., swapping in better searchers or code runners and scaling to additional tools), and tighter instrumentation for process logging, credit assignment, and auditability. It also keeps the Planner‚Äôs global state clean and coherent over long horizons while the Coordinator and Executors handle delegation and grounded actions. While the hierarchical architecture is attractive for deployment, it is not yet practical to train the entire workflow end-to-end. As a result, most research targets a single model (typically the Plan- ner) wired directly to a small set of fundamental tools (search/browse/code), which collapses rollout length and variance, fits existing RL/SFT/DPO infrastructure, and yields cleaner signals. The train- ing objective is to strengthen long-horizon capabili", "response": "Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases. This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL."}
{"prompt": "Title: Learning words in groups: fusion algebras, tensor ranks and grokking\n\n1. Introduction, contribution 1.1. Background and motivation. Studying the means by which statistical models learn and rep- resent operations on finite sets is receiving quite a lot of attention these days. By an operation we refer to a bi-variate function f : G √ó G ‚ÜíG, where G is a general finite set. While there are many real-world examples which fit into this framework, considerable effort is centered on studying the more tractable case when there is an explicit mathematical formulation which governs the result of the operation. Questions of interest here focus, as usual, on expressibility and interpretability, generalization and phenomenological aspects of the dynamics. As the literature shows, such al- gorithmic setups have proved to be a fruitful soil for reconstructing real-world phenomena, while keeping the overall complexity and analytic tractability of the problem low and high respectively. Perhaps the most natural of such operations, at least from a mathematical point of view, is the multiplication operation of a mathematical group. The simplest case of a cyclic group of order p, a canonical representative of which is Zp := {0, . . . , p ‚àí1} with the operation being addition modulo p, was studied by Power et al in [27]. This worked showed that a simple decoder-only transformer architecture is able to represent and, moreover, learn such an operation based on a a fraction of all p2 examples. Interestingly, the authors observed that during training, both train and test accuracy transitioned very sharply from a trivial level to 100%, with the transition in the test-set lagging behind and occurring well beyond the interpolation threshold. This phenomenon, which the authors termed ‚ÄúGrokking‚Äù was later found to occur in many other architectures and learning tasks (see related work section). In an effort to understand this phenomenon better, Gromov [13] studied the simpler setup of a standard Two Layer Perceptron (henceforth TLP, i.e. an MLP with one hidden layer) and demon- strated that the network still exhibits Grokking given the same task of addition modulo p. More- over, he proposed a ‚Äúsolution-ansatz‚Äù for the weights of the network, composed of Fourier basis 1 Technion - Israel Institute of Technology 2 Weizmann Institute E-mail addresses: maorshut@protonmail.com, oren.louidor@gmail.com, ran.tessler@weizmann.ac.il. The work of O.L. is supported by ISF grant nos. 2870/21 and 3782/25 , and by the BSF award 2018330. The work of R.T. is supported by ISF grant no. 1729/23. 1 arXiv:2509.06931v1 [cs.LG] 8 Sep 2025 LEARNING WORDS IN GROUPS: FUSION ALGEBRAS, TENSOR RANKS AND GROKKING 2 vectors, and showed that this solution achieves asymptotically zero test loss and perfect accuracy. Under his ansatz, the rows of the weight matrices are multiples of real-valued Fourier basis vectors whose frequency is the same for matching rows across all weight matrices. His work then veri- fied empirically that the network converges to this solution under standard first order optimization algorithms, given a partial subset of all examples. Convincing evidence of the convergence to suitable variants of this solution have been presented for a simple transformer-based architectures as well [24] at around the same time. The case of a general group was studied shortly after in [5]. The authors showed that a similar architecture as that in [13] is able to learn and generalize many other groups, including non-cyclic and non-abelian ones. Moreover, they proposed and empirically verified, a generalization of the Fourier-based solution for the general case, using (real versions of) irreducible representations, which are the analogs of the Fourier vectors from the cyclic case. Lastly, they showed that the system exhibits ‚ÄúGrokking‚Äù in the same sense as before. 1.2. Contribution. In this work we go a step further and generalize the class of bi-variate op- erations to that of group words. Given a group G with a multiplication operator ¬∑, a word w is a non-empty string of finite length over the literals a, b, a‚àí1, b‚àí1, which represents an expression involving two arguments a and b. For example, w = aba‚àí1 represents the expression a ¬∑ b ¬∑ a‚àí1. In what follows we identify a word with the bi-variate operation defined by the expression it repre- sents, so that the word in the last example is also the operation w(a, b) := a ¬∑ b ¬∑ a‚àí1. This is clearly a natural extension of the usual group multiplication. Using the same simple TLP model used by Gromov in [13], we first verify that the network is able to learn and generalize arbitrary words and groups and that grokking is still exhibited as before. The affirmative results are summarized in Figure 2. The required fraction of examples and how pronounced the grokking turns out to be, depends on the underlying group and word, as well as on the width of the model. Next, we turn to study this problem theoretically. Our analysis relies on representation theory, as in [5]. However, we also appeal t", "response": "In this work, we demonstrate that a simple two-layer neural network with standard activation functions can learn an arbitrary word operation in any finite group, provided sufficient width is available and exhibits grokking while doing so. To explain the mechanism by which this is achieved, we reframe the problem as that of learning a particular $3$-tensor, which we show is typically of low rank. A key insight is that low-rank implementations of this tensor can be obtained by decomposing it along triplets of basic self-conjugate representations of the group and leveraging the fusion structure to rule out many components. Focusing on a phenomenologically similar but more tractable surrogate model, we show that the network is able to find such low-rank implementations (or approximations thereof), thereby using limited width to approximate the word-tensor in a generalizable way. In the case of the simple multiplication word, we further elucidate the form of these low-rank implementations, showing that the network effectively implements efficient matrix multiplication in the sense of Strassen. Our work also sheds light on the mechanism by which a network reaches such a solution under gradient descent."}
{"prompt": "Title: COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens\n\nINTRODUCTION Large language models (LLMs) have achieved remarkable performance across a wide range of nat- ural language tasks, but their ever-growing parameter counts‚Äîreaching billions to hundreds of bil- lions‚Äîmake deployment expensive in terms of memory, inference time, and energy cost. To broaden access and enable real-world applications such as on-device inference, classroom use, or latency- sensitive systems, it is crucial to compress LLMs while retaining as much performance as possible. Pruning has been a major line of compression work, mainly categorized into depth pruning and width pruning. Depth pruning often removes entire transformer blocks (Kim et al., 2024; Song et al., 2024; Gromov et al., 2025), while width pruning trims hidden dimensions such as FFN channels or attention heads (Ma et al., 2023; Ashkboos et al., 2024; An et al., 2024). However, these approaches are limited in two ways: (i) They prune largely mechanistically, without analyzing where parameters are concentrated within LLMs (embeddings, FFNs, or attention). This blind pruning means that methods that work for large LLMs often fail for small language models (SLMs), where parameter distributions differ. (ii) They ignore the linguistic nature of NLP models: not all tokens are equally important, yet pruning typically treats all tokens as if they contribute equally. These oversights lead to non-robust pruning performance, especially across scales. We begin by systematically analyzing parameter distributions across model families and scales. This reveals a clear pattern: embeddings (vocabulary and unembedding layers) dominate in smaller models, while FFNs dominate in larger models. This explains why prior pruning methods lack ro- bustness across scales‚Äîthey prune the same way regardless of where redundancy actually lies. A second insight comes from the statistics of natural language: token frequencies follow a Zipfian dis- tribution, meaning that rare tokens occur extremely infrequently and contribute little to downstream performance. Removing such rare tokens from the vocabulary reduces embedding size without sig- nificantly affecting accuracy, because language tasks are overwhelmingly driven by common tokens. 1 arXiv:2509.06836v1 [cs.CL] 8 Sep 2025 Preprint These two insights naturally lead to our new approach COMPACT, a simple but powerful pruning framework with two modules: (i) Vocabulary pruning removes rare tokens and shrinks embed- ding/unembedding matrices, directly reducing parameters and memory usage, especially in SLMs. (ii) Common-token‚Äìweighted FFN pruning further reduces redundancy by scoring channels using activations, but weighting only the common tokens that remain valid after vocabulary pruning. To- gether, these two complementary modules address the limitations of prior work: pruning is now guided by parameter distribution, respects the linguistic structure of language tasks, and remains compatible with the standard transformer architecture. We evaluate COMPACT on diverse LLM families (Qwen 2.5, LLaMA 3/3.1/3.2, and Gemma 3) and across scales from 0.5B to 70B parameters. We test on seven downstream benchmarks (MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2020), ARC-C/E (Clark et al., 2018;?), PIQA (Bisk et al., 2020), GSM8K (Cobbe et al., 2021)) and also measure pruning time, inference throughput, and GPU memory usage. Our experiments highlight three phenomena: (i) Scale robustness: COMPACT maintains state-of-the-art performance at high pruning ratios even for small models, avoiding the collapse seen in prior work. (ii) Smooth degrada- tion: Unlike depth pruning, which shows abrupt performance drops, COMPACT degrades gracefully with higher pruning. (iii) End-to-end efficiency: COMPACT‚Äôs pruning yields substantial GPU mem- ory savings and improved throughput. Our contributions are threefold: i) We provide a systematic analysis of parameter distribution across embeddings, FFNs, and attention, revealing scale-dependent redundancy that prior pruning methods overlook. ii) We propose COMPACT, a novel pruning method which is linguistically grounded, scale-adaptive, and structure-agnostic. iii) We demonstrate state-of-the-art pruning results across LLM families and scales, showing superior retention on downstream tasks together with clear gains in pruning time, inference efficiency, and GPU memory usage. 2 RELATED WORK Depth Pruning. One well-explored branch of structured pruning is depth pruning (Kim et al., 2024; Song et al., 2024; He et al., 2024; Lu et al., 2024; Gromov et al., 2025; Chen et al., 2025), which removes entire transformer blocks from the model. Depth pruning is desirable because it only reduces the number of layers in an LLM while still maintaining a standard transformer architecture, allowing the pruned model to be compatible with a wide variety of existing inference frameworks. Shortened LLaMA (Kim et al., 2024) prunes the layers that minimize the incre", "response": "Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency."}
{"prompt": "Title: ToonOut: Fine-tuned Background-Removal for Anime Characters\n\nIntroduction Recent advancements in machine learning have led to significant progress in various image processing tasks, including Dichotomous Image Segmentation (DIS) [15], which classifies image pixels into two categories (e.g., foreground and background). DIS is commonly used for background removal, isolating the salient foreground object for downstream applications like image editing, story creation, or logo design. State-of-the-art DIS models [20, 2, 7] demonstrate impressive capabilities, but their performance is degraded on anime-style images, such as those used at toongether [8] (see Figure 1). Even models specifically designed for anime [9, 16] often yield unsatisfactory 1 arXiv:2509.06839v1 [cs.CV] 8 Sep 2025 Figure 2: Examples of predictions made by ToonOut on images contained in our test datasets. Our datasets cover a variety of characters in challenging poses, their interactions with items, and standalone objects. results. This challenge motivated our work to improve back- ground removal for images of anime characters and items. We aimed to select a popular DIS model and enhance its capabilities on anime content. Our pri- mary contributions are: (1) gathering a high-quality custom dataset of anime images depicting characters and items; (2) fine-tuning the popular BiRefNet model on this dataset, demonstrating a performance that matches the best closed-source models (see Table 1); and (3) we introduce a new metric, Pixel Accuracy, to evaluate fine-grained performance of DIS models. 2 Dataset 2.1 Data sourcing Our dataset consists of 1228 images broken down into train / validation / test with an 80% / 10% / 10% ratio (see Table 3). For each image, we provide both the original RGB image and the corresponding pixel-level ground truth mask. This consists of a grayscale image where black pixels rep- resent background, white pixels represent foreground, and intermediate gray values indicate partially transparent pixels (useful in particular to provide a transparency gradient around character edges). We designed our dataset to meet the following es- sential criteria: ‚Ä¢ Domain coverage: The dataset should com- prehensively represent the target domain, en- compassing diverse anime-style content including both character portraits and object illustrations. To ensure broad generalization, we collected im- ages featuring varied character designs, poses, viewing angles, and activities, grouped in various sub-datasets; ‚Ä¢ Data quality: All images are generations of good quality and high resolution (minimum 1024√ó1024 pixels) to preserve fine-grained details essential for accurate segmentation; ‚Ä¢ Sample diversity: To prevent overfitting and ensure robust evaluation metrics, the dataset presents high diversity. Each image represents a unique combination of character, pose, and con- text, so that we verify that the trained model doesn‚Äôt overfit the train & validation distri- bution. 2 Figure 3: Example of images contained in the dataset, covering different characters, poses and items. We generated images using the anime-specialized checkpoint Yamer‚Äôs Anime [17] of Stable Diffusion XL [12, 3]. Generated images were rigorously fil- tered to ensure high quality: we discarded images with anatomical inconsistencies, unclear foreground- background boundaries, and artifacts that would re- sult in visually unappealing segmentation masks. Following the filtering phase, we evaluated the remaining images using the baseline model BiRefNet to identify challenging cases where the baseline model performs poorly. We prioritized these difficult examples for dataset inclusion, as they represent the most valuable training samples for improving model performance. To maintain dataset balance, we also included up to 20% of images where the baseline BiRefNet model already performs well. 2.2 Dataset Composition To ensure comprehensive domain coverage and train- ing diversity, we structured our data collection around six distinct image categories, each targeting specific vi- sual characteristics and segmentation challenges. The distribution and characteristics of each category are as follows (see Table 3): ‚Ä¢ Reference (neutral face portraits): High-quality character portraits, with neutral emotion; ‚Ä¢ Emotion (close-up portraits): Character close- ups expressing various emotions (joy, anger, sad- ness, etc.). These images are a challenge for base- line models due to fine facial details and hair com- plexity; ‚Ä¢ Pose (full-body characters in motion): Full-body character representations in diverse poses (stand- ing, jumping, running, etc.). Baseline model per- formance is generally better for these samples; 3 Model Open code and weights Open data Performance over our test set (126 images) Pixel Accuracy Mean Boundary IoU Weighted F-measure E-measure S-measure MAE F-measure Photoroom ‚úó ‚úó 99.2% 95.2% 99.3% 99.2% 98.7% 0.04 99.2% Briaai2.0 ‚úì ‚úó 97.8% 92.4% 98.8% 98.8% 97.9% 0.08 98.7% BiRefNet ‚úì ‚úì 95.3% 88.5% 97.8% 97.9% 96.9% 0.15 98.0% ToonOut", "response": "While state-of-the-art background removal models excel at realistic imagery, they frequently underperform in specialized domains such as anime-style content, where complex features like hair and transparency present unique challenges. To address this limitation, we collected and annotated a custom dataset of 1,228 high-quality anime images of characters and objects, and fine-tuned the open-sourced BiRefNet model on this dataset. This resulted in marked improvements in background removal accuracy for anime-style images, increasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric. We are open-sourcing the code, the fine-tuned model weights, as well as the dataset at: https://github.com/MatteoKartoon/BiRefNet."}
{"prompt": "Title: Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives\n\n1. Introduction The ever-evolving urban mobility landscape, particularly in the post-pandemic context, presents ongoing challenges for city planners, policymakers, regulators, and operators. A deeper understanding of travelers‚Äô behavioral responses is essential for effectively managing and adapting to dynamic urban mobility demand. Such Travel Behavior Response (TBR) refers to how individuals adjust their travel choices and decisions in reaction to interventions such as pricing policies, infrastructure changes, demand management strategies, or service interruptions. TBR modeling has been conducted from both short-term and long-term per- spectives. The short-term analysis examines the stable behavioral changes at a certain time Email addresses: yuanwu@kth.se (Yuanyuan Wu), zhenlinq@kth.se (Zhenlin Qin), leizhen.wang@monash.edu (Leizhen Wang), xiaolei@buaa.edu.cn (Xiaolei Ma), zhema@kth.se (Zhenliang Ma) arXiv:2509.06656v1 [cs.LG] 8 Sep 2025 after an intervention (Anupriya et al., 2018; Henn et al., 2011), while the long-term analysis focuses on the temporal modeling of the behavior change process (Zhao et al., 2018; Ma et al., 2020). Traditional travel behavior modeling has primarily relied on statistical methods, machine learning (ML) techniques, and deep learning (DL) approaches to analyze and predict travel choices. However, these approaches struggle to fully capture the dynamic, sequential, and policy-sensitive nature of individual travel responsive decisions. Statistical models, such as discrete choice models (DCM) (Ben-Akiva and Lerman, 1985; Ben-Akiva et al., 2002), provide interpretable frameworks for decision-making but often assume static preferences and lim- ited sequential dependencies over time. ML techniques, including clustering and supervised learning, enhance pattern recognition and prediction accuracy but lack an explicit decision- making framework for adaptive behavior (Wang et al., 2021; Sun et al., 2023). DL methods leverage neural networks, such as recurrent neural networks (RNNs) and transformers (Feng et al., 2018; Cui et al., 2018), to improve temporal sequence modeling but remain largely black-box models with limited policy interpretability. To address these limitations, we model the TBR problem as an Markov Decision Process (MDP), which provides a principled frame- work by formulating travel behavior as a sequential decision-making problem under choice uncertainties. MDPs explicitly incorporate state transitions, action choices, and reward structures, al- lowing for the integration of imitation learning (IL) techniques to infer latent preferences and underlying decision-making strategies. IL is a powerful data-driven Artificial Intelli- gence (AI) technique (Osa et al., 2018) and has emerged as a useful approach for analyzing human choice strategies based on trajectory data from sources such as smart cards, GPS, and cellular calls (Zhao and Liang, 2023; Song et al., 2024). It relies on a dataset of expert trajectories œÑ = (st, at), where st represents the states at time t, and at is the corresponding action taken by the expert. The goal of IL is to learn a policy œÄ(a|s) that mimics the expert‚Äôs behavior as closely as possible. Generally, IL methods are categorized into three types. The basic approach is called Behavior Cloning (BC) (Torabi et al., 2018). In BC, the agent learns a direct mapping from states to expert actions using standard classification or regression techniques as a supervised learning problem. It is simple, but its effectiveness highly depends on the quality of the expert data (Codevilla et al., 2019). The second is Inverse Reinforcement learning (IRL), which aims to extract the value of the choice a people make in a given state s, i.e., reward function R(s, a) (Ng et al., 2000). An RL algorithm can then use this learned reward function to derive the desired policy œÄ(a|s). IRL is particularly useful when the goal is to recover specific reward functions of interest. It has been widely applied in various domains, including identifying food deliveryman‚Äôs route preferences (Liu et al., 2020), personalized route recommendation (Liu and Jiang, 2022), taxi drivers‚Äô routing behaviors (Zhao and Liang, 2023), and valuing travelers‚Äô activity-based choices (Song et al., 2024). However, the two-step nature of IRL, first inferring the reward function and then deriving the optimal policy, makes it computationally intensive. Moreover, it typically demands high-quality expert demonstrations to achieve a reliable performance. The other approach, Generative Adversarial Imitation Learning (GAIL) (Goodfellow et al., 2014), offers the advantages of bypassing intermediate IRL steps and directly learns a policy from the observed data. Inspired by Generative Adversarial Networks (GANs) 2 (Goodfellow et al., 2014), GAIL employs the adversarial training approach with two neural networks: Generator/Policy (G) and Discriminator (D). The task of G is to learn and gener- ate behaviors/actions", "response": "Understanding and modeling individual travel behavior responses is crucial for urban mobility regulation and policy evaluation. The Markov decision process (MDP) provides a structured framework for dynamic travel behavior modeling at the individual level. However, solving an MDP in this context is highly data-intensive and faces challenges of data quantity, spatial-temporal coverage, and situational diversity. To address these, we propose a group-effect-enhanced generative adversarial imitation learning (gcGAIL) model that improves the individual behavior modeling efficiency by leveraging shared behavioral patterns among passenger groups. We validate the gcGAIL model using a public transport fare-discount case study and compare against state-of-the-art benchmarks, including adversarial inverse reinforcement learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results demonstrate that gcGAIL outperforms these methods in learning individual travel behavior responses to incentives over time in terms of accuracy, generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust to spatial variation, data sparsity, and behavioral diversity, maintaining strong performance even with partial expert demonstrations and underrepresented passenger groups. The gcGAIL model predicts the individual behavior response at any time, providing the basis for personalized incentives to induce sustainable behavior changes (better timing of incentive injections)."}
{"prompt": "Title: PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification\n\nIntroduction Graph neural networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009) represent a family of fundamental models designed for learning representations from graph-structured data, which consists of nodes with features and edges representing connections. Given an input graph, GNNs iteratively aggregate node features according to the graph‚Äôs topological structure. These aggregated features are then transformed through non-linear ac- tivation functions to generate hidden representations. Through multiple such aggregation-and-transformation iterations, the final hidden features produced by GNNs serve as comprehensive node-level embeddings, which can be subsequently utilized for downstream tasks including node classification (Scarselli et al., 2009), link pre- diction (Kipf and Welling, 2016), and community detection (Cavallari et al., 2017). Furthermore, graph-level representations can be derived by applying a readout function to these node embeddings, enabling applications such as graph classification (Li et al., 2016). The field of GNNs has experienced rapid architectural evolution in recent years, progressing from traditional spatial (Gilmer et al., 2017; Kipf and Welling, 2017; Hamilton et al., 2017; Veliƒçkoviƒá et al., 2018; Xu et al., 2018; Chen et al., 2020) and spectral (Defferrard et al., 2016; Gasteiger et al., 2019; Chien et al., 2021; Zhu et al., 2021; He et al., 2021; Bianchi et al., 2022) GNNs to more advanced graph transformers (Yuan et al., 2025) and, most recently, graph foundation models (Mao et al., 2024; Liu et al., 2025). These advanced architectures have found widespread application across diverse domains, including recommendation systems (Gao et al., 2023), financial applications (Wang et al., 2022), molecular property prediction (Wieder et al., 2020), drug discovery (Fang et al., 2025), and intelligent transportation systems (Rahmani et al., 2023). The remarkable success of graph neural networks (GNNs) in practical applications has motivated researchers to conduct deeper theoretical analyses, in order to further improve their performance or design novel network architectures. Similar to other neural networks (He and Tao, 2025), theoretical analyses of GNNs typically fall into three categories: expressive power (Zhang et al., 2025; Morris et al., 2024), optimization (Morris et al., 2024), and generalization (Morris et al., 2024; Vasileiou et al., 2025). While expressive power is important, we focus more critically on generalization‚Äîthe ability of GNN models to perform well on unseen data‚Äîas this aspect most directly reflects real-world application requirements. We also remarked that optimization would also be included when studying generalization, since the optimization algorithm used for training GNNs would 1 arXiv:2509.06600v1 [cs.LG] 8 Sep 2025 also be considered. Previous studies analyzing the generalization performance of GNNs have primarily focused on two tasks: graph classification and node classification. Graph classification is often considered theoretically simpler, as each graph is treated as an independent data point sampled from a common distribution. In contrast, node classification presents significantly greater analytical challenges due to the inherent structural dependencies among nodes within a graph. Existing studies (Oono and Suzuki, 2020; Esser et al., 2021; Cong et al., 2021; Tang and Liu, 2023) address this issue by adopting the transductive learning framework (referred to as transductive setting 1 or the problem of estimating the values of a function at given points in Vapnik, 1982, 1998), where test nodes are sampled without replacement from the complete node set. Under this setting, the model has access to features from both training and test nodes during training, while only the training node labels are observed, with the objective of predicting labels for the test nodes. This approach offers significant practical and theoretical advantages. From a practical standpoint, it aligns with the standard practice of training GNNs in real-world application scenarios. Theoretically, this framework simplifies analysis by treating training and test nodes partition as the sole source of randomness, thereby circumventing the need to explicitly handle complex inter-node dependencies. However, this setting presents several limitations. First, it requires the graph structure to remain static‚Äî‚Äìan assumption rarely satisfied in real-world applications. For instance, in social networks, new nodes (users) are continuously added, while existing edges (friendships) may be deleted over time (Feng et al., 2024). Second, the transductive framework only evaluates model performance on observed nodes, providing no theoretical guarantees for new nodes whose features and connections are unavailable during training. These limitations motivate us to develop learning guarantees for GNNs under the inductive learning framework, where the population risk is defined over previously un", "response": "Graph neural networks (GNNs) have achieved remarkable success in processing graph-structured data across various applications. A critical aspect of real-world graphs is their dynamic nature, where new nodes are continually added and existing connections may change over time. Previous theoretical studies, largely based on the transductive learning framework, fail to adequately model such temporal evolution and structural dynamics. In this paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional networks (GCNs) for inductive node classification, treating nodes as dependent and non-identically distributed data points. We derive novel generalization bounds for one-layer GCNs that explicitly incorporate the effects of data dependency and non-stationarity, and establish sufficient conditions under which the generalization gap converges to zero as the number of nodes increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal that it requires stronger assumptions on graph topology to guarantee convergence. This work establishes a theoretical foundation for understanding and improving GNN generalization in dynamic graph environments."}
{"prompt": "Title: Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification\n\nIntroduction Large Language Models (LLMs) are emerging as powerful interfaces for accessing knowledge in domains ranging from healthcare and finance to economics and international development. Their fluency makes them attractive to a wide range of users‚Äîfrom policymakers and re- searchers to clinicians, financial analysts, and the public‚Äîbut their usefulness is constrained by their stochastic nature: they may generate numeric hallucinations. Even when given correct input, LLMs may still produce plausible but incorrect val- ues‚Äîsometimes citing the right dataset while presenting the wrong figure (Ji et al., 2023; Banerjee et al., 2024; Xu et al., 2025; Kalai et al., 2025). For example, Wu et al. (2025a) showed that when provided a perturbed drug dosage, an LLM sometimes ‚Äúcorrected‚Äù it to a different value. In another case, a model might state that the Philippines‚Äô GDP growth in 2024 was 6% when the official figure published by The World Bank (2025) was 5.7%. Small deviations like these can erode trust and cascade into flawed medical guidance, misinformed policy, or reputational risks for institutions. ‚àóGitHub/HF: @avsolatorio, avsolatorio@gmail.com 1 arXiv:2509.06902v1 [cs.CL] 8 Sep 2025 PCN presentation-layer pipeline User Query q LLM Generator parse q, plan, generate Retriever resolve claims C Verifier apply policy Œ† User Interface: ‚úìverified (marked with provenance) ‚àÖunverified (no mark by default) retrieve(q) claims C tokens / numbers y verified / unverified Stat DB/MCP Fin API EHR Claim-bound token <claim id=\"CID\" policy=\"P\">VAL</claim> Bare numbers ‚áíunverified by default Policies Œ†: exact, normalized, alias, proto R(t, c; Œ†) = 1 ‚áíverified, else unverified Proof-Carrying Numbers (PCN) Protocol in LLM Applications Figure 1: PCN-compliant architecture with LLM-initiated retrieval. The LLM first parses the query and requests claims from the retriever (dashed top lane); the retriever returns the claim set C on a separate solid lane. The LLM emits claim-bound tokens (or bare numbers). The verifier checks tokens under policy Œ†, and the UI renders verified values with provenance marks; absence of a mark implies unverified by default. External structured data sources feed the retriever via parallel dashed feeders. Existing safeguards only partially address this problem. Retrieval-augmented generation (Lewis et al., 2020) grounds answers in source text, while citations and attribution frame- works (Wu et al., 2025a; Zhang et al., 2025) increase transparency. However, both remain probabilistic: users often assume a cited number is faithful even when it has been misquoted or fabricated (Wu et al., 2025b; Hakim et al., 2025). Similarly, uncertainty estimation (Man- akul et al., 2023) and self-verification approaches can flag suspicious values but offer no binding guarantee. We argue that numeric hallucination is best understood as a presentation-layer prob- lem. Even when authoritative claims are retrieved, LLMs are unreliable at reproducing values faithfully (Banerjee et al., 2024; Xu et al., 2025), and user interfaces lack systematic safeguards against drift or fabrication. To address this gap, we propose Proof-Carrying Numbers (PCN), a protocol that requires every displayed number to be bound to an authoritative claim and verified be- fore presentation. Loosely inspired by proof-carrying code (Necula, 1997), PCN embeds verifiability directly into the interface: verified numbers carry explicit provenance, while unverifiable ones are blocked, flagged, or corrected. Our contributions are threefold: 1. We reframe numeric hallucination as a presentation-layer problem, showing why ex- isting safeguards (retrieval, citations, uncertainty estimation) cannot provide bind- ing guarantees. 2. We design the Proof-Carrying Numbers (PCN) protocol, specifying a claim schema, token syntax, and verifier policies that enforce a fail-closed contract at display time. 3. We show how PCN wraps inherently fallible LLM outputs in a deterministic con- tract: numeric spans are either Verified against authoritative claims with prove- nance, remain Bare if unclaimed, or are Flagged when verification fails. By embedding verification into the presentation pipeline, PCN bridges the gap between LLM fluency and the trustworthiness required in high-stakes numeric applications. 2 Figure 2: Implementation of the Proof-Carrying Numbers (PCN) protocol in an LLM chat application. The LLM retrieves authoritative claims via an MCP server; the verifier applies PCN policies; and the interface renders verified numbers with explicit badges. 2 Background and Related Work Hallucination in large language models (LLMs)‚Äîthe generation of fluent but incorrect con- tent‚Äîposes serious challenges across domains (Ji et al., 2023; Banerjee et al., 2024; Xu et al., 2025). Of particular concern is numeric hallucination, where even small deviations (e.g., re- porting 6.0% instead of 5.7%) can undermine high-stakes applications in policy, healthcare, and finance (Kim e", "response": "Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \\emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \\emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \\emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty."}
{"prompt": "Title: AnalysisGNN: Unified Music Analysis with Graph Neural Networks\n\nIntroduction Symbolic music analysis is a cornerstone of music information retrieval and musi- cology. Traditional methods for harmonic analysis, cadence detection, and phrase segmentation often rely on rule-based systems or statistical models. While recent deep learning approaches have improved performance by leveraging relatively large annotated datasets or self-supervised pretraining, they typically treat each analysis problem separately, thereby missing the inherent interdependencies in musical structure. Multi-task learning offers a promising direction to exploit cross-task knowledge transfer‚Äîfor example, features learned during harmonic analysis may enhance cadence detection‚Äîbut in practice, available datasets focus on single musical elements (harmony, cadence, or hierarchical structure) and suffer from inconsistent ‚ãÜ* Equal contribution. arXiv:2509.06654v1 [cs.SD] 8 Sep 2025 2 E. Karystinaios et al. annotations. To address these challenges, we replace task-incremental pipelines with a unified data-shuffling strategy: during training, mini-batches are sampled across all tasks, with a custom weighted multi-task loss that balances each objective, and logits from task-specific classifiers are fused to reinforce shared representations. Additionally, we incorporate a Non-Chord-Tone prediction branch that filters out passing and non-functional notes before downstream tasks, yielding cleaner input signals and reducing conflicting labels. Graph Neural Networks (GNNs) have emerged as a powerful solution for modeling the complex, non-sequential relationships inherent in music scores. For example, ChordGNN [9] represents notes as nodes in a graph to capture Roman numeral harmony, achieving competitive results compared to previous approaches. Similarly, graph-based methods have improved cadence detection by directly leveraging musical context. Our proposed framework, AnalysisGNN, builds on these ideas by combining a data-shuffling training regimen with weighted multi-task optimization and inter-classifier logit fusion, alongside a non-functional note exclusion mechanism. This design addresses the limitations of fragmented, specialized datasets and isolated models through a single, cohesive Graph Neural Network. The resulting system achieves good performance on each analysis task, including cadence detection, Roman numeral analysis, section and phrase identification, and metrical position estimation, while also maintaining robustness against domain shifts and annotation inconsistencies. Our contributions are four-fold: 1. We propose a training strategy and architecture for heterogeneous multi-task symbolic music analysis.. 2. We introduce a number of new music analysis tasks such as Non-Chord-Tone prediction module that identifies and filters passing and non-functional notes. 3. We assemble and preprocess the largest compilation of heterogeneously anno- tated symbolic music datasets, unified into a graph representation suitable for GNN processing. 4. We demonstrate that AnalysisGNN achieves competitive performance while exhibiting strong resilience to domain shifts and annotation variability. 2 Related Work 2.1 Harmonic Analysis (Roman Numeral Labeling) Automatic analysis of functional harmony has long been studied in the fields of music information retrieval and music theory. Early systems employed rule-based grammars, probabilistic models, dynamic programming, and grammar induction for harmonic and metrical analysis of music [19,22,23]. The advent of deep learning brought significant improvements. Micchi et al. [15] trained one of the first neural networks (a convolutional re- current model) for Roman numeral analysis, and N√°poles L√≥pez et al. [18] later AnalysisGNN: Unified Music Analysis with Graph Neural Networks 3 introduced AugmentedNet, a CNN-LSTM architecture that enhanced perfor- mance via data augmentation and multi-task learning. Subsequently, two more systems using multitask approaches for Roman numeral analysis were introduced using techniques to mitigate the interdependency between the Roman numeral prediction subtasks [14,16]. Despite these advances, purely sequential models struggle with the inherent complexity of polyphonic scores‚Äîthey require the score to be serialized or binned into time slices, which risks losing voice-leading details and long-range dependencies. This limitation has motivated the use of graph neural networks. ChordGNN [9] constructs a graph of all notes in a piece and applies graph convolution to aggregate note features into chord predictions. It treats Roman numeral analysis as a multi-task classification problem (predicting multiple components of the Roman numeral label) similar to previous approaches and introduces an edge contraction algorithm to pool information from note-level to chord-level representations. As a result, ChordGNN achieved high performance on Roman numeral analysis, outperforming previous CNN/CRNN models on standard datasets. The most recent approach by", "response": "Recent years have seen a boom in computational approaches to music analysis, yet each one is typically tailored to a specific analytical domain. In this work, we introduce AnalysisGNN, a novel graph neural network framework that leverages a data-shuffling strategy with a custom weighted multi-task loss and logit fusion between task-specific classifiers to integrate heterogeneously annotated symbolic datasets for comprehensive score analysis. We further integrate a Non-Chord-Tone prediction module, which identifies and excludes passing and non-functional notes from all tasks, thereby improving the consistency of label signals. Experimental evaluations demonstrate that AnalysisGNN achieves performance comparable to traditional static-dataset approaches, while showing increased resilience to domain shifts and annotation inconsistencies across multiple heterogeneous corpora."}
{"prompt": "Title: Outcome-based Exploration for LLM Reasoning\n\nIntroduction Large language models (LLMs) are commonly post-trained with reinforcement learning (RL), both in prefer- ence alignment (Ouyang et al., 2022; Bai et al., 2022) and in reasoning (Shao et al., 2024; Guo et al., 2025). A longstanding difficulty in RL is the design of reward signals: while one might hope to shape intermediate rea- soning steps, recent works have shown that the seemingly crude strategy of rewarding only the final correctness (e.g., whether a math answer is correct) can be remarkably effective (Shao et al., 2024; Guo et al., 2025). However, a growing body of evidence points to an important drawback of RL post-training: a systematic loss of diversity in model generations (Song et al., 2024b; Dang et al., 2025; Yue et al., 2025; Zhao et al., 2025; Wu et al., 2025). This phenomenon is most cleanly captured by the pass@k metric: when k is large (say k = 512), post-trained models exhibit a lower pass@k than the base model. This raises a practical concern: in real-world deployments, diversity is often valuable and can amplify performance through test-time scaling (Wu et al., 2024; Snell et al., 2024), with different sampling processes such as directly sampling from the model or tree search. Indeed, we find that diversity degradation already manifests during training, as models collapse to a reduced set of candidate answers on unsolved problems due to a transfer effect of the diversity degradation induced by concentrating on correct answers, which we detail in Section 2. Exploration is the canonical RL tool for combating such collapse (Bellemare et al., 2016; Azar et al., 2017; Burda et al., 2018). However, directly importing classical techniques such as Upper Confidence Bound (UCB) exploration (Auer et al., 2002) to token-level language modeling is intractable, as it would require searching over exponentially many sequences. Motivated by the success of outcome-based rewards, we therefore study outcome-based exploration, where exploration bonuses depend only on final outcomes. This perspective allows us to adapt UCB-style methods to LLM training, which we further refine by incorporating both positive and negative outcome signals. A subtlety arises, however: in language models, one must distinguish between historical exploration (visiting a more diverse set of states and actions during training) and batch exploration (producing diverse outputs at test time). The latter improves pass@k but does not necessarily increase diversity during training whereas the former improves pass@1 but does not guarantee test-time diversity of the trained model. We introduce and study a batch version of outcome-based exploration, which demonstrates improved tradeoff between accuracy and diversity during test time. Our contributions 1. We study RL post-training dynamics by framing RL as a sampling process (Section 2). This perspective reveals that diversity loss is not limited to test-time behavior, but already occurs on the training set: as RL concentrates probability mass on previously solved questions, the resulting collapse propagates and reduces diversity even on unsolved ones. We term this effect the transfer of diversity degradation. 2. We propose outcome-based exploration (Section 3), which adapts classical exploration bonuses (e.g. UCB) to the outcome space of LLM tasks. We show that naively adapting UCB does not lead to improved testing performance. We thus propose more refined algorithms (UCB-Mean, UCB-Con) which incorporate both positive and negative signals, and show that they improve both training exploration and test generalization, on standard reasoning dataset such as DAPO and models such as Llama-3.1-8B-Instruct. We further provide a theoretical analysis on the benefit of outcome-based exploration in a new bandit setting (outcome-based bandits), inspired by the practical considerations (Section 4.2). 3. We introduce a batch version of the outcome-based exploration algorithm (Batch) (Section 3.3). By penalizing repetitive answers within the latest samples, the algorithm explicitly encourages diverse generations on the batch level, yielding a better accuracy‚Äìdiversity tradeoff at test time. 4. We analyze the interaction between historical and batch exploration, showing that they are not mutually exclusive (Section 4.1). In summary, our proposed methods can be easily incorporated into standard RL for LLMs reasoning training, agnostic to the training algorithm, and consistently improve both accuracy and diversity. 2 2 Diversity Degradation: RL as Sampling 2.1 Preliminaries We consider LLM reasoning training with RL in a verifiable reward setting. Denote the set of questions as X, the training question set Xtrain ‚äÜX and the test question set Xtest ‚äÜX. Further, define the space of intermediate text as Y, and the answer space as A; we consider an LLM to be a policy œÄ : X ‚Üí‚àÜ(Y √ó A), i.e, given any question x ‚ààX, the LLM generates a sample (y, a) ‚àºœÄ(¬∑ | x), where y ‚àºœÄ(¬∑ | x) is the intermediate reasoning", "response": "Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment."}
{"prompt": "Title: HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models\n\nIntroduction Large language models (LLMs) are increasingly deployed in retrieval-augmented [1] and long-context scenarios where correct answers should be directly grounded in supplied evidence. Yet hallucinations persist: even when attention patterns highlight relevant spans, the generated output can drift away from the facts. This gap arises because attention does not perfectly reflect how internal signals shape next-token predictions, leaving faithful decoding an open challenge. Prior work [2] has explored leveraging attention maps to guide decoding, but two fundamental issues remain. First, head importance is typically treated as static, despite clear variation across domains and inputs. Relying on fixed arXiv:2509.06596v1 [cs.CL] 8 Sep 2025 Running Title for Header head weights underutilizes context-sensitive heads and amplifies noisy ones. Second, raw attention is a poor proxy for contribution to the residual stream update that ultimately determines token probabilities. Tokens that are frequently ‚Äúread‚Äù may exert little influence, while ‚Äúsink‚Äù tokens such as special or whitespace symbols can dominate. These limitations hinder the reliability of attention-based evidence. We propose HAVE (Head-Adaptive Gating and ValuE Calibration), a single-step, parameter-free framework for faithful decoding. HAVE is efficient: it requires only the standard forward pass of the base model, introduces no new parameters, and is robust to grouped-query attention and sliding-window key‚Äìvalue caches. Moreover, its internal signals‚Äîdynamic head weights and calibrated token scores‚Äîare interpretable and can be visualized or ablated, providing diagnostic value beyond performance gains. Specifically, this work makes the following contributions: ‚Ä¢ We introduce a new perspective on decoding-time evidence, showing that raw attention is insufficient and proposing to calibrate it with value signals that directly approximate contribution to the residual stream. ‚Ä¢ We develop HAVE, a parameter-free framework with two novel modules: (i) Head-Adaptive Gating, which performs instance-level soft reweighting of attention heads, and (ii) Value Calibration, which debiases sinks and augments attention with value norms to produce contribution-aware token evidence. ‚Ä¢ We provide a general and efficient implementation that operates in a single forward pass and is compatible with modern architectures (grouped-query attention, sliding-window caches), while exposing interpretable internal signals (dynamic head weights, calibrated token scores). Through comprehensive experiments on QA benchmarks and across LLM families, HAVE achieves consistent im- provements over strong attention-guided baselines, establishing new state-of-the-art in several settings with modest overhead. By combining interpretability with effectiveness, HAVE offers a practical step toward trustworthy LLM generation in retrieval-augmented and long-context settings. 2 Related Work Across the entire pipeline‚Äîfrom user input to model output‚Äîfactors such as input data, model training, and inference strategies all play critical roles in shaping factual accuracy. To address these challenges, prior research has explored three major categories of enhancement methods: training-based improvements, input-based strategies, and output-level optimizations. Training-based enhancements have emerged as a prominent area of focus in improving factual accuracy. These approaches incorporate augmentation mechanisms during pre-training [3], fine-tuning [4], and preference alignment [5]. By adjusting the model‚Äôs memory weights, they aim to maintain generative fluency while simultaneously ensuring greater factual consistency. Another line of research leverages the emergent properties of LLMs, including in-context learning, instruction following, and chain-of-thought reasoning. Prompt engineering, in particular, has been shown to improve factual robustness. For example, carefully designed prompts can encourage models to assess their own confidence and perform self-correction [6]. Similarly, in-context demonstrations can provide models with updated examples, helping them maintain critical facts while filtering out irrelevant information [7]. Such strategies effectively enable ‚Äútemporary‚Äù knowledge updates without modifying model parameters. However, their reliability in handling complex, dynamically evolving knowledge domains or multi-step reasoning tasks still requires further validation. Complementary to these methods, the Retrieval- Augmented Generation (RAG) framework [1] strengthens factuality by integrating external knowledge bases through indexing, retrieval, and generation. Finally, output optimization strategies aim to refine factuality during or after the decoding process. One class of methods optimizes the decoding process of LLMs, enhancing the factual consistency of their outputs without modifying the model parameters. CAD [8] computes both a ‚Äúcontext-aware‚Äù output distribution and a ‚Äúcontext-free", "response": "Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings."}
{"prompt": "Title: Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet\n\nIntroduction Recent reasoning models, such as such as GPT-5 [20], Gemini 2.5 [9] and Qwen3 [28], have demonstrated impressive performance on many challenging tasks, like competition-level math prob- lems [6, 12, 25, 26, 29, 37]. A key technique behind these improvements is test-time scaling, where models generate long chain-of-thought (CoT) reasoning traces before producing an answer [19]. Despite these advances, frontier models still suffer from hallucinations, responses that contradict world knowledge [2, 11, 30]. This remains a fundamental challenge, especially in knowledge- intensive tasks that require models to ensure factual accuracy and minimize hallucinations [14, 32]. Given that test-time scaling has shown promise across many domains, a natural question arises: Is test-time scaling effective for knowledge-intensive tasks? To answer this question, we conduct a comprehensive study of test-time scaling on two knowledge- intensive tasks. We evaluate 12 reasoning models by increasing their test-time computation (Section 3). Our findings, summarized in Table 1, challenge the common assumption that thinking more leads to better performance. Across models and tasks, increasing thinking time does not consistently improve accuracy, with Gemini 2.5 Flash being the only exception. Moreover, thinking more does not reduce hallucinations for most models and even increases them, such as in GPT-5 mini and Gemini 2.5 Flash. To understand hallucination changes with increased test-time computation, we analyze how model behavior shifts across different thinking levels (Section 4). We find that these changes are largely driven by the model‚Äôs willingness to answer. Specifically, reduced hallucinations are primarily due to ‚àóCorrespondence to: James Xu Zhao (xu.zhao@u.nus.edu) Preprint. arXiv:2509.06861v1 [cs.AI] 8 Sep 2025 Table 1: Summary of model behavior with test-time scaling. Increasing test-time computation does not consistently improve accuracy or reduce hallucinations for most models. It can even increase hallucinations for several models. For ACCURACY, ‚Üëdenotes consistent improvement with >2% accuracy gains across consecutive reasoning levels, while ‚àºindicates no consistent trend. For HALLUCINATION, ‚Üìdenotes consistent reduction with >2% hallucination decrease, ‚Üëindicates degradation with >2% hallucination increase, and ‚àºreflects inconsistent or fluctuating patterns. Metric GPT-5 mini o3-mini o4-mini gpt-oss-20b Grok-3 mini Gemini 2.5 Flash ACCURACY ‚àº ‚àº ‚àº ‚àº ‚àº ‚Üë HALLUCINATION ‚Üë ‚Üë ‚àº ‚Üë ‚Üì ‚Üë Metric Claude Sonnet 4 R1-Distill- Qwen-7B R1-Distill- Qwen-14B R1-Distill- Llama-8B Qwen3-8B Qwen3-14B ACCURACY ‚àº ‚àº ‚àº ‚àº ‚àº ‚àº HALLUCINATION ‚àº ‚àº ‚Üì ‚àº ‚àº ‚àº simple abstention, rather than improved factual accuracy. Conversely, when hallucinations increase, it is often because extended reasoning leads the model to attempt previously unanswered questions. Through case studies on gpt-oss-20b, we observe signs of confirmation bias [18], where the model fabricates details to support its prior belief, resulting in overconfident hallucinations. Given that increasing thinking time does not reliably improve factuality, we ask a follow-up question: Is thinking helpful, compared to non-thinking? In Section 5, we evaluate models that natively support both thinking and non-thinking modes. Our results show that enabling the model to ‚Äúthink‚Äù before answering still offers benefits. Firstly, it improves accuracy on knowledge-intensive tasks, particularly those requiring multi-hop reasoning. Secondly, it reduces hallucinations for most models, with Gemini 2.5 Flash again being an exception. To summarize, while test-time scaling in reasoning models has led to strong performance in many domains, it is not yet effective for knowledge-intensive tasks. Increasing inference time does not con- sistently improve factual accuracy, and contrary to expectations, it can even increase hallucinations. 2 Related Work 2.1 Test-Time Scaling Test-time scaling has emerged as a promising strategy for enhancing the capabilities of large lan- guage models. It is typically categorized into two main paradigms [38]: (1) the parallel approach, which samples multiple outputs independently and aggregates them [4, 27]; and (2) the sequential approach, where the model generates long chain-of-thought (CoT) reasoning traces before producing an answer [17, 19, 31]. In this work, we focus on the sequential paradigm, which has become the dominant test-time scaling method for improving model performance. It is widely adopted in frontier reasoning models [1, 9, 10, 20, 28, 35], and has demonstrated strong performance across a range of challenging tasks [12, 25, 26, 29]. However, recent studies suggest that in some tasks, increasing test-time computation does not necessarily bring better performance. Gema et al. [8] reveal inverse scaling effects and identify multiple failure modes, where longer reasoning may reinforce problematic patterns rather than improve accuracy. Liu et al. [16] show", "response": "Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge"}
{"prompt": "Title: Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems\n\nI. INTRODUCTION D YNAMIC nonlinear systems are widely encountered in signal processing, communications, and control applica- tions [1], [2]. Modeling real-world dynamic nonlinear systems is challenging due to time-varying interactions among mem- ory, nonlinearities, and external influences. A memoryless non- linear system can be described by Taylor expansion or static regression, while a linear memory system can be approximated as time-variant and treated as an linear time-invariant (LTI) system. Dynamic nonlinear systems combine both effects, producing time-varying distortions that cannot be captured by S. S. Krishna Chaitanya Bulusu is with the Centre for Wireless Commu- nications (CWC), University of Oulu, 90570 Oulu, Finland. (Corresponding author: S. S. Krishna Chaitanya Bulusu (email:sri.bulusu@oulu.fi)) Mikko. J. Sillanp¬®a¬®a is with the Department of Mathemat- ical Sciences (DMS), University of Oulu, 90570 Oulu, Fin- land. ((email:mikko.sillanpaa@oulu.fi)) superposition or impulse responses, and their behavior strongly depends on structure and operating conditions [3]. Further complexity arises from practical factors such as thermal drift, aging, bias instability, and environmental variations (e.g., temperature and mechanical stress), which continuously alter system response beyond static or deterministic models [4]. There is no universally accepted model for dynamic nonlin- ear systems, as different approaches balance accuracy, inter- pretability, efficiency, and scope. Common techniques include Volterra-based models [3], block-structured models [5], neural network models [6], nonlinear auto-regressive moving average (NARMA)-based methods [7], and Kalman filter approaches [8]. Different models are preferred based on the specific system characteristics and constraints. In wireless communi- cations, Volterra and neural network models are widely used to capture amplifier nonlinearities [9]. In robotics and adaptive control, Kalman filters and NARMA models support real- time identification [10], [11]. In biomedical signal processing, block-structured models are preferred for their physiological interpretability [12], [13]. This diversity underscores the chal- lenge of accurately representing dynamic nonlinear systems across operating conditions using a single model. Conventional block-structured models such as Wiener, Hammerstein, and Wiener‚ÄìHammerstein approximate dy- namic nonlinear systems by combining LTI components with static nonlinearities [3]. These models assume strict separation between nonlinear effects and memory, an assumption often violated in practice due to environmental and operational vari- ations [14]. To address such limitations, artificial intelligence (AI)-based frameworks have emerged as adaptive, data-driven alternatives. Unlike conventional models with fixed structures, machine learning methods learn directly from data, enabling flexibility where memory and nonlinearity are tightly coupled. However, their high computational cost remains a bottleneck, particularly in hardware-constrained scenarios [15]. Our earlier work introduced an AI-based block-structured framework for modeling dynamic nonlinear systems, demon- strated on power amplifiers in communication systems [16]. A PA exemplifies a dynamic nonlinear system, with nonlineari- ties arising from static effects such as AM‚ÄìAM and AM‚ÄìPM conversion, and dynamic effects including thermal variation, bias drift, and memory [17]. The framework in [16] employs two blocks: a static nonlinear block and a neural network block. By isolating the dynamic residual and modeling only this component with a neural network, the method adapts to time-varying conditions while reducing training and inference complexity. This selective learning strategy effectively com- pensates for the nonlinear memory without modeling the entire distortion space. Experimental validation on a real PA showed arXiv:2509.06599v1 [cs.LG] 8 Sep 2025 THIS WORK SHALL BE SUBMITTED TO IEEE TRANSACTIONS FOR CONSIDERATION 2 superior performance over conventional approaches such as those in [18]. Unlike the classical progression where theoretical results precede experimental validation, this work originated from an empirical anomaly. Our apparent empirical success raised a deeper theoretical question: Can we rigorously prove why structured residual learning works effectively in real-world dynamic nonlinear systems? This question served as the primary motivation for the present work. In seeking its resolution, we uncovered several valuable theoretical insights, which we now formalize and present in this paper. The key contributions of this work are summarized as follows: ‚Ä¢ Deviation-induced Orthogonality Bound (DOB): We establish the DOB theorem in Appendix A, which pro- vides directional lower bounds on deviations in inner product spaces under power dominance. This result gen- eralizes classical orthogonality and forms the theoretical basis for variance and memory analysis.", "response": "Dynamic nonlinear systems exhibit distortions arising from coupled static and dynamic effects. Their intertwined nature poses major challenges for data-driven modeling. This paper presents a theoretical framework grounded in structured decomposition, variance analysis, and task-centric complexity bounds. The framework employs a directional lower bound on interactions between measurable system components, extending orthogonality in inner product spaces to structurally asymmetric settings. This bound supports variance inequalities for decomposed systems. Key behavioral indicators are introduced along with a memory finiteness index. A rigorous power-based condition establishes a measurable link between finite memory in realizable systems and the First Law of Thermodynamics. This offers a more foundational perspective than classical bounds based on the Second Law. Building on this foundation, we formulate a `Behavioral Uncertainty Principle,' demonstrating that static and dynamic distortions cannot be minimized simultaneously. We identify that real-world systems seem to resist complete deterministic decomposition due to entangled static and dynamic effects. We also present two general-purpose theorems linking function variance to mean-squared Lipschitz continuity and learning complexity. This yields a model-agnostic, task-aware complexity metric, showing that lower-variance components are inherently easier to learn. These insights explain the empirical benefits of structured residual learning, including improved generalization, reduced parameter count, and lower training cost, as previously observed in power amplifier linearization experiments. The framework is broadly applicable and offers a scalable, theoretically grounded approach to modeling complex dynamic nonlinear systems."}
{"prompt": "Title: UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward\n\nIntroduction Image customization, which aims to create images that simultaneously adhere to the semantic content of textual prompts and the visual appearance of reference images, has garnered significant research attention in recent years. Among various subjects, the customization of human identities (i.e., ID) has attracted particular interest due to its broad range of real-world applications, such as personalized film production and virtual avatar creation. Different from other subjects, humans are exceptionally sensitive to identity customization, i.e., even subtle discrepancies in appearance can lead to a noticeable loss of identity fidelity, thereby making human ID customization significantly more challenging. This challenge is further amplified when multiple identities need to be customized simultaneously, as it requires the model not only to preserve the unique characteristics of each individual ID, but also to maintain clear distinctions among them within the generated images. * Corresponding author ‚Ä† Project lead. 1 arXiv:2509.06818v1 [cs.CV] 8 Sep 2025 Single Identity Two Identities Two Identities + Subject Single Identity + Subject More Identities Figure 1 Showcase of our UMO model in different scenarios. The detailed prompts are listed in Table 9. Existing multi-identity customization methods primarily focus on constructing improved multi-identity paired data to enhance the consistency of multiple identities. For example, DreamO [18] and OmniGen [33] build large-scale training datasets for identity customization, including multi-identity paired data derived from either video sources or in-context image generation. Meanwhile, some recent approaches aim to mitigate confusion between multiple identities by employing identity masks to explicitly constrain the location or 2 UMO Single Identity Multi-Identity Interaction Multi-Identity Stylization Multi-Identity Portrait Figure 2 Our UMO unleashes multi-identity consistency and alleviates identity confusion. Existing image customization methods suffer low facial fidelity and severe identity confusion, while UMO can tackle these problems with results in blue boxes. position of each identity. For instance, MS-Diffusion [29] introduces a layout-guided mechanism to explicitly control the generation location of each identity. RealCustom++ [13, 17] further proposes to explicitly separate the influence masks of each identity in order to disentangle their respective generations. In summary, existing methods mainly adopt a one-to-one mapping paradigm, in which focuses on learning a direct correspondence between each identity in the reference image and the corresponding generated one. In this study, we argue that the existing one-to-one mapping paradigm fails to comprehensively address both intra-ID variability and inter-ID distinction, leading to increased identity confusion and reduced identity similarity as the number of identities grows. On the one hand, intra-ID variability refers to the inherent variability within a single identity, where the same individual may present different attributes (e.g., poses, expressions, etc.) between the reference image and the generated output. On the other hand, inter-ID distinction underscores the importance of not only accurately capturing the distinctive characteristics of the target identity during generation, but also explicitly suppressing the features associated with other identities, thereby ensuring clear separation and minimizing identity confusion in multi-identity scenarios. As the number of identities increases, the risk arises that the distinctions between different identities may become less salient, potentially approaching the degree of variability observed within a single identity. Therefore, by focusing exclusively on the one-to-one mapping between each identity and its corresponding reference, existing methods overlook the increasing overlap between intra-ID variability and inter-ID distinction as the number of identities grows. This limitation fundamentally restricts their scalability, as they are unable to effectively preserve clear identity boundaries in large-scale multi-identity scenarios. To address the challenge, we propose a novel multi-to-multi matching paradigm, whose key idea is to reformulate multi-identity generation as a global assignment optimization problem, aiming to maximize the 3 overall matching quality between multiple identities and multiple references. Therefore, each generated identity can be paired with the most suitable reference, which maximizes inter-ID distinction while minimizing the impact of intra-ID variability, thereby enabling accurate and scalable multi-identity generation. Technically, to ensure that our method remains concise and readily applicable to various customized models to improve identity consistency, we propose a Unified Multi-identity Optimization (UMO) framework, which operationalizes the multi-to-multi matching paradigm through a novel Reference Reward", "response": "Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO"}
{"prompt": "Title: Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors\n\nIntroduction Reasoning-oriented language models have recently made striking gains Jaech et al. [2024], Guo et al. [2025]. Many top systems are trained with reinforcement learning on verifiable tasks, especially mathematics, where correctness provides reliable rewards. Yet we still lack a mechanistic account of what this training changes inside the network. Following Sinii et al. [2025], we train steering vectors ‚Äì learned additive directions injected into the residual stream, while freezing the base model. This parameterization isolates a small set of features that can be probed, ablated, and composed with mechanistic-interpretability tools. To isolate layer-wise effects, we fit one steering vector per layer ‚Ñìand measure its behavioral impact, then analyze in depth the layers with the clearest effects. Our contributions are as follows.Layer-wise isolation of RL-induced gains. One steering vector per layer, frozen base; evaluated across two models and six math benchmarks. Last layer behaves like first-token substitution. The final-layer vector acts at unembedding, boosting opening tokens (e.g., ‚ÄúTo‚Äù/‚ÄúStep‚Äù); simply prefixing that token recovers ‚àº10‚Äì11 points ‚Äì about three quarters of the explicit last-layer gain. Penultimate-layer circuit for Qwen-2.5-Math-7B. Skip-Attn retains over half the gain, and injecting only into V1 recovers the full penultimate-layer effect; analytically, the value-path addition yields a linear term independent of attention, consistent with an OV+MLP mechanism plus a direct unembedding push. *Corresponding author: v.siniy@tbank.ru Preprint. arXiv:2509.06608v1 [cs.LG] 8 Sep 2025 0 5 10 15 20 25 Layer 20 30 40 Mean Acc. Base Model Base Model. = 1 Steering Figure 1: Single-layer steering. Mean accuracy on six benchmarks for Qwen2.5-Math-7B when training a single vector s‚Ñìat layer ‚Ñìwith all other layers frozen. Mid-layer vectors yield the largest gains but never match all-layer steering, indicating the improvement is distributed across layers. \"To\" \" To\" \").\" \"\\n\" \":\\n\" \"To\" at Pos. 0 0.0 0.2 0.4 0.6 0.8 Probability Difference (a) Qwen2.5-Math-7B: Distribution of token-level probability change ‚àÜP induced by the last-layer vec- tor over 256 DeepScaleR prompts. Five tokens with the largest maxima are shown and a separate distribu- tion for \"To\" at the first generation position. Greedy Sampling 0 10 20 30 40 Performance 24.8 14.3 35.5 25.5 38.7 29.4 Base Base + \"To\" Last Steering (b) Prepending \"To\" to each prompt raises base-model accuracy by 10‚Äì11 points under both greedy and sam- pling, capturing about 75% of the gain from the ex- plicit last-layer vector. Figure 2: Last-layer analysis. Left: the last-layer vector mainly boosts the initial token \"To\". Right: prefixing that token reproduces most of the observed performance gain. 2 Single-layer steering vectors Setup. We study two base models ‚Äì Qwen2.5-Math-7B [Team, 2024] and Llama3.1-8B-Instruct [Grattafiori et al., 2024]. Steering vectors are trained with the RLOO objective [Ahmadian et al., 2024] on the DeepScaleR dataset [Luo et al., 2025]. Evaluation spans six math benchmarks: AIME24/25, AMC23, MATH500 [Hendrycks et al., 2021], MinervaMath [Lewkowycz et al., 2022], and Olympiad- Bench [He et al., 2024]. We report the mean score across these benchmarks. See Section B for further details. Result. For each layer ‚Ñì, we train a single steering vector s‚Ñìwhile freezing all others. Figure 1 reports per-layer results for Qwen2.5-Math-7B (see Section C for LLaMa3.1-8B-It), compared with (i) all-layer steering, (ii) the base model with greedy decoding, and (iii) the base model sampled at œÑ = 1.0 (the training initialization). Most layers improve over the initialization, but none matches all-layer steering; under greedy decoding, several do (Section D), suggesting that single-layer vectors target the right mechanisms yet cannot on their own sufficiently reduce the next-token distribution‚Äôs entropy. In Qwen2.5-Math-7B, s23 and s24 underperform their neighboring layers; we trace the issue to vectors passing through the input layer-norm in layer 25 (Section E). The value of single-layer steering is the simplified interpretability: each vector‚Äôs effect is isolated, avoiding cross-layer interactions. We build on this in the next section. 3 Last Layer ‚Äì Token Substitution Training only the last-layer vector s27 closes over 50% of the gap between the base model and all-layer steering, indicating a strong task signal. With no subsequent layers to process it, s27 acts 2 0 3 6 9 12 15 18 21 24 27 Head 28 30 32 34 36 38 Mean Acc. Steer Q-Proj Skip-Attn Skip-Layer s26 s27 0 1 2 3 Head Steer K-Proj Skip-Attn Skip-Layer s26 s27 0 1 2 3 Head Steer V-Proj Skip-Attn Skip-Layer s26 s27 Figure 3: Penultimate-layer steering in Qwen2.5-Math-7B. Mean accuracy when injecting s26 into a single projection of the final block: Q (left), K (center), V (right). Placing s26 only in V1 closes the gap between Skip-Attn and s26, indicating the effect is carried by the V1 ‚ÜíW", "response": "The mechanisms by which reasoning training reshapes language-model computations remain poorly understood. We study lightweight steering vectors inserted into the base model's residual stream and trained with a reinforcement-learning objective, which can match full fine-tuning performance while retaining the interpretability of small, additive interventions. Using logit-lens readouts, path patching, and circuit analyses, we analyze two models and find: (i) the last-layer steering vector behaves like a token-substitution bias concentrated on the first generated token, consistently boosting tokens such as \"To\" and \"Step\"; and (ii) the penultimate-layer steering vector leaves attention patterns largely unchanged and instead acts through the MLP and unembedding, preferentially up-weighting process words and structure symbols. These results establish a principled framework for interpreting the behavioral changes induced by reasoning training."}
{"prompt": "Title: MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization\n\nIntroduction In recent years, the field of Process Systems Engineering (PSE) has experienced a significant shift towards advanced optimization techniques that are capable of handling the inherent complexities of modern systems, whether in process control (Li√±√°n and Ricardez-Sandoval, 2025), sustainable supply chains (Qiu et al., 2024; Grossmann and Guill√©n- Gos√°lbez, 2010; Rangel-Martinez et al., 2021), or other industrial operations (Szatm√°ri et al., 2024). A key challenge in PSE is the need to optimize multiple, often conflicting objectives simultaneously, such as minimizing costs, maximizing throughput, ensuring quality, and reducing environmental impact. This complexity has become particularly evident as the development of sustainable practices has emerged as a key strategic focus for many organizations, driven by the increase in regulatory requirements and pressure from consumers to adopt environmentally friendly practices. As industries strive towards making their value chains more sustainable, the integration of these principles into every aspect of their operations, particularly in decision-making, becomes essential. The historical roots of multi-objective optimization trace back to the work of Vilfredo Pareto in the late 19th century (Pareto, 1935). Pareto efficiency, or Pareto optimality, refers to a situation where no objective can be improved without worsening another. In the context of multi-objective optimization, Pareto-optimal solutions represent a set of decisions where the trade-offs between competing objectives are balanced in the most efficient way. This foundational concept forms the basis of the modern approach to multi-objective optimization, where the goal is to generate a set of Pareto-optimal solutions for decision-makers to choose from, based on their specific priorities (Gunantara, 2018). While traditional MOO method, including Linear Programming (LP) (√ñzceylan and Paksoy, 2013) and Evolutionary Algorithms (EAs) (Liao et al., 2011), are well-established, the increasing complexity of real-world process systems requires dynamic and adaptable solutions. This has led to the emergence of Dynamic Multi-Objective Optimization (DMOO), which seeks to optimize systems that evolve over time in response to changing environmental, operational, and economic conditions. DMOO methods adapt to evolving conditions, re-optimizing solutions as objectives shift, which is particularly important for process systems engineering where decisions must be continuously adapted to accommodate variations in process dynamics, disturbances, and uncertain operating conditions which require fast and flexible decision-making. In such cases, the Pareto set must be recomputed as the environment changes. However, in fast-paced environments, the computational overhead involved in repeatedly solving DMOO problems can hinder fast, adaptive, real-time decision-making. This motivates the use of multi-objective reinforcement learning (MORL), which builds on RL principles to address multi-objective decision-making. Recent advances in MORL have demonstrated its ability to solve multi-objective decision-making problems in a series of applications in process systems engineering domains (Li et al., 2022), inventory management (Qiu et al., 2024), robotics control (Xu et al., 2020) and energy management (Wu et al., 2023). 2 MORSE: Multi-Objective RL for Supply Chain Optimization 1.1 Related Work Traditional Multi-Objective Optimization (MOO) approaches focus on finding a set of optimal solutions, known as the Pareto optimal, that balance trade-offs between different objectives such as cost, service level and environmental emissions. The collection of all Pareto optimal solutions forms the Pareto Front, which provides decision-makers with a range of non-dominated solutions, allowing them to choose the most appropriate solution based on their system requirements and preference. Methods such as Linear Programming (LP) (√ñzceylan and Paksoy, 2013), Goal Programming (Dutta and Kumar, 2015), and Evolutionary Algorithms (EAs) (Liao et al., 2011), including Non-dominated Sorting Genetic Algorithm II (NSGA-II) (Hnaien et al., 2010), have been widely used to approximate the Pareto front effectively. These approaches focus on two key criteria: (1) proximity - ensuring solutions closely approximate the true Pareto front, and (2) diversity - ensuring solutions span a wide range of trade-offs across the objective space. MOO has been widely applied in inventory management, integrating numerous objectives such as cost efficiency, service levels, and sustainable practices (Aslam and Amos, 2010). For example, Agrawal et al. (2020) explored a deterministic multi-objective optimization process that targeted multi-item inventory management, emphasizing the balance between reducing shortages and controlling costs through multi-criteria decision-making techniques. Recent literature has also explored the integration of sustainability principles into inven", "response": "In supply chain management, decision-making often involves balancing multiple conflicting objectives, such as cost reduction, service level improvement, and environmental sustainability. Traditional multi-objective optimization methods, such as linear programming and evolutionary algorithms, struggle to adapt in real-time to the dynamic nature of supply chains. In this paper, we propose an approach that combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to address these challenges for dynamic multi-objective optimization under uncertainty. Our method leverages MOEAs to search the parameter space of policy neural networks, generating a Pareto front of policies. This provides decision-makers with a diverse population of policies that can be dynamically switched based on the current system objectives, ensuring flexibility and adaptability in real-time decision-making. We also introduce Conditional Value-at-Risk (CVaR) to incorporate risk-sensitive decision-making, enhancing resilience in uncertain environments. We demonstrate the effectiveness of our approach through case studies, showcasing its ability to respond to supply chain dynamics and outperforming state-of-the-art methods in an inventory management case study. The proposed strategy not only improves decision-making efficiency but also offers a more robust framework for managing uncertainty and optimizing performance in supply chains."}
{"prompt": "Title: Nested Optimal Transport Distances\n\nIntroduction Recent advances in generative AI have opened new frontiers for applications in finance, such as financial modeling, stress testing, scenario generation, automated financial services, and decision- making under uncertainty; see [1, 2]. The goal of generative AI is to generate synthetic financial data ‚Äúindistinguishable‚Äù from real financial data, and the ‚Äúindistinguishability‚Äù are evaluated by probabilistic metrics. Due to the unique characteristics in different applications of generated financial data, it is natural to expect no consensus metrics like FID (Fr√©chet Inception Distance) in image generation evaluation. But the lack of even an intra-application consensus-metric makes it difficult to compare different models [3]. A qualified consensus-metric is supposed to satisfy the following two criteria: robustness and tractability. Robustness: The consensus-metric should robustly control a basket of quantities of interest. Tractability: The metric should be computationally tractable with samples. A large class of generative AI models for financial time series is trained for decision-making ap- plications, including dynamic hedging, optimal stopping, utility maximization, and reinforcement learning. Notably, these problems are not continuous with respect to widely-used distances, such as the Maximum Mean Discrepancy (MMD) and the Wasserstein distances (W-distances). However, these problems are Lipschitz-continuous with respect to stronger metrics, called adapted Wasserstein distances (AW-distances), also known as nested Wasserstein distances, which have been introduced as a variant of W-distances that accounts for the time-causal structure of stochastic processes. AW- distances serve as robust distances for many dynamic stochastic optimization problems across several fields, particularly in mathematical finance and economics; see [4‚Äì7]. Therefore, for robustness purposes, it is desirable to evaluate the generated time series distribution under AW-distances. While AW-distances ensure robustness, the existing implementation calculating AW-distances are slow and does not scale to long time series (see implementations in [8‚Äì10]). In this paper, we propose a natural parallelization algorithm to calculate AW-distances, which achieves orders-of-magnitude speedups over existing implementations and also behaves statistically consistently. Preprint. arXiv:2509.06702v1 [cs.LG] 8 Sep 2025 2 Nested optimal transport distances We regard RdT as the space of d-dimensional discrete-time paths with T time steps. Notation follows standard conventions and is intended to be self-explanatory; further details or clarifications of notations are provided in Appendix A.1. Definition 1. For ¬µ, ŒΩ ‚ààP2(RdT ), the Wasserstein-2 distance W2(¬∑, ¬∑) on P2(RdT ) is defined by W2 2(¬µ, ŒΩ) := inf œÄ‚ààCpl(¬µ,ŒΩ) Z ‚à•x ‚àíy‚à•2 œÄ(dx, dy), (1) where Cpl(¬µ, ŒΩ) denotes the set of couplings between ¬µ and ŒΩ, that is, probabilities in P(RdT √óRdT ) with first marginal ¬µ and second marginal ŒΩ. Next, we restrict our attention to couplings œÄ ‚ààCpl(¬µ, ŒΩ) such that the conditional law of œÄ is still a coupling of the conditional laws of ¬µ and ŒΩ, that is for all t = 0, . . . , T ‚àí1, œÄx1:t,y1:t ‚àà Cpl \u0000¬µx1:t, ŒΩy1:t \u0001 . Such couplings are called bi-causal, and denoted by Cplbc(¬µ, ŒΩ). Definition 2. For ¬µ, ŒΩ ‚ààP2(RdT ), the adapted Wasserstein-2 distance AW2(¬∑, ¬∑) on P(RdT ) is defined by AW2 2(¬µ, ŒΩ) := inf œÄ‚ààCplbc(¬µ,ŒΩ) Z T X t=1 ‚à•xt ‚àíyt‚à•2 œÄ(dx, dy). (2) The adaptedness (or bi-causality) imposed on couplings modifies the Wasserstein distance to ensure robustness of a large class of ‚Äúwell-defined\" dynamic optimization problems, ranging from optimal stopping and utility maximization to dynamic risk minimization and dynamic hedging; see [5‚Äì7, 11‚Äì 13]. We therefore present only the general statement here, while providing illustrative examples in the appendix and referring the reader to the cited references for further details. Robustness ([7, 11‚Äì13]). Let ¬µ, ŒΩ ‚ààP2(RdT ) and v: P2(RdT ) ‚ÜíR, where v(¬µ) denotes the optimal value of a ‚Äúwell-defined‚Äù dynamic optimization problem under ¬µ. Then under ‚Äúmild conditions‚Äù, there exists L ‚â•0 such that the following holds |v(¬µ) ‚àív(ŒΩ)| ‚â§L AW2(¬µ, ŒΩ). From the perspective of nested disintegration, Pflug-Pichler define the adapted Wasserstein distance as nested distance in [11] and establish an alternative representation of AW2(¬∑, ¬∑) through the dynamic programming principle; see the proof of Proposition 1 in Appendix A.3. Proposition 1 (Dynamic programming principle). Let ¬µ, ŒΩ ‚ààP2(RdT ). Set V ¬µ,ŒΩ T ‚â°0 and define for all t = 0, . . . , T ‚àí1, V ¬µ,ŒΩ t (x1:t, y1:t) = inf œÄt+1 x1:t,y1:t‚ààCplbc(¬µt+1 x1:t,ŒΩt+1 y1:t) Z h ‚à•xt+1‚àíyt+1‚à•2+V ¬µ,ŒΩ t+1(x1:t+1, y1:t+1) i dœÄt+1 x1:t,y1:t. Then for all t = 0, . . . , T ‚àí1, V ¬µ,ŒΩ t (x1:t, y1:t) = AW2 2(¬µx1:t, ŒΩy1:t) and V ¬µ,ŒΩ 0 ‚â°AW2 2(¬µ, ŒΩ). The computation of V ¬µ,ŒΩ t (x1:t, y1:t) can be fully parallelized over all admissible pairs (x1:t, y1:t), which is the key reason why our algorithm is naturally paral", "response": "Simulating realistic financial time series is essential for stress testing, scenario generation, and decision-making under uncertainty. Despite advances in deep generative models, there is no consensus metric for their evaluation. We focus on generative AI for financial time series in decision-making applications and employ the nested optimal transport distance, a time-causal variant of optimal transport distance, which is robust to tasks such as hedging, optimal stopping, and reinforcement learning. Moreover, we propose a statistically consistent, naturally parallelizable algorithm for its computation, achieving substantial speedups over existing approaches."}
{"prompt": "Title: TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations\n\nI. INTRODUCTION C OMMUNICATION and routing are challenging in a vehicular ad hoc network (VANET) [1], as vehicles can observe only part of the network, and the network‚Äôs structure shifts rapidly; a previously obtained observation may soon become obsolete (as shown by Figure 1). Although compared to classical software algorithms, RL routing algorithms can potentially deal with more complex objectives (e.g., optimising delay while minimising the bandwidth overhead) [2], the problems of partial observation and network dynamics put a strain on the RL routing models. Several studies have shown that graph neural networks (GNNs) generalise better on routing tasks compared to other neural networks like multilayer perceptrons (MLPs) [3]‚Äì[7]. This work will be submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Xiaolu Fu is an AI research engineer at Unicom Data Intelligence, China Unicom, Hangzhou, China (fuxl67@chinaunicom.cn), and a former student of the Computing Department, Imperial College London, London, UK (email: andy.fu23@alumni.imperial.ac.uk). Ziyuan Bao is an independent researcher and a former MSc student of the Computing Department, Imperial College London, London, UK (email: ziyuan.bao23@alumni.imperial.ac.uk). Eiman Kanjo is a Professor with Pervasive Sensing & TinyML and the Head of the Smart Sensing Lab at Nottingham Trent University, Nottingham, UK (email: eiman.kanjo@ntu.ac.uk); and Provost‚Äôs Visiting Professor in tinyML at Imperial College London, London, UK (email: e.kanjo@imperial.ac.uk). Reachable via routing Trajectory predection Directly connected Fig. 1. Conceptual street-view diagram of a VANET. The blue vehicle represents the ego vehicle, yellow vehicles are within its communication range, and the remaining vehicles are unobservable while in motion. Because GNNs process the topological information of the network, they do not have to overfit to a certain set of net- work structures. Theoretically, a GNN-based routing algorithm could be generalised to highly dynamic VANET environments and unseen network graphs. However, there is still a significant gap between experimental assumptions and the complexities of real-world VANET environments: existing research on GNN- based routing often tests their methods on graphs with a fixed small number of nodes and d-regularity (usually a small d); whereas in real-world environments, neither the node number nor the node degree is fixed, and they could potentially be significant. Moreover, GNN-based routing research rarely address partial observations, making them less compatible with VANET routing. GNNs can handle inputs of arbitrary sizes, but the outputs‚Äô data structure remains graphs. To bridge the gap between GNNs‚Äô outputs and the action space of a routing policy, a common approach is to concatenate all node features into one vector, then several MLP layers reduce the vector‚Äôs dimension to the number of next-hop choices. Since MLPs require fixed input and output sizes, a routing model that generalises across different network sizes and node degrees must initialise the MLP with large dimensions and pad inputs or outputs when the actual sizes are smaller. However, this approach introduces both the curse of dimensionality and the long tail problem. We propose a novel system that addresses the above arXiv:2509.06665v1 [cs.LG] 8 Sep 2025 2 problems. Firstly, agents proactively broadcast and exchange their knowledge with neighbours to form more complete but outdated observations, and a trajectory prediction algorithm estimates the real-time positions of other vehicles. We design an action space pruning approach, which alleviates the long tail problem and the curse of dimensionality by ignoring neighbours that are less likely to contribute to the routing task. We adopted the cross-attention from the transformer [8] to address the curse of dimensionality, as well as enhanced the model‚Äôs ability to learn inductively. Our main contributions are listed in the following: ‚Ä¢ Proposed TrajAware, a novel reinforcement learning al- gorithm that generalises across graphs with varying node degrees and network sizes, addressing the limitations of prior methods constrained to fixed topologies. ‚Ä¢ Enriched VANET optimisation with Graph Cross- Attention, improving scalability and inductive learning compared to conventional graph neural network‚Äìbased approaches. ‚Ä¢ Tackled the challenge of partial and outdated observations in VANETs by integrating trajectory-aware prediction, enabling vehicles to estimate real-time positions of neigh- bours and maintain robust decision-making. ‚Ä¢ Extended the evaluation of existing methods to more complex and realistic environments, demonstrating gen- eralisability beyond simplified benchmarks. We validated TrajAware with the open-source SUMO traffic simulator and real-world city road networks. To the best of our knowledge, although there exists r", "response": "Vehicular ad hoc networks (VANETs) are a crucial component of intelligent transportation systems; however, routing remains challenging due to dynamic topologies, incomplete observations, and the limited resources of edge devices. Existing reinforcement learning (RL) approaches often assume fixed graph structures and require retraining when network conditions change, making them unsuitable for deployment on constrained hardware. We present TrajAware, an RL-based framework designed for edge AI deployment in VANETs. TrajAware integrates three components: (i) action space pruning, which reduces redundant neighbour options while preserving two-hop reachability, alleviating the curse of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to the global graph context, producing features that generalise across diverse network sizes; and (iii) trajectory-aware prediction, which uses historical routes and junction information to estimate real-time positions under partial observations. We evaluate TrajAware in the open-source SUMO simulator using real-world city maps with a leave-one-city-out setup. Results show that TrajAware achieves near-shortest paths and high delivery ratios while maintaining efficiency suitable for constrained edge devices, outperforming state-of-the-art baselines in both full and partial observation scenarios."}
{"prompt": "Title: Detection of trade in products derived from threatened species using machine learning and a smartphone\n\nINTRODUCTION The unsustainable trade in wild animals and plants is one of the biggest threats to biodiversity worldwide [1]. Wildlife trade has been embedded in human history for millennia, with evidence of economic exchange for wildlife and their products, tracing back to early Greek and Egyptian cultures[2]. However, the scale and intensity of this trade have increased dramatically in modern times, driven by the industrial revolution and rapid technological innovation[3], leading to significant declines and even extinctions of species, with far-reaching negative effects on food webs and ecosys- tem functioning. Thousands of species and derived products are traded for several reasons, including as pets, ornaments, trophies, medicines, food, and fashion[4]. Unsustainable trade in wildlife can also threaten the livelihoods of rural communities and can pose significant risks to global food security[5]. 1 arXiv:2509.06585v1 [cs.CV] 8 Sep 2025 It also raises serious concerns for animal welfare and public health, particularly in relation to the spillover of zoonotic diseases[6]. Wildlife trade continues to expand and is also gaining ground on the Internet, where wildlife and derived products are increasingly offered for sale[7]. This growing trend poses an even more prominent threat to biodiversity and people. The shift of wildlife trade to online and social media platforms has greatly complicated enforcement efforts[8]. Sellers use websites, e-commerce, and social media to promote their products, connect with potential buyers, and negotiate prices[9]. Online trade makes it difficult to distinguish between legal and illegal activities, as national boundaries become unclear and identifying source and destination countries is challenging[10]. Unlike physical markets, where transactions can be observed and traced, online trade provides anonymity and a global reach, allowing traders to operate across borders with minimal risk of detection[7]. Figure 1: Machine learning‚Äìbased object recognition models were trained on images of elephant ivory and skins, pangolin scales and claws, and tiger skins and bones. The optimized model achieved high detection accuracy and was hosted on the cloud to be integrated into a smartphone application. This enabled real-time identification of potentially illegal products. This pipeline supports both online monitoring of wildlife trade and on-the-ground enforcement by authorities. With limited resources available to counteract the global biodiversity crisis, digital surveillance methods offer cost-effective solutions to help monitor online wildlife trade and its legality[11]. Ma- chine learning methods, in particular, offer cost-effective solutions to automatically identify image 2 and/or text content that pertains to wildlife trade on digital platforms[8][12]. Methods like natural language processing analyze human language through text vectorization, while machine vision mod- els discern characteristics in images to find pertinent details[13]. Natural language processing algo- rithms can search social media, e-commerce platforms, and dark web forums for wildlife trade-related keywords[14]. Likewise, image recognition models, mainly using convolutional neural networks, can detect species in online images, highlighting suspicious posts for additional scrutiny[15][16]. How- ever, computer vision methods capable of automatically identifying wildlife products from digital content are mostly missing. In this study, we developed a machine learning-based approach to detect and classify wildlife products using digital images. Specifically, we implemented a Faster-Region based Convolutional Neural Network (F-RCNN) model trained on a dataset comprising images of elephant, pangolin, and tiger products identified either as illegally traded or having strict regulations on trade. To optimize model performance, we compared the effectiveness of two loss functions‚Äîcross-entropy loss and asymmetric focal loss‚Äîin terms of precision, recall, and classification accuracy. Our experiments included training both species-specific models and a single model capable of detecting products from all three target species. In addition, we integrated the model into a smartphone-based application, enabling real-time identification of wildlife products from images by law enforcement agencies and conservation officers. 2 Methods 2.1 Data acquisition Data for model training was complied by members of the International Fund for Animal Welfare (IFAW www.ifaw.org/uk/projects/wildlife-crime-prevention-china) and consisted of a total of 39,941 images. The data covered illegally traded wildlife products from three species, namely elephant, pangolin, and tiger, as shown in table 1. For all three species, along with images of products, there was also a category labeled as ‚Äúwrong‚Äù which included images of products that look similar to the products of the target species but are, in fact, not wildlife trade related (e.g., products mad", "response": "Unsustainable trade in wildlife is a major threat to biodiversity and is now increasingly prevalent in digital marketplaces and social media. With the sheer volume of digital content, the need for automated methods to detect wildlife trade listings is growing. These methods are especially needed for the automatic identification of wildlife products, such as ivory. We developed machine learning-based object recognition models that can identify wildlife products within images and highlight them. The data consists of images of elephant, pangolin, and tiger products that were identified as being sold illegally or that were confiscated by authorities. Specifically, the wildlife products included elephant ivory and skins, pangolin scales, and claws (raw and crafted), and tiger skins and bones. We investigated various combinations of training strategies and two loss functions to identify the best model to use in the automatic detection of these wildlife products. Models were trained for each species while also developing a single model to identify products from all three species. The best model showed an overall accuracy of 84.2% with accuracies of 71.1%, 90.2% and 93.5% in detecting products derived from elephants, pangolins, and tigers, respectively. We further demonstrate that the machine learning model can be made easily available to stakeholders, such as government authorities and law enforcement agencies, by developing a smartphone-based application that had an overall accuracy of 91.3%. The application can be used in real time to click images and help identify potentially prohibited products of target species. Thus, the proposed method is not only applicable for monitoring trade on the web but can also be used e.g. in physical markets for monitoring wildlife trade."}
{"prompt": "Title: \\texttt{R$^\\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World\n\nIntroduction Recent years have witnessed rapid developments and huge breakthroughs in AI, leading to its integra- tion into everyday life and establishing it as a foundational infrastructure in society (Van Der Vlist et al., 2024). As AI systems are increasingly deployed in safety-critical domains (e.g., scientiÔ¨Åc research (Jumper et al., 2021; Zhang et al., 2023; Novikov et al., 2025), autonomous driving (Wang et al., 2021; Rowe et al., 2024), healthcare (Panayides et al., 2020; Bekbolatova et al., 2024), law (Lai et al., 2024)), the risks posed by unsafe or unreliable outputs have become more pronounced. In such settings, failures can result in severe, even catastrophic, consequences. Beyond these near-term concerns, the continued advancement toward highly autonomous and superhuman-level AI raises long-term existential risks (Dalrymple et al., 2024; Bengio et al., 2025a,b; Kulveit et al., 2025; Clymer et al., 2025; Shanghai AI Lab, 2025a). As capabilities scale, so does the diÔ¨Éculty of aligning, controlling, and governing these systems, 1 arXiv:2509.06786v1 [cs.LG] 8 Sep 2025 R2AI: Towards Resistant and Resilient AI in an Evolving World AI Safety AI Capability 4 5¬∞ Approximate Alignment Intervention Reflection 45¬∞Line Yellow Line Red Line Current Roadmap Safe AGI LLaMA3 GPT-4 Superalignment RLHF RLAIF ChatGPT GPT-3 GPT-5 Claude 4 Gemini 2.5 Transformer InternLM Qwen SFT Safety Score Capability Score 45¬∞Line AI Capability (a) (b) (c) Figure 1 The AI-45‚ó¶Law (Yang et al., 2024): coevolving capability with safety.1(a) Empirical distribution of leading foundation models, showing a widening gap between capability scores and safety scores across major labs. (b) Conceptual safety‚Äìcapability plane comparing the current roadmap (pink) with the yellow, red, and 45‚ó¶ trajectories toward safe AGI, emphasizing transitions from approximate alignment to reÔ¨Çection. (c) Historical timeline of frontier models, from Transformer (Vaswani et al., 2017) to GPT-5 (OpenAI, 2025), Claude-4 (Anthropic, 2025), and Gemini-2.5 (Comanici et al., 2025), illustrating the divergence between capability scaling and current alignment methods (e.g., SFT (Ouyang et al., 2022), RLHF (Christiano et al., 2017), RLAIF (Bai et al., 2022)), and the need for a coevolutionary path to Safe AGI. thus potentially leading to scenarios with irreversible societal or civilizational impacts (Bengio et al., 2025c; Shanghai AI Lab & Concordia AI, 2025). Despite escalating risks, safety progress has lagged far behind capability growth. As shown in Figure 1a, evaluations show a consistent pattern: leading AI models worldwide‚Äîsuch as GPT-5 (OpenAI, 2025), Claude 4 (Anthropic, 2025), and Gemini-2.5 (Comanici et al., 2025)‚Äîdemonstrate signiÔ¨Åcantly higher capability scores than safety scores. This imbalance reveals a structural problem: current safety approaches are reactive, fragmented, and incapable of scaling with capability. To capture this tension, Shanghai AI Lab proposed the AI-45‚ó¶Law (Yang et al., 2024): safety and capability must coevolve along a 45‚ó¶diagonal trajectory. Temporary deviations are tolerable, but persistent dips below the 45¬∞ line increase the risk of catastrophic misalignment, while rising above it may unnecessarily stall innovation. We further deÔ¨Åne two thresholds: yellow lines serve as early warnings when capability begins to outpace safety; red lines denote irreversible, catastrophic risks that must never be crossed (IDAIS, 2024, 2025). Current safety research can be broadly categorized into two paradigms. The dominant ‚ÄúMake AI Safe‚Äù paradigm seeks to improve safety after model development, typically through alignment Ô¨Åne-tuning (e.g., RLHF (Christiano et al., 2017), RLAIF (Bai et al., 2022)), red teaming (Perez et al., 2022; Ganguli et al., 2022; Pavlova et al., 2024), and guardrail (Bai et al., 2022; Rajpal, 2023; Oh et al., 2024). While eÔ¨Äective in mitigating known risks, these methods are often reactive, brittle, expensive, and struggle to address unknown or emerging risks. In contrast, the ‚ÄúMake Safe AI‚Äù paradigm emphasizes intrinsic 1Figure 1a is reproduced from data available at https://aiben.ch. Figures 1b and 1c are adapted from Figure 1 in Yang et al. (2024). 2 R2AI: Towards Resistant and Resilient AI in an Evolving World safety, designing systems to be safe by construction. Prominent directions include formal guarantees (Szegedy, 2020; Dalrymple et al., 2024) and Scientist AI (Bengio et al., 2025a). Yet even these approaches often fall short in open-ended environments where novel risks cannot be fully anticipated. To achieve scalable safety in an envolving world, we must rethink what ‚ÄúMake Safe AI‚Äù entails. We argue that its core principle should be coevolution: safety must not be treated as a constraint or one-time guarantee, but as a continuous, adaptive capability that evolves alongside intelligence in uncertain, dynamic environments. We therefore propose safe-by-coevolution as a new formulation for ‚ÄúMake Safe AI‚Äù, inspired by", "response": "In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into ``Make AI Safe'', which applies post-hoc alignment and guardrails but remains brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose \\textit{safe-by-coevolution} as a new formulation of the ``Make Safe AI'' paradigm, inspired by biological immunity, in which safety becomes a dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce \\texttt{R$^2$AI} -- \\textit{Resistant and Resilient AI} -- as a practical framework that unites resistance against known threats with resilience to unforeseen risks. \\texttt{R$^2$AI} integrates \\textit{fast and slow safe models}, adversarial simulation and verification through a \\textit{safety wind tunnel}, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework offers a scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI."}
{"prompt": "Title: Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition\n\nIntroduction Artificial intelligence (AI) models have achieved remarkable performance across myrid of domains including computer vision and natural language processing. Yet, a persistent challenge arises in real-world deployment: these models often fail to recognize inputs from unfamiliar data distributions, leading to overly confident and potentially misleading predictions [1]. This limitation underscores the importance of out-of-distribution (OOD) detection for building trustworthy AI systems, particularly in high-stakes domains such as autonomous driving [2] and medical diagnostics [3]. The goal of OOD detection is not only to provide accurate prediction on seen data distributions but also to flag inputs from novel or unobserved distributions [4]. OOD detection has been an active topic of research in the field of AI for many decades; a recent survey can be found in [5]. A key focus in this field is detecting semantic shifts‚Äîscenarios where new, previously unseen classes appear in the test data, resulting in a mismatch between the label spaces of in-distribution (ID) and OOD samples. A wide range of methods have been proposed for OOD detection, including softmax/logit-based post-hoc techniques [4, 6, 7, 8, 9, 10] and feature distance-based strategies [11, 12, 13, 14, 15]. Nonetheless, most existing OOD detection methods are developed under the assumption that models are trained on clean, correctly labeled data. However, in practice, training datasets often contain noisy labels, stemming from the scarcity of expert annotators and the high cost of accurate label acquisition [16]. Recent empirical studies have brought serious attention to this issue, revealing that the presence of label noise can significantly degrade the performance of state-of-the-art OOD detection methods [17]. This highlights a critical gap in current Preprint. arXiv:2509.06918v1 [cs.LG] 8 Sep 2025 Figure 1: The effect of label noise for OOD detection. The figure shows the UMAP representations of the latent feature vectors h(x) learned using the cross entropy loss-based training using the noisily labeled dataset {xn, byn} for various synthetic noise rates. The false positive ratio (FPR) for OOD detection using kNN score is also reported. The clusters are more distorted for the training data, losing the ID-ness characteristics, resulting in degraded performance in OOD detection during test time. research and underscores the need to develop robust OOD detection frameworks that remain reliable under real-world label noise. The effect of label noise on the classification performance of the deep learning models has been extensively studied in recent years; see the survey [18]. It is now well-established that training deep neural network (DNN) models with noisy labels can severely degrade classification performance, leading to poor generalization and overfitting [19, 20]. To address this, a variety of label noise-robust methods have been proposed, including loss correction strategies such as probabilistic modeling techniques [21, 22, 23, 24, 25, 26], robust loss function designs [27, 28, 29], and in-built sample selection strategies [30, 31, 32, 33, 34], However, their effectiveness in OOD detection under label noise remains largely unexplored. The key challenge lies in the misalignment of objectives: while label noise methods aim to correct the prediction probabilities within the training distribution, OOD detection requires learning discriminative feature representations to detect the samples that does not belong to the training distribution. Hence, most existing label-noise approaches exhibit poor OOD detection performance when applied directly, as we will demonstrate in detail in subsequent sections. Our Contributions. In this work, we investigate the critical challenge of robust OOD detection in the presence of noisy labels in the training set. Unlike existing studies that focus solely on the empirical limitations of current OOD detection methods [17], we identify a key gap, where the label noise- robust methods improves generalization under noisy supervision for classification settings, yet are largely ineffective when directly applied for OOD detection. To address this limitation, we propose a novel learning framework, named as Noise-robust Out-Of-Distribution Learning (NOODLE), by leveraging the loss correction techniques with low-rank and sparse decomposition methods. To the best of our knowledge, this work is the first to offer a principled solution to the problem, achieving substantial improvements over state-of-the-art OOD detection methods in the presence of label noise. Notation. Notations are defined in the supplementary materials. 2 Problem Statement Consider an input feature space X ‚äÇRD, where D denotes the dimensionality of the input features. Let the label space be defined as Y = {1, . . . , K}, corresponding to K distinct classes for the ID data. We define the training dataset D as: D = {(xn, yn)}N n=1, xn ‚ààX, yn ‚ààY, wher", "response": "Robust out-of-distribution (OOD) detection is an indispensable component of modern artificial intelligence (AI) systems, especially in safety-critical applications where models must identify inputs from unfamiliar classes not seen during training. While OOD detection has been extensively studied in the machine learning literature--with both post hoc and training-based approaches--its effectiveness under noisy training labels remains underexplored. Recent studies suggest that label noise can significantly degrade OOD performance, yet principled solutions to this issue are lacking. In this work, we demonstrate that directly combining existing label noise-robust methods with OOD detection strategies is insufficient to address this critical challenge. To overcome this, we propose a robust OOD detection framework that integrates loss correction techniques from the noisy label learning literature with low-rank and sparse decomposition methods from signal processing. Extensive experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms the state-of-the-art OOD detection techniques, particularly under severe noisy label settings."}
{"prompt": "Title: Approximating Condorcet Ordering for Vector-valued Mathematical Morphology\n\nIntroduction Mathematical morphology, a cornerstone of nonlinear image processing, presents unique challenges when applied to vector-valued data, such as color images or hyperspectral remote sensing datasets [1,2,20]. Accordingly, morphological oper- ators can be defined on complete lattices, which are ordered sets with well-defined extrema operations [9,18]. However, there is no natural ordering for vectors, leading to the development of various approaches to vector-valued mathematical morphology. This paper applies concepts from social choice theory to address the challenge of selecting an appropriate ordering for vector-valued mathemati- cal morphology. Social choice theory studies how individual preferences can be aggregated to reach a collective decision, such as electing a candidate through voting [4]. In our context, we aim to establish consensus ordering for vector- valued mathematical morphology. arXiv:2509.06577v1 [cs.CV] 8 Sep 2025 2 M.E. Valle et al. To our knowledge, integrating different orderings to define vector-valued mathematical morphology was first proposed by Lezoray [13]. Specifically, Lezo- ray proposed a weighted version of the Borda rule for combining stochastic per- mutation orderings. Although the Borda rule is computationally straightforward, it may produce an ordering that does not reflect the majority opinion [7]. Instead of using the Borda rule, this paper considers Condorcet‚Äôs principle, a more robust method for preference aggregation that prioritizes the most preferred candidates [4,12]. The primary limitation of the Condorcet voting method is the compu- tational complexity of the optimization techniques used to reach the consensus ordering. By framing ordering relations as pairwise comparisons between vec- tors, we propose a learning-based framework to derive simplified representative mappings that approach Condorcet-optimal rankings. This paper is structured as follows: Section 2 provides an overview of fundamental concepts in mathemat- ical morphology, Section 3 introduces the methodology for learning Condorcet reduced orderings, Section 4 presents computational experiments, and Section 5 concludes with final remarks and insights. 2 Basic Concepts on Mathematical Morphology In this paper, we focus on morphological operators for vector-valued images. A vector-valued image, denoted as I, corresponds to a mapping I : D ‚ÜíV, where D represents the image domain and V ‚äÇ¬ØRd, with ¬ØR = R ‚à™{‚àí‚àû, +‚àû} and d ‚â•2, denotes a set of vector values. Throughout this paper, we also assume that the domain D is a finite subset of a non-empty discrete additive Abelian group, (E, +). Mathematical morphology deals with non-linear operators successfully used for image processing and analysis [9]. Generally speaking, morphological opera- tors explore shapes and geometrical forms present in images. Complete lattices provide suitable frameworks for defining morphological operators [16]. A com- plete lattice L is a partially ordered set (poset) where every subset has both a supremum and an infimum [3]. The supremum and infimum of a set X ‚äÜL are denoted by W X and V X, respectively. We assume that the vector value set V is a complete lattice in the following. Dilations and erosions are two elementary morphological operations [18]. For a given set S ‚äÇE, referred to as a structuring element, the dilation and erosion of the image I by S are defined by the equations below, respectively: Œ¥S(I)(p) = _ s‚ààS p‚àís‚ààD I(p‚àís) and ŒµS(I)(p) = ^ s‚ààS p+s‚ààD I(p+s), for all p ‚ààD. (1) Other morphological operators are obtained by combining elementary morpho- logical operators [9]. For instance, the combinations of dilations and erosions yield opening and closing, which exhibit notable topological properties and func- tion as non-linear image filters [18]. Specifically, an opening is characterized by the composition of an erosion succeeded by a dilation, both using the same struc- turing element S. In contrast, a closing is defined as a dilation followed by an Approximating Condorcet Ordering for Vector-Valued MM 3 erosion. In mathematical terms, the opening Œ≥S and the closing œïS of an image I by a structuring element S are defined, respectively, by Œ≥S(I) = Œ¥S (ŒµS(I)) and œïS(I) = ŒµS (Œ¥S(I)) . (2) We would like to highlight that dilations and erosions are defined in (1) through supremum and infimum operations. As a result, a partial order with well-defined extremal operations is sufficient for constructing morphological op- erators. However, unlike the one-dimensional (scalar) case, there is no inherent natural ordering for vectors, and various methods exist for ordering vector values. Examples of partial orderings used in vector-valued mathematical morphology include the marginal or Cartesian product ordering as well as conditional or- dering schemes like the RGB-lexicographical ordering [1,2]. Moreover, despite not being partial orders but only pre-orders, reduced orderings have also been successfully used to develop vecto", "response": "Mathematical morphology provides a nonlinear framework for image and spatial data processing and analysis. Although there have been many successful applications of mathematical morphology to vector-valued images, such as color and hyperspectral images, there is still no consensus on the most suitable vector ordering for constructing morphological operators. This paper addresses this issue by examining a reduced ordering approximating the Condorcet ranking derived from a set of vector orderings. Inspired by voting problems, the Condorcet ordering ranks elements from most to least voted, with voters representing different orderings. In this paper, we develop a machine learning approach that learns a reduced ordering that approximates the Condorcet ordering. Preliminary computational experiments confirm the effectiveness of learning the reduced mapping to define vector-valued morphological operators for color images."}
{"prompt": "Title: An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection\n\nI. INTRODUCTION Malicious insider threats are a human problem that manifest in computer networks as data loss, theft, or destruction [1]. Motivations for malicious insider threat activities include financial gain [2], retribution for perceived fault [3], and rationalization [4], sometimes driven by subclinical traits [5]. The evasive nature and small number of insider threats creates detection difficulties for organizations [6]. Insider threats are minimally observed on corporate networks, with centralized system logging (syslog) as an important aggregation tool to enable discovery [7]. Organizations rely on many detection solutions in addition to syslog, including security incident event monitoring (SIEM) [8], user entity behavioral analysis (UEBA) [9], machine learning-based log analysis [10], ML- driven log inspection [11], and human intervention [12]. An emerging area of insider threat research is the use of large language models (LLM) to detect insider threats in existing datasets [13]. However, access to realistic datasets for research purposes can be limited due to the proprietary, and confidential nature of such data. Further, the human element of insider threat research requires careful attention to data privacy concerns which might prevent the release of real-world data. This research addresses the ethical and access issues of real-world data by leveraging LLMs to generate a unique synthetic syslog dataset with dynamically created log content. Synthetic message generation with commonly used syslog fields for validity, and LLM-generated code elements ensures a novel, realistic approach while maintaining data privacy. Insider threat detection tests are then performed by LLMs on the data. The study includes an imbalanced mix of standard (non-insider threat) and insider threat syslog messages to mimic, with minor limitations, realistic syslog messages for LLM inspection. To evaluate LLM detection accuracy within an acceptable margin of error, a small percentage of the log population (1%) was dynamically generated as insider threat logs. According to an exhaustive literature search, this study appears to be the first of its kind to use the methodology described in this paper. The primary novel contributions of this insider threat research are: 1) addressing limited access to real-world data by utilizing LLMs to synthesize syslog messages for insider threat indicators, and 2) producing a more comprehensive analysis of LLM- automated detection of insider threats than previously seen. The following research questions guide the study: ‚Ä¢ RQ1: How effective are LLMs in identifying insider threats within synthetically generated syslog messages? ‚Ä¢ RQ2: What is the comparative performance between the LLMs for highly imbalanced syslog datasets? ‚Ä¢ RQ3: What are the comparative false alarm rates when utilizing LLM-based insider threat detection? II. METHODOLOGY The methodology follows a three-phase process: 1) gener- ation of synthetic syslog datasets, 2) analysis of the logs for insider threats by the LLMs via API, and 3) statistical analysis of detection results. Each phase is described below. arXiv:2509.06920v1 [cs.CR] 8 Sep 2025 A. Synthetic Syslog Generation Syslog messages were generated through a menu-driven program (SysGen)1 to configure the number of standard logs, the number of insider threat logs, the syslog server IP and port, and export filenames for raw and structured output. Cochran‚Äôs sample size formula [14] applied to a population of 10 million messages dictated a sample size of 385. Two datasets were generated: one control dataset with 385 standard logs and no insider threat messages, and one 385 syslog message dataset with 381 standard logs and four insider threat logs (1%). These imbalanced datasets intended to reflect the real-world rarity of insider threats and formed the basis for this research. The syslog server required SIEM compatible syslog mes- sages with basic formatting logic and an embedded JSON- formatted message field. Existing datasets such as CERT [15] are widely used in insider threat research but were not appropriate for this study due to field limitations and lack of SIEM-level integration. The logs include realistic field values drawn from RFC 3164 and 5424 standards, with validation references from Palo Alto Networks [16], Palo Alto Networks [16], CISA [17], the MITRE ATT&CK¬Æ Framework [18], and NIST 800-53r5 [19]. Fields include: timestamp, username, session id, auth method, src ip, src hostname, action, object, resource, command, status, bytes, app name, dst ip, dst port, protocol, duration, network zone, location category, criticality, is approved application, and previous occurrence count. Each log entry includes a unique session ID for validation, although session ID is not part of the standard syslog format. Logs were exported in structured CSV format with optional raw text output for inspection. Integrity checking code an- alyzed the unique dataset session IDs for l", "response": "Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection."}
{"prompt": "Title: RAFFLES: Reasoning-based Attribution of Faults for LLM Systems\n\nIntroduction As large language models (LLMs) evolve into complex, multi-component systems, a critical gap has emerged between what they can do and how we can effectively evaluate them. The rise of language-conditioned agentic architectures such as ReAct [1], Toolformer [2], and Reflexion [3] allows systems to plan, reason, and act over long time horizons. However, these new architectures also introduce novel failure modes that current evaluation strategies are not equipped to detect. Most current strategies are confined to isolated metrics that focus on overall outcomes and they struggle with longer contexts [4‚Äì6]. This leaves manual \"detective work\" as the only way to identify root cause errors. In long-horizon agentic systems, a single, subtle error can quickly cascade, creating a ripple effect of unexpected behaviors. Finding these root cause errors is a true \"needle in the haystack\" problem. To date, when such step-level evaluation is done manually, it takes on the order of many minutes to tens of minutes per data instance [4, 7, 8]. Such manual debugging is too costly and can become a bottleneck. Instead, automatic evaluations of multi-turn, multi-component systems must shift focus from end performance to better understanding where failures originate and how they form. arXiv:2509.06822v1 [cs.AI] 8 Sep 2025 Figure 1: Our proposed RAFFLES framework for multi-turn agentic evaluation by reasoning- based fault attribution. Evaluating new LLM systems requires a new generation of evaluators that integrate multi-dimensional metrics, explainable judgments, and iterative refinement. RAFFLES leverages specialized Evaluators designed to assess candidate faults based on the criteria of a decisive fault. Each Evaluator takes in the full log œÑ and intermediate reasoning, which are passed to subsequent iterations until a decisive fault is determined. The presented evolution of LLM systems (orange) is inspired by a tutorial by Hassani, Karbasi, and Robey [13]. Automatic failure attribution within LLM multi-component agentic systems is an emerging area of focus for the evaluation community [7‚Äì10]. However, preliminary results have demonstrated limited success due to the challenges imposed by detecting faults in complex agentic systems. For example, the LLM-as-a-judge has become a cornerstone of automated evaluation due to its flexibility in approximating human evaluators [11, 12], and yet single-pass LLM evaluators struggle to detect faults within the long trajectories of agentic systems [7, 8]. Effective evaluation by which one can pinpoint faulty planning, logic, tool calls, calculations, code, etc. within multi-component systems requires its own set of capabilities (e.g., reasoning, planning, tool calling, iterating). At a recent ICML tutorial, a framework was introduced describing the progression from simple Chat-LLM, to Routers, Tool-Caller, Iterators, and, finally, Fully Autonomous systems [13]. We believe that evaluation systems must likewise evolve in a parallel manner: from single judgment Chat-LLMs, to aspect-specific evaluator routing, deciding with tools, iterators that loop and refine, and eventually, fully autonomous evaluators. We present this evolution of evaluators in Figure 1. The need to evolve evaluation strategies to meet the demands of multi-component LLM systems can be demonstrated via a simple example of implementing Retrieval-Augmented Generation (RAG). RAG systems are conventionally judged by the final summary‚Äôs fidelity to an ideal summary [14, 15]. A more insightful evaluation, however, would be to determine the point of failure (e.g., retrieval vs. generation) and characterize its nature (e.g., irrelevant retrieval vs. incoherent retrieved sets), to provide a more accurate depiction of the system‚Äôs capabilities and shortcomings. Now, suppose we also extend this system to include different tools for retrieval and a reflection loop to self-correct those retrieved documents (i.e. [16]); every additional component that gets added increases the scope of agent activity, the interaction effects, and the multitude of decisions that need to be understood and evaluated. In this paper, we attempt to realize this evolution in evaluation capabilities. We compare different classes of evaluation architectures (Figure 1) with the explicit goal of detecting decisive, trajectory- altering faults in multi-component systems. First, we extend and structure the reasoning capabilities of our LLM-as-a-judge by introducing Evaluators. Second, we introduce iterative loops by which our Judge can reflect on the confidence scores of the Evaluators and adjust its own assessments. We test these capabilities on the Who&When benchmark [7], which consists of agentic logs and agent-step fault pairs for a set of queries from GAIA [17] and AssistantBench [18]. Our experiments demonstrate 2 that structured iterative reasoning provides a significant and robust performance advantage over existing methods, consistently across div", "response": "We have reached a critical roadblock in the development and enhancement of long-horizon, multi-component LLM agentic systems: it is incredibly tricky to identify where these systems break down and why. Evaluation capabilities that currently exist today (e.g., single pass LLM-as-a-judge) are limited in that they often focus on individual metrics or capabilities, end-to-end outcomes, and are narrowly grounded on the preferences of humans. We argue that to match the agentic capabilities, evaluation frameworks must also be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons. In this paper, we present RAFFLES - an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself, thereby building a history of hypotheses. We tested RAFFLES against several baselines on the Who&When dataset, a benchmark designed to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure. RAFFLES outperforms these baselines, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset (a substantial increase from the previously published best of 16.6%) and over 20% on the Hand-Crafted dataset (surpassing the previously published best of 8.8%). These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review."}
{"prompt": "Title: Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework\n\n1. Introduction Among all essential macro-nutrients, nitrogen (N) defi- ciency represents a major constraint on plant growth, devel- opment, and productivity [1]. As a fundamental component of amino acids, proteins, nucleic acids, and chlorophyll [2], nitrogen plays a central role in multiple physiological and metabolic processes. Its deficiency disrupts these pathways, resulting in reduced leaf area, chlorosis, lower leaf count, and stunted plant height [3]. Beyond nutrient limitations, abiotic stressors such as drought and biotic pressures like weed competition frequently co-occur, compounding the negative effects on plant health. For example, water stress re- stricts nutrient mobility and uptake, thereby intensifying the impacts of nitrogen deficiency [4]. In natural environments, plants rarely face single stress factors in isolation. Rather, stress events often occur simultaneously or sequentially, interacting in synergistic or antagonistic ways [5, 6]. These multi-stress combinations induce overlapping phenotypic symptoms, complicating efforts to diagnose the underlying causes [7]. Despite this, the majority of plant stress pheno- typing studies have focused on single-stress scenarios, with relatively limited progress in disentangling or classifying coexisting stresses [8, 9, 10]. This gap demands the need ‚àóCorresponding author aswinipatra@gmail.com/akp@nerist.ac.in (A.K. Patra); ls@iitg.ac.in (L. Sahoo) ORCID(s): for advanced tools capable of modeling the intricate, multi- dimensional dynamics of plant responses under combined stress conditions. The advent of imaging and sensor technologies has trans- formed plant research through the development of high- throughput phenotyping platforms [11]. These platforms, driven by computer vision and imaging-based phenomics, facilitate non-invasive and automated monitoring of key morphological and physiological traits [12]. Such techniques enable rapid, scalable, and effective assessment of plant health, significantly enhancing yield prediction and stress diagnosis capabilities [13]. Machine learning (ML) [14] and deep learning (DL) [15] have become indispensable tools in this domain, capable of capturing subtle, nonlinear patterns indicative of diverse stress conditions. Among DL models, Convolutional Neural Networks (CNNs) have demonstrated strong performance in extracting spatially significant fea- tures from RGB, hyperspectral, and thermal imagery [16]. When integrated with temporal modeling frameworks like Long Short-Term Memory (LSTM) networks, these models can learn the progression of stress responses over time. Recent research reflects a growing interest in nitrogen stress detection under both isolated and combined stress conditions. Clarke et al. [17] examines how spatial and temporal soil variability influences nitrogen use efficiency (NUE) in wheat using the Sirius crop simulation model and long-term field data. It finds that soil electrical conductivity First Author et al.: Preprint submitted to Elsevier Page 1 of 13 arXiv:2509.06625v1 [cs.CV] 8 Sep 2025 (ECa) can guide site-specific nitrogen management, with lower water-holding soils requiring less nitrogen but posing higher leaching risks. Sarkar et al. [18] investigates how abi- otic stressors‚Äîespecially drought and temperature‚Äîaffect nitrogen dynamics and crop productivity in dryland for- age systems. Using field data and machine learning (ML) analysis, the study compares conventional tillage and no- till practices, along with the impact of green manures such as field peas. The results show that no-till systems with green manuring significantly improve nitrogen use efficiency (NUE) and reduce the negative effects of drought on plant growth. Combining SPAD data from multiple leaf positions significantly improves the estimation of the Nitrogen Nu- trition Index (NNI), as demonstrated in another study by Wang et al. [19], where machine learning models like Ran- dom Forest and XGBoost outperformed linear regression in predicting NNI. A spatio-temporal spectral framework combining RGB, infrared, hyperspectral data and derived plant traits like canopy cover, height, biomass, and vege- tation indices to detect drought, nitrogen, and weed stress in sugar beet. Machine learning models, especially SVM, showed high accuracy with multi-modal features outper- forming single ones [20]. A reinforcement learning (RL) environment was developed by Kallenberg et al. [21] where agents learn crop management policies through crop growth models. In a nitrogen management case study for winter wheat, the RL agent successfully detected crop nitrogen requirements by analyzing growth states and guided optimal fertilizer application. Ghazal et al. evaluates machine learn- ing models for nitrogen stress detection in maize using RGB images under field conditions. Among tested models, Ef- ficientNetB0 achieved the highest accuracy, outperforming vision transformers and other CNNs[22]. A study developed machine learning and de", "response": "Plants in their natural habitats endure an array of interacting stresses, both biotic and abiotic, that rarely occur in isolation. Nutrient stress-particularly nitrogen deficiency-becomes even more critical when compounded with drought and weed competition, making it increasingly difficult to distinguish and address its effects. Early detection of nitrogen stress is therefore crucial for protecting plant health and implementing effective management strategies. This study proposes a novel deep learning framework to accurately classify nitrogen stress severity in a combined stress environment. Our model uses a unique blend of four imaging modalities-RGB, multispectral, and two infrared wavelengths-to capture a wide range of physiological plant responses from canopy images. These images, provided as time-series data, document plant health across three levels of nitrogen availability (low, medium, and high) under varying water stress and weed pressures. The core of our approach is a spatio-temporal deep learning pipeline that merges a Convolutional Neural Network (CNN) for extracting spatial features from images with a Long Short-Term Memory (LSTM) network to capture temporal dependencies. We also devised and evaluated a spatial-only CNN pipeline for comparison. Our CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively surpassing the spatial-only model's 80.45% and other previously reported machine learning method's 76%. These results bring actionable insights based on the power of our CNN-LSTM approach in effectively capturing the subtle and complex interactions between nitrogen deficiency, water stress, and weed pressure. This robust platform offers a promising tool for the timely and proactive identification of nitrogen stress severity, enabling better crop management and improved plant health."}
{"prompt": "Title: Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning\n\nIntroduction In recent years, many of the most effective machine learning paradigms have capitalized on vast amounts of unlabeled or weakly labeled data. Similarly, in dynamic systems learning, Offline GCRL has emerged as a pivotal framework, enabling the use of large-scale, multitask datasets without requiring explicit reward annotations. Specifically, Offline RL [1, 2] leverages passively collected trajectories to learn control policies, offering great promise for applications such as autonomous navigation, locomotion, and manipulation, where interactive training is usually costly and unsafe. GCRL [3, 4] extends this capability by enabling learning across diverse datasets without explicit rewards. Despite its potential, Offline GCRL faces significant challenges, including accurate Goal- Conditioned Value Function (GCVF) estimation from limited data, policy extraction from imperfect value functions, and generalization to unseen state-goal pairs [5]. Among these issues, GCVF 1https://github.com/VittorioGiammarino/PI-HIQL Preprint. Under review. arXiv:2509.06782v1 [cs.LG] 8 Sep 2025 (a) Pi-HIQL (ours) (b) HIQL [10] Figure 1: Contour plots of the GCVF for antmaze-giant-navigate-v0 in [11], learned after 100,000 training steps by our Pi algorithm Pi-HIQL, and the standard HIQL. The plots are generated by varying the agent‚Äôs center of mass x-y coordinates while keeping all other states fixed. Recall that the policy œÄ is trained to move the agent in the direction that maximizes the GCVF. The effects of the Eikonal regularizer are evident in Fig. 1a, where the contour plot closely follows the maze structure, in contrast to Fig. 1b, where the learned GCVF ignores the maze structure. estimation remains the most fundamental, as improvements in this area can enhance both policy extraction and generalization, ultimately advancing the entire field of Offline GCRL. Pi inductive biases offer a promising direction for improving GCVF estimation in the offline setting. As demonstrated in prior work [6], Pi methods can introduce physically or geometrically meaningful structure into the learned value function, enhancing sample efficiency and generalization. In Fig. 1, we illustrate a representative GCRL task in which an agent must navigate from various starting positions in a maze to a specified goal. Fig. 1b shows the contour plot of a GCVF learned by a non-Pi state-of-the-art (SOTA) algorithm. The resulting value function fails to robustly encode obstacle constraints, leading to suboptimal policies that often fail to reach the goal. These limitations motivate the use of Pi regularizers as a principled means to incorporate structural priors into value learning, and thus improve GCVF estimation and policy reliability in complex environments. The primary contribution of this work is the introduction of a Pi Eikonal regularizer for GCVF estimation in Offline GCRL tasks. Inspired by the Eikonal PDE [7], this regularizer imposes a distance-like cost-to-go structure on the learned GCVF, serving as an effective inductive bias during training. By enforcing this structure, the regularizer improves value estimation accuracy and promotes generalization to unseen states, while also reducing the number of required training steps compared to non-Pi approaches (see Fig. 1a). In contrast to Hamilton-Jacobi-Bellman (HJB) PDE-based methods [6], which require explicit system dynamics and often suffer from numerical instability [8, 9], our method is model-free and easy to implement. Empirically, it outperforms both HJB-regularized and unregularized baselines while adding only minimal computational overhead. To validate the effectiveness of our Eikonal regularizer, we integrate it into the HIQL framework [10], a current SOTA algorithm for Offline GCRL. We refer to our method as Pi-HIQL. This choice is motivated by HIQL‚Äôs strong baseline performance, making it an ideal candidate for demonstrating the benefits of our approach. In our evaluation, conducted on the challenging OGbench benchmark [11], we compare Pi-HIQL against Quasimetric RL (QRL) [12], Contrastive RL (CRL) [13], and standard HIQL. Pi-HIQL consistently outperforms or matches the performance of HIQL and other baselines across most tasks, achieving SOTA results particularly in large-scale navigation and trajectory stitching scenarios. These gains underscore the utility of the Eikonal regularizer in enhancing GCVF estimation and overall Offline GCRL performance, with limited exceptions in tasks involving complex object interactions. 2 Related work To the best of our knowledge, Pi regularization for value estimation has only recently been explored. Notably, Lien et al. [6] propose an Offline RL objective derived from the HJB equation in continuous- time optimal control [8, 9], aiming to enforce first-order derivative consistency within the critic network. In contrast, we introduce a simpler, model-free Pi regularizer for GCVF learning, based on 2 the residual of the Eikonal PDE. We sho", "response": "Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise for domains such as autonomous navigation and locomotion, where collecting interactive data is costly and unsafe. However, it remains challenging in practice due to the need to learn from datasets with limited coverage of the state-action space and to generalize across long-horizon tasks. To improve on these challenges, we propose a Physics-informed (Pi) regularized loss for value learning, derived from the Eikonal Partial Differential Equation (PDE) and which induces a geometric inductive bias in the learned value function. Unlike generic gradient penalties that are primarily used to stabilize training, our formulation is grounded in continuous-time optimal control and encourages value functions to align with cost-to-go structures. The proposed regularizer is broadly compatible with temporal-difference-based value learning and can be integrated into existing Offline GCRL algorithms. When combined with Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed HIQL (Pi-HIQL), yields significant improvements in both performance and generalization, with pronounced gains in stitching regimes and large-scale navigation tasks."}
{"prompt": "Title: The First Voice Timbre Attribute Detection Challenge\n\nIntroduction Voice timbre, or voice quality, is inherently perceptual, reflecting the psycholog- ical impression generated by a physical stimulus, and is determined by both the listener and the voice [1]. It has been widely researched across disciplines such as acoustics [2], medicine [3], and psychology [4]. Specifically, within acoustics, voice recognition has been extensively studied, primarily focusing on the similarity be- tween voice characteristics [5]. Recently, voice explainability has emerged as a critical factor in speech technologies, such as speech generation [6] and forensic voice analysis [7]. The first voice timbre attribute detection (vTAD) challenge [8] was held at the National Conference on Man-Machine Speech Communication 2025 (NCMMSC2025) conference, with the objective of explaining voice timbre from the perspective of human impression. In this challenge, a set of timbre descriptors derived from sensory attributes across various modalities was built, including sound (hoarse, ‚ãÜThis work was supported in part by the National Key Research and Development Program of China Project 2024YFE0217200, the Innovation and Technology Fund of the Hong Kong SAR MHP/048/24, the National Natural Science Foundation of China under Grant U23B2053, and the Fundamental Research Funds for the Central Universities WK2100000043. arXiv:2509.06635v1 [cs.SD] 8 Sep 2025 2 L. Chen et al. rich), vision (bright, dark), texture (soft, hard), physical attribute (magnetic, transparent), and so on. Given a pair of speech utterances and a specified de- scriptor dimension, systems were developed to compare the intensity of the two utterances along that dimension. Finally, in this challenge, six teams submit- ted their outputs, with five providing descriptions of their methodologies. This paper summarizes the NCMMSC2025-vTAD challenge, including the task and submitted systems. 2 Task This section describes the challenge task in terms of task definition, dataset, and evaluation metrics. 2.1 Task definition In this task, a set of timbre descriptors V was defined. Given two utterances from distinct speakers A and B, denoted as OA and OB, and a specified timbre descriptor v ‚ààV, participants were required to develop one or more vTAD systems to compare the intensity of OA and OB in v. Mathematically, a hypothesis about the intensity difference was defined as H (‚ü®OA, OB‚ü©, v), meaning that OB was stronger than OA in the descriptor di- mension v. Specifically, H ‚àà{0, 1}, where H = 1 indicated that the hypothesis H was correct, and H = 0 indicated that the hypothesis was incorrect. Partici- pants were required to provide two types of output: the score sv ‚ü®A,B‚ü©, indicating the likelihood that the hypothesis H (‚ü®OA, OB‚ü©, v) = 1 was true, and a deci- sion about whether the hypothesis H (‚ü®OA, OB‚ü©, v) = 1 was true. The likelihood scores and decisions were measured with equal error rate (EER) and accuracy (ACC), respectively. Moreover, in this challenge, two evaluation tracks were defined, including unseen and seen. In the unseen scenario, the speakers used in the evaluation phase were not present in the training phase. In the seen scenario, the speakers employed in the evaluation phase were applied in the training phase, while dis- tinct utterances were utilized for training and evaluation, respectively. Moreover, given a specific speaker, the ordered pairs composed with different speakers were exclusively used in training and evaluation. 2.2 Dataset The challenge utilized the VCTK-RVA dataset [6], wherein the publicly available VCTK database was annotated for timbre intensity. In the dataset, 18 timbre descriptors were defined in V, as listed in Table 1. In total, 101 speakers were involved, forming 6,038 annotated ordered speaker pairs {Speaker A, Speaker B, voice attribute v}, indicating that Speaker B was stronger than Speaker A in the specific descriptor v. The number of descriptor dimensions annotated for each ordered speaker pair ranged from 1 to 3. The First Voice Timbre Attribute Detection Challenge 3 Table 1: The descriptor set used for describing the timbre. The Trans. column gives the corresponding Chinese word. The Perc. column presents the percentage (%) of each descriptor in the VCTK-RVA dataset. The descriptors shrill and husky are exclusively annotated for female and male, respectively. Descriptor Trans. Perc. Descriptor Trans. Perc. Bright Êòé‰∫Æ 17.10 Thin ÂçïËñÑ 13.03 Coarse Á≤ó 11.62 Slim ÁªÜ 11.31 Low ‰ΩéÊ≤â 7.43 Pure Âπ≤ÂáÄ 5.48 Rich ÂéöÂÆû 4.71 Magnetic Á£ÅÊÄß 3.64 Muddy ÊµëÊµä 3.59 Hoarse Ê≤ôÂìë 3.32 Round ÂúÜÊ∂¶ 2.48 Flat Âπ≥Ê∑° 2.15 Shrill (female only) Â∞ñÈîê 2.08 Shriveled Âπ≤Áò™ 1.74 Muffled Ê≤âÈó∑ 1.44 Soft ÊüîÂíå 0.82 Transparent ÈÄöÈÄè 0.66 Husky (male only) Âπ≤Âìë 0.59 In this challenge, the VCTK-RVA dataset was partitioned for training and evaluation, respectively. The evaluations were defined in two tracks regarding whether the test speakers were seen or not in the training. In the seen evalua- tion track, the speakers were included in the training data, while the eval", "response": "The first voice timbre attribute detection challenge is featured in a special session at NCMMSC 2025. It focuses on the explainability of voice timbre and compares the intensity of two speech utterances in a specified timbre descriptor dimension. The evaluation was conducted on the VCTK-RVA dataset. Participants developed their systems and submitted their outputs to the organizer, who evaluated the performance and sent feedback to them. Six teams submitted their outputs, with five providing descriptions of their methodologies."}
{"prompt": "Title: Neutron Reflectometry by Gradient Descent\n\nIntroduction Neutron reflectometry (NR) is a widely used technique for the nanometer scale characterisation of soft matter [1], thin films [2] and bio-materials [3]. Increasingly, this has involved the study of more complex layered structures and ones that are evolving in time (dynamic). The application of NR has lead to many high-profile breakthroughs in the physical and biological sciences. Polarized neutron reflectivity (PNR) was crucial for understanding the oscillatory exchange coupling behaviour so fundamental to giant magnetoresistance (GMR) [4], a pivotal modern technology. Ongoing work has investigated how surfactant mixtures perform at varying temperatures using surface tension and neutron reflectivity [5], this has been crucial for developing more efficient, low-temperature detergents, vital for reducing the carbon footprint of washing laundry. Ongoing NR studies have also been important for the study of biological membranes [6], their interaction with drugs [7], such as antimicrobial peptides and other antibiotics. NR is a scattering technique that illuminates a sample of interest using thermal neutrons produced and moderated from either a reactor or a spallation neutron source. NR measures the specular reflectivity R as a function of the momentum transfer Q from counting neutrons using a detector [8]. Theoretically, the forward reflectivity model of complex materials is well understood. Given the physical and apparatus parameters Œ∏, one can appeal to Fresnel reflectivity to compute the predicted specular reflectivity ÀÜR. NR is however, inherently an indirect technique, which requires the reflectivity data to be modelled, as it is the absolute square of Fourier transform of the scattering length density, which gives rise to a loss in phase information. The parameters of interest cannot be directly accessed from measurements of the reflectivity. Fitting a parametric model of the sample is the inverse problem at the core of NR measurements and data analysis. In the four decades since the widespread adoption of NR as a measurement technique, many methods have been proposed to solve the inverse modelling problem. Until recently, advances in NR analysis and fitting have kept close pace with the progress in machine learning (ML). The rapid rise of deep learning, automatic differentiation and specialised computation libraries has revolutionised the scalability of ML in many other fields. Yet, these analysis routes have so far been very slow to translate to NR fitting. Among the leading software packages for NR analysis [9, 10], the underlying fitting routines are often based on traditional data optimisation techniques such as Levenberg-Marquardt (LM) [11, 12], Broyden-Fletcher-Goldfarb-Shanno (BFGS) [13], or heuristic techniques such as differential evolution [14] or particle swarms [15]. Other approaches follow Bayesian methodologies such as Markov-chain Monte-Carlo (MCMC) [16, 17] or nested sampling [18]. The authors are not currently aware of any widely-used NR software that fully utilises the power of modern automatic-differentiation and gradient-based optimisation techniques to directly solve the inverse problem. Although no direct applications of automatic-differentiation are to be found in the literature, surrogate models have very recently been proposed to replace the inverse problem [19‚Äì22] with ML. These ML models supplant the forward reflectivity model and are trained to predict the parameters of interest directly from the measured R. Although such approaches are able to leverage advanced ML technologies, it is inevitable that physical intuition is lost when replacing governing equations with fast machine learning. Instead, we propose a more robust approach to efficiently solve the NR inverse problem by evaluating the gradient of the forward physical model itself with respect to the parameters of interest. In this paper, we demonstrate that modern tensor computation libraries supporting automatic differentiation are able to quickly compute these derivatives, unlocking the potential to leverage a host of powerful optimisation and inference techniques that remain thus far unexploited in the context of neutron reflectometry. The principal contributions of this work are: ‚Ä¢ The gradient of the forward specular reflectivity of a thin multilayer film (w.r.t the unknown parameters of interest) is computed using automatic differentiation techniques. ‚Ä¢ The gradient information is supplied to cutting edge ML optimisation (stochastic gradient descent) and inference (Hamiltonian Monte-Carlo, variational inference) techniques, previously unexploited in the context of NR inverse problems. 2 Neutron Reflectometry by Gradient Descent PREPRINT Figure 1: The inverse problem at the heart of NR. Both the sample and the reflectometer instrument are described by unknown parameters Œ∏. By selection of an appropriate error function e, the identification of the unknown parameters is reduced to an optimisation problem.", "response": "Neutron reflectometry (NR) is a powerful technique to probe surfaces and interfaces. NR is inherently an indirect measurement technique, access to the physical quantities of interest (layer thickness, scattering length density, roughness), necessitate the solution of an inverse modelling problem, that is inefficient for large amounts of data or complex multiplayer structures (e.g. lithium batteries / electrodes). Recently, surrogate machine learning models have been proposed as an alternative to existing optimisation routines. Although such approaches have been successful, physical intuition is lost when replacing governing equations with fast neural networks. Instead, we propose a novel and efficient approach; to optimise reflectivity data analysis by performing gradient descent on the forward reflection model itself. Herein, automatic differentiation techniques are used to evaluate exact gradients of the error function with respect to the parameters of interest. Access to these quantities enables users of neutron reflectometry to harness a host of powerful modern optimisation and inference techniques that remain thus far unexploited in the context of neutron reflectometry. This paper presents two benchmark case studies; demonstrating state-of-the-art performance on a thick oxide quartz film, and robust co-fitting performance in the high complexity regime of organic LED multilayer devices. Additionally, we provide an open-source library of differentiable reflectometry kernels in the python programming language so that gradient based approaches can readily be applied to other NR datasets."}
{"prompt": "Title: Long-Range Graph Wavelet Networks\n\nIntroduction Long-range interactions are central to complex systems, from electron correlations in quantum chemistry [2, 25] to allosteric effects in biology [12, 48]. In graph neural networks (GNNs) [21, 40, 19, 8], capturing these interactions requires models that can propagate information beyond local neighborhoods without the computational overhead of dense global interactions [1, 14]. Wavelet-based graph neural networks (WGNNs) [22], inspired by wavelet theory [33], provide a principled framework for this challenge, defining spectral filters that can in principle capture different scales of information propagation through their filtering characteristics. However, designing exact wavelet filters face a fundamental computational bottleneck, as it would require computing the full eigenvalue decomposition of the graph Laplacian, which is prohibitive for large graphs. To circumvent this cost, current WGNNs [22, 47, 28] approximate wavelet filters using low-order polynomials. While computationally efficient, this approach suffers from two critical limitations. First, polynomial filters aggregate information only within a finite number of hops [4], inherently restricting their spatial reach. More fundamentally, polynomial approximations face an inherent trade-off between computational efficiency and functional expressiveness: while high-degree polynomials can theoretically approximate any function defined on the real interval (Weierstrass theorem), practically feasible orders cannot accurately represent the discontinuous or steep characteristics needed for selective frequency filtering, such as wavelets [18]. Achieving both computational efficiency and sufficient functional expressiveness requires rethinking wavelet filter parametrization. Current approaches fail to resolve this trade-off: low-order polynomials confine propagation to local neighborhoods (Fig. 1a), while higher orders extend the radius at additional computational cost, but still fail to achieve global propagation (Fig. 1b). In contrast, the exact solution via full EVD enables global information flow but at prohibitive cost (Fig. 1c). Preprint. arXiv:2509.06743v1 [cs.LG] 8 Sep 2025 Low-order polynomial œÅ = 20 Full EVD k = N = 2503 (Ours) Low-order poly. + Partial EVD , œÅ = 8 k = 12 Node where signal is localized Propagated signal value -1 +1 Global prop. Efficiency (a) (b) (c) (d) Global prop. Efficiency Global prop. Efficiency Global prop. Efficiency High-order polynomial œÅ = 50 Figure 1: Signal propagation under different Mexican hat wavelet filter approximations. (a) Low-order poly- nomial (œÅ = 20) restricts propagation to local neighborhoods. (b) Higher-order polynomial (œÅ = 50) extends reach but remains spatially bounded. (c) Full EVD (k = N = 2503) enables global propagation at prohibitive cost. (d) LR-GWN (œÅ = 8, k = 12) achieves global propagation with minimal computational overhead. We propose Long-Range Graph Wavelet Networks (LR-GWN), which overcome the limitations of polynomial-based WGNNs through a hybrid filter design. Our method decomposes each wavelet filter into two components: a low-order polynomial for efficient local aggregation and a spectral filter operating on the low-frequency eigenspace to enable long-range propagation (Fig. 1d). This approach requires only a partial eigendecomposition at preprocessing time, making it computationally practical while achieving the global reach that purely polynomial methods cannot provide. LR-GWN can operate under strict wavelet theory or with relaxed constraints for enhanced performance, providing a principled yet flexible framework for wavelet-based long-range graph learning. Our contributions are: (i) a hybrid parametrization combining polynomial spatial filters with learnable spectral filters operating on truncated eigenspaces; (ii) a principled wavelet framework that maintains theoretical admissibility constraints while optionally allowing relaxation when needed; (iii) an efficient implementation requiring only partial EVD, adding minimal preprocessing overhead; and (iv) state-of-the-art performance among wavelet-based GNNs on long-range benchmarks. 2 Background We recall the main tools from spectral graph theory and wavelet analysis; we refer the reader to Section A for a detailed exposition on these topics. Graphs. We consider undirected graphs G = (A, X) with adjacency matrix A ‚àà{0, 1}n√ón and node features X ‚ààRn√ód. Throughout we use the symmetrically normalized Laplacian L = I ‚àíD‚àí1 2 AD‚àí1 2 , with D = diag(A1), which admits eigendecomposition L = UŒõU ‚ä§ with orthogonal eigenvectors U ‚ààRn√ón and eigenvalues Œõ = diag(Œª1, . . . , Œªn) ‚ààRn√ón, 0 = Œª1 ‚â§¬∑ ¬∑ ¬∑ ‚â§Œªn ‚â§2 [10]. The eigenvectors of L form an orthogonal basis, often called the graph Fourier basis. Any graph signal x ‚ààRn can be decomposed in this basis via the graph Fourier transform (GFT) as bx = U ‚ä§x, with inverse GFT x = U bx. Spectral filtering ap- plies a kernel g : R ‚ÜíR by modulating its Fourier coefficients: g ‚àóG", "response": "Modeling long-range interactions, the propagation of information across distant parts of a graph, is a central challenge in graph machine learning. Graph wavelets, inspired by multi-resolution signal processing, provide a principled way to capture both local and global structures. However, existing wavelet-based graph neural networks rely on finite-order polynomial approximations, which limit their receptive fields and hinder long-range propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which decompose wavelet filters into complementary local and global components. Local aggregation is handled with efficient low-order polynomials, while long-range interactions are captured through a flexible spectral domain parameterization. This hybrid design unifies short- and long-distance information flow within a principled wavelet framework. Experiments show that LR-GWN achieves state-of-the-art performance among wavelet-based methods on long-range benchmarks, while remaining competitive on short-range datasets."}
{"prompt": "Title: MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML\n\nINTRODUCTION LLMs possess broad world knowledge, general perceptual as well as reasoning abilities, making them promising few-shot learners (Brown et al., 2020). However, they often fail to learn new tasks despite being given many-shot demonstrations on standard ML benchmarks (Agarwal et al., 2024; Gardner et al., 2024), or to fully exploit rich experiences stored in agent memory in interactive, open-ended workflows (Wang et al., 2023; Shinn et al., 2023; Wang et al., 2024). In addition, accuracy gains typically plateau ‚Äîoften after just a handful of demonstrations‚Äî and are sensitive to the label biases and choice/order of examples (Chen et al., 2023; Liu et al., 2024a; Zhao et al., 2021; Fei et al., 2023). In practice, LLMs are largely guided by surface-level signals, such as distributional, formatting cues (Min et al., 2022) and nearest-neighbor imitation (Agarwal et al., 2024), and rarely uncover new causal mechanisms or statistical dependencies that are required to yield accurate predictions. Orthogonally, pioneering tabular models (Hollmann et al., 2025; Qu et al., 2025) have demonstrated ML tasks can be solved purely by ICL‚Äîwithout gradient descent. However, these tabular-specific model architectures cannot leverage the broad prior world knowledge and general-purpose multi- modal perception that LLMs acquire during pretraining; consequently, they depend heavily on well- designed featurization (Shi et al., 2021; Mr√°z et al., 2025) and abundant labeled data for training. At this intersection, we ask: Can we teach an LLM to ‚Äúdo ML in context‚Äù while preserving general abilities? To this end, we propose a pretraining-plus-prompting framework, MACHINELEARNINGLM, that equips LLMs with in-context ML capabilities to fully exploit many-shot in-context examples. MA- CHINELEARNINGLM performs LoRA-based (Hu et al., 2022) continued pretraining with a stan- dard next-token objective on millions of synthetic tabular prediction tasks drawn from SCM-based priors (Peters et al., 2017; Pearl, 2009): a task generator samples arbitrarily large numbers of bina- ry/multiclass tasks under SCM-based priors spanning diverse feature types, marginal distributions, and label mechanisms following (Qu et al., 2025). This approach ensures strict non-overlap between our pretraining data and evaluation datasets. After pretraining, the model can directly leverage in- context examples of a new task to generate predictions for unseen instances‚Äîwithout parameter update. As summarized in Table 1, unlike previous instruction-tuning methods (Wang et al., 2022c; Chung et al., 2022) that rely on a limited set of real tasks (typically ‚àº103), we train on O(106) synthetic tasks with diverse causal mechanisms and varied shot counts. Our approach also differs from spe- cialized tabular learners as we preserve the versatility of LLMs, which enables them to continue to leverage contextual task descriptions, draw on external knowledge, and interact directly with multimodal, heterogeneous inputs. Through large-scale pretraining, our method is able to equip LLMs with striking many-shot scaling and robust numerical modeling. We hope MACHINELEARN- INGLM is a promising paradigm to inspire new research that leverages LLMs‚Äô general perception and reasoning capabilities, and also possibly extending to more modalities beyond text, as detailed in Section 6. Table 1: Comparison across ML paradigms. Method In-context learning Robust numerical modeling General knowledge priors Native multimodal input RF, Boosted trees ‚úó ‚úì ‚úó ‚úó TabPFN, TabICL ‚úì ‚úì ‚úó ‚úó TabLLM ‚úó ‚úó ‚úì ‚úì GPT-5, Qwen ‚úì ‚úó ‚úì ‚úì MACHINELEARNINGLM ‚úì ‚úì ‚úì ‚úì Key: ‚úìYes, ‚úóNo. Note: ‚ÄúNative multimodal‚Äù primarily means textual + numerical + tabular and can be naturally extended to modalities like images by building on multimodal LLM backbones. 2 Work in Progress Warm-up training with a Random Forest teacher. Directly training on synthetic tasks can lead to model collapse or underperformance, particularly when tasks are too complex to be learn from only a few (e.g., 64) examples. This makes strong ML methods no better than random guessing or always predicting the majority class. We stabilize the onset by mimicking a random-forest (RF) teacher on each task‚Äîfirst matching example predictions‚Äîbefore transitioning to self-reliant in- context prediction. This leverages knowledge distillation for improved optimization (Hinton et al., 2015). We choose a random forest teacher for its balance of robustness and interpretability. Its decision process can be transparently decomposed into rule paths and feature attributions and di- rectly serialized into interpretable ‚Äúreasoning steps‚Äù‚Äî‚Äìthen rule chains and faithful local expla- nations (Deng, 2014; Friedman & Popescu, 2008; Lundberg & Lee, 2017), which naturally align with the chain-of-thought (CoT) reasoning. In this work, we distill the predicted labels only, but future directions could leverage its ‚Äúreasoning steps‚Äù as rationales for reasoning-augmented train- ing (DeepSeek-AI,", "response": "Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows. Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference. Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU."}
{"prompt": "Title: Automated Hierarchical Graph Construction for Multi-source Electronic Health Records\n\nIntroduction The widespread adoption of Electronic Health Record (EHR) systems has led to the accumulation of rich clinical data across healthcare institutions, creating substantial op- portunities for predictive modeling (Lipton, 2015; Rajkomar et al., 2018), decision support (Federico et al., 2015; Chunchu et al., 2012), and knowledge discovery (Gu et al., 2021). Despite this promise, the potential of EHR data remains underutilized in multi-institutional settings because of fundamental obstacles in how medical codes are adopted and organized (Hripcsak and Albers, 2013; Kush et al., 2008). These obstacles limit interoperability and interpretability, and they impede the portability of statistical and machine learning models across health systems. The first major challenge in multi-institutional EHR analysis is the widespread use of local, institution-specific codes. Many health systems rely on internally defined identifiers for varying proportion of diagnoses, medications, and laboratory tests that do not map cleanly to standardized vocabularies such as the International Classification of Diseases (ICD) for disease conditions (Organization et al., 1978; Organization, 2004), RxNorm for standardized clinical drug nomenclature (Bennett, 2012), and Logical Observation Identi- fiers Names and Codes (LOINC) for laboratory tests (McDonald et al., 2003). These local codes are also often poorly documented, making them difficult to interpret. As a result, the same clinical concept, such as the C-reactive protein laboratory test, may be repre- sented differently across institutions, creating substantial barriers to cross-system analysis and model generalizability. Overcoming this heterogeneity requires automated, data-driven code harmonization methods that align semantically equivalent concepts across institutions into a shared representation. The second major challenge arises from the excessive granularity and lack of unified hierarchical structure in EHR coding systems, which hinders concept-level aggregation and interpretability in clinical research. Standardized terminologies like ICD-10-CM and LOINC contain tens of thousands of codes, many differing only in minor details such as laterality or specimen type. This leads to artificial fragmentation that complicates gener- alizable analysis (Wu et al., 2019; McDonald et al., 2003). Moreover, widely used systems often lack a unified and clinically coherent hierarchy; for example RxNorm includes multiple overlapping hierarchies curated by different agencies (Pathak and Chute, 2010), including 2 resources within the Unified Medical Language System (UMLS) (Bodenreider, 2004). Man- ual curation of hierarchical mappings is labor-intensive, error-prone, and unsustainable as code systems grow. These limitations highlight the need for automated, data-driven hi- erarchical clustering methods that can infer clinically meaningful structure, recover latent parent concepts, and organize codes into interpretable multi-level taxonomies to support modeling, cohort definition, and other downstream applications. To address these challenges, a growing body of research has focused on learning low- dimensional embeddings of medical codes using empirical usage patterns and existing knowledge graphs Chen et al. (2024); Hou et al. (2014); Li et al. (2016); Hong et al. (2021). For example, simple word2vec-based methods leverage co-occurrence statistics to derive embeddings that reflect functional relationships between codes (Hong et al., 2021). Others incorporate semantic information of the code descriptions by leveraging language models trained on biomedical corpora(Yuan et al., 2022; Zeng et al., 2022), or integrate both co- occurrence statistics and semantic embeddings to improve robustness across domains(Zhou et al., 2022, 2025). While these methods provide strong foundations, embeddings trained independently across institutions often reside in non-aligned geometric spaces, limiting their utility for multi-institutional analysis. Simple ad hoc alignment strategies, such as Procrustes analysis (Kementchedjhieva et al., 2018), are generally insufficient to achieve robust harmonization when coding practices differ substantially. More recent work has explored the use of optimal transport as a principled approach to cross-domain alignment (Perrot et al., 2016; Hoyos-Idrobo, 2020), though its adaptation to multi-institutional EHR data remain underdeveloped. A related line of related research focuses on learning hierarchical representation of med- ical concepts. Curated resources such as the UMLS, RxNorm, and LOINC provide man- ually defined hierarchies that group concepts into clinically meaningful categories. While invaluable, these hierarchies are costly to maintain, inconsistent across domains, and typ- ically exclude the large number of institution-specific local codes common in real-world EHRs. To address these gaps, statistical and machine learning methods have been pro- posed to infer hi", "response": "Electronic Health Records (EHRs), comprising diverse clinical data such as diagnoses, medications, and laboratory results, hold great promise for translational research. EHR-derived data have advanced disease prevention, improved clinical trial recruitment, and generated real-world evidence. Synthesizing EHRs across institutions enables large-scale, generalizable studies that capture rare diseases and population diversity, but remains hindered by the heterogeneity of medical codes, institution-specific terminologies, and the absence of standardized data structures. These barriers limit the interpretability, comparability, and scalability of EHR-based analyses, underscoring the need for robust methods to harmonize and extract meaningful insights from distributed, heterogeneous data. To address this, we propose MASH (Multi-source Automated Structured Hierarchy), a fully automated framework that aligns medical codes across institutions using neural optimal transport and constructs hierarchical graphs with learned hyperbolic embeddings. During training, MASH integrates information from pre-trained language models, co-occurrence patterns, textual descriptions, and supervised labels to capture semantic and hierarchical relationships among medical concepts more effectively. Applied to real-world EHR data, including diagnosis, medication, and laboratory codes, MASH produces interpretable hierarchical graphs that facilitate the navigation and understanding of heterogeneous clinical data. Notably, it generates the first automated hierarchies for unstructured local laboratory codes, establishing foundational references for downstream applications."}
{"prompt": "Title: Lane Change Intention Prediction of two distinct Populations using a Transformer\n\nI. INTRODUCTION With the goal of increasing the safety and efficiency of the driving experience, automakers and governments have in the recent years started to invest more and more in research projects leading to assisted and automated driving technologies with the possible end goal of achieving the full automation of passenger and commercial vehicles. The prediction of human‚Äôs drivers‚Äô next maneuver could greatly impact both safety and efficiency and has the potential of seriously impacting the future of the car industry by improving the path planning capabilities of autonomous vehicles. While most authors approached the problem by selecting a suitable dataset of naturalistic trajectories to test their methods [1] [2] [3] [4] [5] [6], not much research was done regarding the possibility of training a method on a dataset to then deploy it in a region different to the one in which the dataset was collected . In this paper we use the exiD dataset [7] and the Hong Kong dataset, both collected by levelXdata [8], which contain nat- uralistic trajectories recorded on highways/freeways, to train transformer networks to predict lane change maneuver within an upcoming time interval. We will in particular concentrate on the differences in performances between transformers trained on different (combinations of) datasets. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. The paper is structured as follows: in Section II the problem is described and the exiD and Hongkong datasets are presented and briefly discussed in addition to an explanation of the data processing. In Section III transformers are introduced and the task of designing them is described. In Section IV the experiments are explained and the results of the prediction task are presented. In Section IV the results are discussed and interpreted. Finally, Section VI contains our final comments and recommendations for future developments of the research. The work presented in this article is a continuation of the work presented in [9]. For this reason, parts of this paper, images and formulas might resemble or might be taken from the previous work. The results obtained in this paper are, though, completely novel and have not been presented in earlier publications. II. PROBLEM DEFINITION AND INPUT DATA In this work, both the exiD dataset [7] and the Hongkong dataset will be used. The exiD dataset is a dataset of natu- ralistic driving trajectory collected by levelXdata on German highways using drones at a frequency of 25Hz [7]. The whole dataset includes 16 hours of measurement data for a total of 69172 vehicle trajectories recorded on 7 different locations (roads). The Hongkong dataset is a similarly structured data also collected by levelXdata on Hong Kong‚Äôs, China, highways and freeways using drones at a frequency of 30Hz. The whole dataset includes 13.8 hours of measurement data for a total of 99842 vehicle trajectories recorded on 5 different locations (roads). Before proceeding with the processing and labeling of the dataset it is important to understand which scenario is considered in this work, which problem is tackled and how data is used to solve it. In this section these themes will be dealt with and the data preparation will be explained in detail. A. Scenario Definition This work focuses on highway scenarios. The objective is to predict the behavior of a single vehicle (called target vehicle) and in doing so its surrounding environment will also be taken in consideration, see Fig. 1. A maximum of eight surrounding vehicles will be taken in consideration. Both the datasets under consideration present a small number of frames for which two vehicles are listed as alongside on the same side. This happens due to how the data was processed. Given the small amount of data which these cases make up, they were not taken in consideration for prediction (the relative target vehicles are still used as surrounding vehicles for other target vehicles arXiv:2509.06529v1 [cs.LG] 8 Sep 2025 2 Fig. 1: Scenario considered in this work. The target (t) vehicle is surrounded by the right following vehicle (rf), the right alongside vehicle (ra), the right preceding vehicle (rp), the following alongside vehicle (fa), the preceding vehicle (p), the left following vehicle (lf), the left alongside vehicle (la) and the left preceding vehicle (lp). Figure taken from [9]. though). Surrounding vehicles driving on an on-ramp or off- ramp are considered only if their lateral distance to the target vehicles (as it will be defined later) is smaller or equal to 6.0m to account for complex road structures. B. Problem Definition The goal is to predict if the target vehicle will perform a left lane change maneuver (LLC) or left right change maneuver (RLC) a within the next ‚àÜtp,MAX seconds (max- imum prediction time) or if it will perform a lane keeping maneuver (LK), simil", "response": "As a result of the growing importance of lane change intention prediction for a safe and efficient driving experience in complex driving scenarios, researchers have in recent years started to train novel machine learning algorithms on available datasets with promising results. A shortcoming of this recent research effort, though, is that the vast majority of the proposed algorithms are trained on a single datasets. In doing so, researchers failed to test if their algorithm would be as effective if tested on a different dataset and, by extension, on a different population with respect to the one on which they were trained. In this article we test a transformer designed for lane change intention prediction on two datasets collected by LevelX in Germany and Hong Kong. We found that the transformer's accuracy plummeted when tested on a population different to the one it was trained on with accuracy values as low as 39.43%, but that when trained on both populations simultaneously it could achieve an accuracy as high as 86.71%. - This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible."}
{"prompt": "Title: Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks\n\nINTRODUCTION Graph Neural Networks [1], [2], [3] predominantly aggregate localized information, which proves to be insufficient for tasks requiring long-range interactions, like molecular property predictions [4], [5]. The interactions between the multi-hop nodes [6] can be leveraged by enhancing the size of neighborhoods. Yet, the increasing number of neighbors introduces the problem of compressing the growing amount of information in the fixed-dimensional vectors, which creates information bottlenecks. This phenomenon is commonly known as Oversquashing [7], [8] that causes significant information loss, particularly affecting signals from distant nodes, crucial for the downstream task [9]. Limitations of Related Works. Mitigating oversquashing begins with converting the input graph into a fully connected (FA) graph [7] and applying it to either the final layer or every layer of the base GNN model. Recent approaches have attempted to address oversquashing through graph rewiring methods [10], [11], [12], [13], [14], [15], which add edges as ‚Äùshortcuts‚Äù between long-distance nodes to facilitate multi-hop information flow. Nevertheless, FA or rewiring methods tamper with the original graph topology, potentially compromising the inductive bias and incurring information loss. As an alternative avenue, Graph Transformers (GTs) [16] enable global message passing between all pairs of nodes, but they introduce prohibitive computational complexity, particularly for large graphs. More recently, PANDA [17] proposed expanding the feature dimensions of high-centrality nodes. However, this approach increases model parameters and discards potentially valuable information when selecting top-ranked features. Motivation To eliminate those limitations, we design an efficient asynchronous message passing strategy that selectively updates node features across layers, unlike standard message passing architectures like GCN [18], GAT [19], GCNII [20], etc. To the best of our knowledge, we are the first to introduce a novel asynchronous message passing framework to combat oversquashing. Our framework creates node batches in every layer based on some pre-defined criterion and updates features of the nodes belonging to those batches. In contrast, unselected nodes preserve their features from previous layers. Recently, GwAC [21] processes messages from nodes to their neighboring nodes at different times. Yet, this method overlooks the channel capacity and offers no solution to tackle oversquashing. On the contrary, our asynchronous framework explicitly manages fixed-capacity channels by accessing aggregated messages at some particular layer, preventing information overflow. This layer-dependent access prevents simultaneous compression of vast amounts of information, effectively utilizing the same channel capacity without modifying graph topology or increasing the model parameters. ‚Ä¢ Electronics and Communication Sciences Unit, Indian Statistical Institute, Kolkata, India. E-mail: kushalbose92@gmail.com, swagatam.das@isical.ac.in arXiv:2509.06777v1 [cs.LG] 8 Sep 2025 2 Fig. 1: The workflow of CAMP is presented. The degree centrality values are sorted in descending order. For a 3-layered GNN model, the centrality set is divided into three disjoint subsets. In each layer, the features of the nodes belonging to the pertinent subset are updated. Graph adjacency alters at every layer, and the updated node features are carried forward to the next layer. In this work, we propose a novel plug-and-play framework Centrality-aware Asynchronous Message Passing or CAMP that adopts node centrality (degree, betweenness, etc) as our selection criterion to create node batches. CAMP performs ordering (ascending or descending) of the node centrality values and partitions the whole set into multiple disjoint subsets, each corresponding to a distinct message-passing layer. The complete workflow is illustrated in Figure 1. This partitioning ensures that every node gets a chance to exchange messages with its neighbors at some layer. In a network, the high-centrality nodes are regarded as potential bottlenecks in message propagation. By processing these critical nodes earlier, CAMP allows their updated representations to flow to low-centrality nodes in subsequent layers. This approach strategically manages the information flow through these bottleneck nodes. Thus, asynchronous feature updates prevent the simultaneous compression of exponentially growing neighborhood information into fixed-capacity channels, diminishing the effect of oversquashing. We also offer theoretical underpinnings to validate the efficacy of our proposed framework. Unlike PANDA, our framework does not require increasing the dimension of node features, thereby asserting learning in a low-parameter regime. Notably, PANDA performs synchronous message updates, which strictly differ from CAMP. Contribution Our contributions are summarized as: ‚Ä¢ Asynchronous Message Passing We design a gener", "response": "Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when tasks require long-range interactions. The problem arises from the presence of bottlenecks that limit the propagation of messages among distant nodes. Recently, graph rewiring methods modify edge connectivity and are expected to perform well on long-range tasks. Yet, graph rewiring compromises the inductive bias, incurring significant information loss in solving the downstream task. Furthermore, increasing channel capacity may overcome information bottlenecks but enhance the parameter complexity of the model. To alleviate these shortcomings, we propose an efficient model-agnostic framework that asynchronously updates node features, unlike traditional synchronous message passing GNNs. Our framework creates node batches in every layer based on the node centrality values. The features of the nodes belonging to these batches will only get updated. Asynchronous message updates process information sequentially across layers, avoiding simultaneous compression into fixed-capacity channels. We also theoretically establish that our proposed framework maintains higher feature sensitivity bounds compared to standard synchronous approaches. Our framework is applied to six standard graph datasets and two long-range datasets to perform graph classification and achieves impressive performances with a $5\\%$ and $4\\%$ improvements on REDDIT-BINARY and Peptides-struct, respectively."}
{"prompt": "Title: Integrated Detection and Tracking Based on Radar Range-Doppler Feature\n\nIntroduction Detection and tracking are the basic tasks of a radar system. The classic processing chain: targets are first detected from the echo data based on signal energy thresholds, then the tracker interrogates the detector to obtain state information of the measurements above threshold, and finally the number and state of targets are estimated from measurement points. The constant false alarm rate (CFAR) [1] and the Kalman filter (KF) [2] have been proved to be effective algorithms, and a series of subsequent algorithms [3, 4, 5] have been generated, which are applied to various popular frameworks. In light of the information flow relationship between detection and tracking tasks, several studies have explored joint detection and tracking algorithms [6, 7, 8, 9]. The fundamental concept behind these algorithms is to enhance detection performance by modifying detector thresholds using the predicted measurement position and innovations covariance supplied by the tracker. These works let detection and tracking work jointly in an attempt to improve the utilization of information by the model. However, the ability to express information using only traditional model-driven approaches is limited. Considering the detection task as a complex nonlinear transformation, the threshold decision approach is relatively coarse, resulting in the underutilization of certain signal structure information [10]. For example, phase information is lost in the mode-taking operation. In addition, the tracker has limited access to radar signal information [11], making it difficult to associate data in scenarios with high false alarm rates. Another approach is track-before-detect ‚àóThis work was supported in part by the National Natural Science Foundation of China under Grant 61771110 and U19B2017, in part by the Chang Jiang Scholars Program and the 111 Project No. B17008. ‚Ä†Authors‚Äô addresses: University of Electronic Science and Technology of China, School of Information and Communication Engineering, Chengdu, China. ‚Ä°Email address: kussoyi@gmail.com (Corresponding author: Wei Yi) arXiv:2509.06569v1 [eess.SP] 8 Sep 2025 Running Title for Header Radar Detector Tracker Target Position Radar Detector Tracker Data-driven Confidence degree Signal feature Threshold method Figure 1: Conceptual comparison of the integrated detection and tracking methods. (TBD) [12, 13], which utilizes non-coherent integration of multi-frame observation data in the temporal dimension to address the weak target detection problem. The integration process relies on the accuracy of the predefined models. Recently, data-driven approaches have been shown as a promising alternative to traditional methods [14, 15]. Deep learning can train a network to extract the feature information contained in radar echo signals so as to predict the location of targets in complex scenes [16, 17, 18]. Under the premise of ensuring accuracy, reasoning speed has obvious advantages. In addition, MT3 has achieved the same performance as model-based state-of-the-art (SOTA) algorithm Poisson multi-bernoulli mixture filter (PMBM) [5] in the field of radar multi-target tracking [19]. In essence, these data-driven approaches are processed by extracting high-dimensional features of targets, noise, and clutter. Is it possible to make better use of signal data by being data-driven, to improve the information utilization of the integrated detection and tracking model. We notice the problem of multi-object tracking in the field of computer vision. An accurate and stable framework is \"Tracking by detection\"[20, 21], which tracks based on bounding boxes of detection and their appearance feature. The top-performing method (StrongSORT [22]) has achieved SOTA accuracy. Inspired by this, we propose an integrated detection and tracking model based on radar signals. And our motivation stems from the development of radar methods and the completely different challenges of high false alarms, sparse information, and so on. The information flow relationship between the detection task and the tracking task has been better utilized as shown in Fig. 1. We retain the key components of the radar signal processing chain: pulse compression, coherent integration [23]. Raw echoes are processed into radar Range-Doppler (RD) data as model input, incorporating data-driven approach to the classical processing chain. First, the threshold discrimination in the traditional method is replaced by a data-driven detection network that extracts pulse RD features using a deep learning algorithm. Second, following the filter plus data association tracking structure, in order to obtain more a priori information for the tracker, we provide the confidence of the detection results to the filter and the extracted RD feature information to the data association algorithm. Compared to the classical processing flow, our method uses network to extract RD features, which makes fuller use of signal information in detection and tracking.", "response": "Detection and tracking are the basic tasks of radar systems. Current joint detection tracking methods, which focus on dynamically adjusting detection thresholds from tracking results, still present challenges in fully utilizing the potential of radar signals. These are mainly reflected in the limited capacity of the constant false-alarm rate model to accurately represent information, the insufficient depiction of complex scenes, and the limited information acquired by the tracker. We introduce the Integrated Detection and Tracking based on radar feature (InDT) method, which comprises a network architecture for radar signal detection and a tracker that leverages detection assistance. The InDT detector extracts feature information from each Range-Doppler (RD) matrix and then returns the target position through the feature enhancement module and the detection head. The InDT tracker adaptively updates the measurement noise covariance of the Kalman filter based on detection confidence. The similarity of target RD features is measured by cosine distance, which enhances the data association process by combining location and feature information. Finally, the efficacy of the proposed method was validated through testing on both simulated data and publicly available datasets."}
{"prompt": "Title: A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models\n\nI. INTRODUCTION With the advances in information technology, graph- structured data has become a ubiquitous data structure in online services, including social media [11], e-commerce [44], and autonomous agents [9], [23], [26], [34]. This widespread usage has led to a significant increase in various malicious activities, including hacking, spam, and fake news. In order to identify these anomalous entities and behaviors from graph- structured data, graph anomaly detection (GAD) has emerged as an active research topic in recent years. To date, GAD has been applied to a wide range of domains, including but not limited to cybersecurity, financial fraud detection, recommender systems, and social network analysis, where identifying anomalous patterns is crucial for maintaining sys- tem integrity and user trust [3], [16], [19], [25], [32], [37]. ‚Ä†Corresponding author. This research was partly funded by the Australian Research Council (ARC) under grant DP240101547. Despite the growing popularity of GAD, conventional GAD paradigm (Fig. 1(a)) usually operate under well-controlled in vitro settings: On one hand, they typically assume that the training and testing sets are drawn from the same distribution, making them hard to transfer to other domains or unseen data; On the other hand, models are often tailored to a specific GAD task or scenario, limiting their generalizability across different application contexts. These assumptions limit the robustness and flexibility of current approaches in real-world settings, where GAD models are expected to adapt to varying data distributions and diverse scenarios [7], [52]. For example, in the application of fraud detection, where transaction networks continuously evolve over time, the same-distribution assump- tion may hinder GAD models from adapting to emerging fraudulent patterns and distribution shifts. Moreover, in data- scarce or privacy-sensitive scenarios, such as healthcare and finance, training a specific GAD model can be challenging due to limited access to annotated data and strict privacy constraints [21]. In order to enhance the practicality of GAD and extend its applicability to more real-world scenarios, a recently emerging trend is to improve the generalization capability of GAD meth- ods. One promising research direction is to empower GAD methods with transfer learning (Fig. 1(b)), where knowledge from related source datasets is leveraged to improve anomaly detection on a similar target dataset. Using various techniques to learn transferable knowledge and capture target-domain anomaly patterns, recent transfer learning-based GAD methods can exploit data from related domains to build more powerful detection models, thereby enhancing generalization and reduc- ing data dependency [4], [39]. Following the success of large- scale pre-trained models [52], a recent emerging direction is to build foundation models (Fig. 1(c)) for GAD, which are capa- ble of generalizing across diverse anomaly detection scenarios in graph data. Under this direction, advanced approaches have achieved GAD at multiple granularities [13] as well as zero- shot GAD on arbitrary unseen datasets [27]. Despite the growing research trend toward generalization of GAD, there is still no systematic review and categoriza- arXiv:2509.06609v1 [cs.LG] 8 Sep 2025 (c) Foundation Models for GAD Target Dataset Source Dataset Target Dataset (a) Conventional GAD (b) Transfer Learning for GAD Train Test GAD Model Target Dataset Train Adapt GAD Model Training Datasets Target Datasets Train Adapt GAD Model Fig. 1. The learning paradigms of (a) conventional GAD methods; (b) transfer learning for GAD; and (c) foundation models for GAD. tion of studies in this field. This highlights the need for a comprehensive survey to organize existing work and guide future research. To fill the gap, in this paper, we provide a comprehensive and systematic survey of generalization in GAD. Specifically, the contributions of this paper are: ‚Ä¢ Problem Formulation. We discuss the evolution of gener- alizability in GAD, highlighting the problem formulations and the underlying motivations. ‚Ä¢ Taxonomy. Under the umbrella of two generalized paradigms, namely transfer learning and foundation models, we develop a taxonomy to organize existing generalized GAD approaches into more fine-grained categories. ‚Ä¢ Timely Review. For each category, we offer a comprehen- sive review of recent advances, discussing the underlying motivations and design principles. ‚Ä¢ Future Directions. We outline several open research direc- tions to guide future research of this promising topic. II. PROBLEM FORMULATION AND TAXONOMY In this section, we introduce the notations and problem statement in graph anomaly detection (GAD) and provide an overview of the taxonomy that illustrates increasing levels of generalizability. We begin with the statement of conventional GAD, then move to transfer learning approaches that enhance the generalizability of GAD methods w", "response": "Graph anomaly detection (GAD) has attracted increasing attention in recent years for identifying malicious samples in a wide range of graph-based applications, such as social media and e-commerce. However, most GAD methods assume identical training and testing distributions and are tailored to specific tasks, resulting in limited adaptability to real-world scenarios such as shifting data distributions and scarce training samples in new applications. To address the limitations, recent work has focused on improving the generalization capability of GAD models through transfer learning that leverages knowledge from related domains to enhance detection performance, or developing \"one-for-all\" GAD foundation models that generalize across multiple applications. Since a systematic understanding of generalization in GAD is still lacking, in this paper, we provide a comprehensive review of generalization in GAD. We first trace the evolution of generalization in GAD and formalize the problem settings, which further leads to our systematic taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive review is conducted for the existing generalized GAD methods. Finally, we identify current open challenges and suggest future directions to inspire future research in this emerging field."}
{"prompt": "Title: MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture\n\n1.Introduction Brain tumor is a benign or malignant mass formed by the uncontrolled and abnormal growth of cells in the brain tissue[1, 2]. Today, brain tumors affect hundreds of thousands of people worldwide every year, have high mortality rates, especially with malignant types, and are among the most deadly diseases among central nervous system tumors. In 2024, approximately 25,000 new cases of brain tumors were diagnosed in the USA and a significant proportion of these cases resulted in death[3]. The most common types of brain tumors encountered clinically are glioma, meningioma and pituitary tumors[4]. Glioma originates from glial cells, the support cells of the brain. Structurally heterogeneous, a glioma is usually a rapidly progressing and aggressive tumor[5, 6]. Meningioma is a tumor that develops from the meninges, the membrane tissue surrounding the brain and spinal cord[7]. It is usually benign and tends to grow slowly[8]. However, it is possible for a meningioma to cause various neurological problems by pressing on the brain tissue as it grows. Pituitary Tumor occurs in the pituitary gland, which is one of the important endocrine structures of the brain. It is classified as functional and non- functional according to its hormone secretion status[9]. While functional pituitary tumors affect the hormone levels in the body and cause serious clinical symptoms[10], non-functional pituitary tumors usually cause symptoms that occur as a result of pressure on surrounding tissues[11]. MRI is one of the most common imaging modalities for detecting brain tumors[12, 13]. MRI provides soft tissue contrast with high resolution, allowing detailed examination of the location and size of tumors. However, the analysis of MRI images is a time-consuming and complex process that can vary depending on the interpreter[14]. In particular, the morphological diversity of tumors, difficulties in distinguishing similar tissue densities, and the possibility of missing small lesions are limitations of manual evaluation. Therefore, the need for computer-aided diagnosis (CAD) systems is becoming more and more important. There are various approaches to detect brain tumors in the literature. Swati et al.[15] proposed a convolutional neural network (CNN) model trained from scratch for the classification of brain tumors. The Figshare dataset of T1-weighted contrast-enhanced MRI images was used in the study, and a total of 3064 images were classified into three tumor classes (glioma, meningioma, pituitary). The model architecture consists of five convolution-pool blocks. The classification performance achieved an accuracy of 96.49%, 92.79% precision, 96.33% recall and 94.47% f1 score on the test set. Aamir et al. [16] proposed a Deep Learning(DL) based approach to classify brain tumors using MRI images. In order to improve image quality, illumination boost and non-linear stretching were applied. Then, features extracted with EfficientNet-B0 and ResNet50 models are combined with the Partial Least Squares (PLS) method. Tumor regions were determined with agglomerative clustering and classification was performed with ROI pooling. The study was performed on the Figshare dataset and 98.95% accuracy rate was achieved. Ismail et al. [17] used the ResNet50 deep learning model in their study conducted on the same dataset. They utilized various data preprocessing steps, data augmentation techniques, and transfer learning approaches to improve the model's performance. According to the results reported in the study, 97% accuracy, 98% precision, 97% recall, and an F1-score of 97% were achieved. Hashemzehi et al. [18] proposed a hybrid model combining NADE and CNN methods. In this method, convolutional layers were used instead of fully connected layers. Their architecture consists of two separate networks, and the final layers of these networks are connected to the output layer. The authors achieved recall, precision, and F1 score values of 94.64%, 94.49%, and 94.56%, respectively, with their proposed approach on the same dataset. Sharma et al. [19] proposed a transfer learning based deep learning method for brain tumor classification. In the study, the ResNet50 model, customized by removing the last layer, was used on an other MRI image dataset containing glioma, meningioma, pituitary and normal classes. The performance of the model achieved 92% accuracy, 89% recall and 93% specificity in tumor type classification. Khaliki et al. [20] evaluated different CNN architectures (Inception-V3, EfficientNetB4 and VGG19) on a total of 2870 brain MRI images including four classes: glioma, meningioma, tumor-free and pituitary. In the experiments, the highest performance was achieved by VGG16 with 98% accuracy, 97% F-score, 98% recall and 98% precision. Khan et al. [21] proposed a novel lightweight CNN model called ShallowMRI. The model utilizes an efficient erosion/dilation-based contour extraction", "response": "Brain tumors are serious health problems that require early diagnosis due to their high mortality rates. Diagnosing tumors by examining Magnetic Resonance Imaging (MRI) images is a process that requires expertise and is prone to error. Therefore, the need for automated diagnosis systems is increasing day by day. In this context, a robust and explainable Deep Learning (DL) model for the classification of brain tumors is proposed. In this study, a publicly available Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI images of three tumor types was used. First, the classification performance of nine well-known CNN architectures was evaluated to determine the most effective backbone. Among these, EfficientNetV2 demonstrated the best performance and was selected as the backbone for further development. Subsequently, an attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to enhance its classification capability. The performance of the final model was comprehensively compared with basic CNNs and the methods in the literature. Additionally, Grad-CAM visualization was used to interpret and validate the decision-making process of the proposed model. The proposed model's performance was evaluated using the five-fold cross-validation method. The proposed model demonstrated superior performance with 99.50% accuracy, 99.47% precision, 99.52% recall and 99.49% F1 score. The results obtained show that the model outperforms the studies in the literature. Moreover, Grad-CAM visualizations demonstrate that the model effectively focuses on relevant regions of MRI images, thus improving interpretability and clinical reliability. A robust deep learning model for clinical decision support systems has been obtained by combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy and interpretability in brain tumor classification."}
{"prompt": "Title: Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem\n\nIntroduction Advancing the mathematical reasoning capabilities of Large Language Models (LLMs) is a central goal in AI, yet their performance in rigorous, multi-step logical deduction remains a critical weakness [Li et al., 2024]. This deficiency is largely attributed to a foundational issue: the extreme scarcity of high-quality training data [Wang and Deng, 2020, Xin et al., 2024]. Unlike domains rich with web-scale text or images, the corpus of formalized mathematical proofs is small, expensive to create, and requires specialized human expertise, making it impractical to crowdsource at scale [Wang and Deng, 2020, Trinh et al., 2024]. This data bottleneck severely hampers the training of more capable mathematical reasoners, motivating approaches that are logically grounded, resource-aware, and reproducible. Recently, a promising line of research has focused on synthetic data generation to overcome this scarcity [Yin and Gao, 2025, Wang and Deng, 2020]. However, many of these approaches either generate problems by prompting other LLMs, which carries a significant risk of introducing factual inconsistencies that require complex external verification in Interactive Theorem Provers (ITPs) like Lean or Isabelle [Xin et al., 2024, Wu et al., 2022], or generate random logical formulas that may lack mathematical relevance [Zombori et al., 2019]. In this paper, we propose a different paradigm. Our framework is built in symbiosis with the mature automated theorem proving ecosystem. We use the saturation-based theorem prover E [Schulz, 2002] 1Code and data available at: : https://github.com/sileod/reasoning_core : https://hf.co/datasets/reasoning-core/rc1 Preprint. Under review. arXiv:2509.06809v1 [cs.CL] 8 Sep 2025 not as a solver, but as a generative engine to exhaustively derive logical consequences from the rich TPTP axiom library [Sutcliffe, 2017]. This purely symbolic approach ensures the mathematical validity of every generated theorem by construction. After curating these results for \"interestingness\" with AGInTRater [Puzis et al., 2006], we deconstruct the process of theorem proving into three complementary facets of logical reasoning, each embodied by a distinct task with controllable complexity: 1. Conjecture Entailment: A true/false task testing deductive verification. 2. Minimal Premise Selection: A distractor-based task requiring the identification of the minimal sufficient set of hypotheses 3. Proof Graph Reconstruction: A structural reasoning task that involves ordering shuffled proof steps. Our contribution is not a static dataset but a generative mechanism that produces varied and complex tasks on demand. It provides a granular method for evaluating LLM reasoning depth, isolating the pure logical capabilities of models from the ambiguities of natural language. 2 Related Work Our work addresses the critical need for high-quality training data in automated theorem proving. We position our contribution with respect to two dominant paradigms: LLM-based autoformalization and procedural generation within Interactive Theorem Provers (ITPs). Synthetic Data via LLM-driven Pipelines. A major line of research leverages LLMs to create new training instances, primarily through autoformalization‚Äîtranslating natural language problems into formal systems like Lean [Xin et al., 2024, Wu et al., 2022]. As seen in projects like TheoremLlama [Wang et al., 2024], this paradigm can generate large datasets. However, this \"LLM-in-the-loop\" approach faces significant drawbacks. It relies on the very models it seeks to improve and, because LLM outputs are not guaranteed to be sound, necessitates a computationally expensive \"generate- then-verify\" cycle using an Interactive Theorem Prover (ITP). This dependency not only confines the generation process to a specific and complex proof-assistant syntax (e.g., Lean/Isabelle) but also suffers from the high cost of rejection sampling due to frequent invalid proposals. Furthermore, being trained on human data, LLMs tend to generate problems with a generative bias, favoring familiar structures over more \"exotic\" or diverse logical paths. Our framework avoids these issues entirely: by using saturation within the standardized TPTP ecosystem, we guarantee validity by construction, operate independently of any proof-assistant syntax, and systematically explore the deductive space for maximal diversity. Procedural Generation and Proof Automation in ITPs. A second stream of work focuses on procedural generation and proof automation within ITPs. Similar to our work, Ayg√ºn et al. [2021] also generate synthetic theorems from TPTP axioms using a forward proposer. However, their method uses a simple linear resolution strategy with random choices, which does not guarantee the exploration of complex or diverse logical structures. Our approach is more systematic: by using a full saturation engine (E-prover), we exhaustively explore a part of the deductive closure of the axioms, a process limited", "response": "The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for \"interesting\" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available. https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1"}
{"prompt": "Title: When Secure Isn't: Assessing the Security of Machine Learning Model Sharing\n\nIntroduction In recent years, the adoption of Machine Learning (ML) has grown rapidly, with advanced tools once limited to experts now accessible to a broad audience. Among other factors, this trend is driven by the rise of platforms for sharing pre-trained models [22, 26, 36, 56, 70]. Such models are often advertised as ready-to-use, lowering entry barriers and enabling practitioners with different levels of expertise to incorporate ML into their systems and workflows. In many ways, this trend reflects the evolution of traditional software development, where open-source repositories enable the reuse and adaptation of algorithms, code, and libraries. Today, mod- els are shared and reused, enabling a collaborative ecosystem that is reshaping how ML is practiced. However, unlike the well-studied risks of code sharing and software supply chains [2, 20, 34, 39, 41, 45, 67], the security implications of ML model sharing remain largely underexplored. Prior work on malicious code injection in models [25, 68, 69, 73] and on hub security gaps [32, 33, 35, 73] is fragmented, resulting in limited awareness among users, frame- work developers, and sharing hubs. The immaturity of the model sharing security field is further illustrated by Zhu et al. [74], who show how TensorFlow APIs can be abused for file and network ac- cess at inference time. The growing relevance of the problem is also reflected in the recent DEF CON 33 talk by Parzian [47], which high- lighted persistent risks in model sharing, such as insecure pickle deserialization. Our work addresses the fragmented and immature state of secu- rity research on model sharing, aiming to raise awareness among everyday ML users of its hidden risks, debunk common misconcep- tions, and encourage the security community to study these threats with the same rigor applied to open-source software at large. We consider a threat model where attackers craft malicious model artifacts to compromise a victim‚Äôs system at load time, with arbitrary code execution as the goal. The paper, in its structure, is driven by the research questions defined below. RQ1. What is the security posture of the model sharing mechanisms adopted by popular ML frameworks and hubs? RQ2. Are approaches claiming security guarantees, while offering full model object sharing, actually secure? RQ3. Is the user‚Äôs perception of the security posture consistent with reality, or does the security narrative affect their understanding of the sharing-associated risks? To answer RQ1, we analyze the security of the most popular ML frameworks and hubs, focusing on their sharing approaches and characteristics. We observe that while some of them do not address security at all, others provide security-oriented mecha- nisms based on shifting responsibility to other parts of the sharing workflow or by restricting the expressiveness of the model rep- resentation. Some go further, with certain frameworks providing complete model-sharing capabilities while explicitly promoting themselves as security-oriented, and some hubs emphasizing the same security focus through safeguards such as content scanning. This narrative is reinforced through documentation and naming choices (e.g., Keras‚Äôs ‚Äúsafe mode‚Äù [38]), and is often justified at the framework level by the use of data-based formats, assumed to reduce the risk of arbitrary code execution. arXiv:2509.06703v1 [cs.CR] 8 Sep 2025 arXiv, preprint, 2025 Digregorio et al. This last category led to our RQ2, which motivates a vulnera- bility assessment of the configurations offered by these sharing frameworks and hubs. Our analysis uncovered six 0-day vulnera- bilities (i.e., previously undisclosed), each enabling arbitrary code execution. These findings debunk the widespread assumption that data-based formats (e.g., JSON) are inherently secure when used to share full model objects. Our answer to RQ2 reveals a critical gap between the security narrative and the actual implementation. The lack of public scrutiny and awareness is further evidenced by the low number of pre-existing Common Vulnerabilities and Exposures (referred to as CVEs) for these mechanisms (i.e., before our work). For instance, our findings represent the first officially recognized CVEs targeting Keras‚Äôs ‚Äúsafe mode‚Äù [9, 13]. This understanding led to RQ3. To answer it, we conducted a survey targeting ML practitioners to understand how frameworks and hubs‚Äô security narratives influence user perception. Results worryingly indicate that design terminology and documentation significantly shape users‚Äô sense of security. We conclude our paper with takeaways and suggestions for users and developers, synthesized from the observations and insights obtained through our research questions. Contributions. This paper makes the following contributions: ‚Ä¢ We systematize and analyze the security of ML model-sharing mechanisms, covering both framework-level and hub-level per- spectives. ‚Ä¢ We assess methods that claim security guarante", "response": "The rise of model-sharing through frameworks and dedicated hubs makes Machine Learning significantly more accessible. Despite their benefits, these tools expose users to underexplored security risks, while security awareness remains limited among both practitioners and developers. To enable a more security-conscious culture in Machine Learning model sharing, in this paper we evaluate the security posture of frameworks and hubs, assess whether security-oriented mechanisms offer real protection, and survey how users perceive the security narratives surrounding model sharing. Our evaluation shows that most frameworks and hubs address security risks partially at best, often by shifting responsibility to the user. More concerningly, our analysis of frameworks advertising security-oriented settings and complete model sharing uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through this analysis, we debunk the misconceptions that the model-sharing problem is largely solved and that its security can be guaranteed by the file format used for sharing. As expected, our survey shows that the surrounding security narrative leads users to consider security-oriented settings as trustworthy, despite the weaknesses shown in this work. From this, we derive takeaways and suggestions to strengthen the security of model-sharing ecosystems."}
{"prompt": "Title: Learning spatially structured open quantum dynamics with regional-attention transformers\n\n1 Introduction Open quantum systems are fundamental to the operation of quantum memories, network nodes, repeaters, and light‚Äìmatter interfaces across quantum information science. These devices are realized across diverse platforms including cold atom ensembles[1‚Äì3], atom arrays[4‚Äì6], room-temperature vapor-based devices[7, 8], and engineered light‚Äìmatter interfaces in waveguide[4, 9] or cavity QED[10]. In many of these platforms, spatial propagation and time-dependent driving fields fundamen- tally shape the dynamics, giving rise to rich interplay between coherent quantum evolution, dissipative processes, and structured spatiotemporal behavior. A paradig- matic example is the electromagnetically induced transparency (EIT)-based quantum memory[11‚Äì17], where probe pulses propagate through a spatially extended atomic medium under a time-dependent control field. Such systems are usually modeled by quantum master equation coupled with field propagation equations[12]. Accurate simulation over extended spatiotemporal domains and under time dependent control protocols is computationally intensive[8, 18], particularly when required repeatedly for optimization, network-scale modeling, or real-time experimental feedback. Deep learning is increasingly being explored as a tool for assisting study of physical dynamic systems[19‚Äì21], as well as for assisting quantum information experiments[22]. Prior work has applied neural networks to simulate Lindblad evolution, quan- tum trajectories, and operator dynamics in relatively low-dimensional or few-body settings[23‚Äì26]. Among deep learning approaches, transformer architectures, originally developed for language and vision tasks [27, 28], have shown strong performance in learning physics systems dynamics governed by partial differential equations [29, 30], and have begun to be applied to quantum models[31]. However, most existing studies focus on temporally localized or low-dimensional quantum systems, and do not address quantum systems with spatial propagation, global control protocols, and decoherence. Moreover, the quadratic scaling of standard self-attention mechanisms in sequence length poses a challenge for modeling spatial-temporal quantum systems with fine resolution. Recent advances in scalable attention mechanisms from the computer vision and geoscience communities, including axial attention [32], Swin Transformers [33, 34], and Earthformer [35] offer potential architectural solutions, but their utility in modeling control-driven, dissipative quantum systems remains largely unexplored. In this work, we propose a physics-informed regional transformer architecture designed to efficiently learn the dynamics of structured open quantum systems under external driving. The architecture is based on regional attention, which exploits translation invariance of the physical laws as an inductive bias to achieve scalable complexity. The architecture encodes local density matrix with build-in Hermiticity, and employs a causal decoder-only structure for autoregressive, physically consistent generation. It further supports conditioning on time-dependent global parameters, allowing it to capture how external control fields drive the system‚Äôs evolution. We evaluate the architecture on (i) a driven dissipative qubit and (ii) a spatially extended EIT quantum memory, benchmarking fidelity, physical constraint preserva- tion, and experimentally relevant observables (readout delay, pulse energy) under both in-distribution and out-of-distribution control parameters, with and without decoher- ence. In both cases, the model achieves high fidelity and robust generalization while 2 providing up to 1485x acceleration over numerical solvers on modern GPUs. Together, these results highlight a general surrogate modeling framework for structured open quantum dynamics, with potential applications in large-scale quantum network sim- ulation, real-time experimental feedback, and scalable quantum information device optimization. 2 Results 2.1 Problem Setup We confine our interest in open quantum systems with a spatial structure(Fig. 1a), for example, a grid or a lattice. Each site may also coupled to a global time depen- dent control field œï(t), and a propagating field œà(ri, t) satisfied by some propagation equation. The system Hamiltonian has the general form ÀÜH = X i X l œµlÀÜœÉi l + œï(t) X i X lm ÀÜœÉi lm + X i X lm œà(ri, t)ÀÜœÉi lm (1) where ÀÜœÉi l = |l‚ü©‚ü®l| and ÀÜœÉi lm = |l‚ü©‚ü®m| at site i. We further confine the system envi- ronment interaction under the Born‚ÄìMarkov approximation. The system evolution is thus governed by the quantum master equation dœÅi dt = ‚àíi[H, œÅi] + X j Œ≥j \u0012 LjœÅiL‚Ä† j ‚àí1 2 n L‚Ä† jLj, œÅio\u0013 . (2) The model is often seen in quantum information applications involving light matter interaction. 2.2 Model Design and Architecture We model the spatiotemporal evolution of structured open quantum systems on a discretized domain. The system is defined on a uniform spacetime grid, where each point (ri,", "response": "Simulating the dynamics of open quantum systems with spatial structure and external control is an important challenge in quantum information science. Classical numerical solvers for such systems require integrating coupled master and field equations, which is computationally demanding for simulation and optimization tasks and often precluding real-time use in network-scale simulations or feedback control. We introduce a regional attention-based neural architecture that learns the spatiotemporal dynamics of structured open quantum systems. The model incorporates translational invariance of physical laws as an inductive bias to achieve scalable complexity, and supports conditioning on time-dependent global control parameters. We demonstrate learning on two representative systems: a driven dissipative single qubit and an electromagnetically induced transparency (EIT) quantum memory. The model achieves high predictive fidelity under both in-distribution and out-of-distribution control protocols, and provides substantial acceleration up to three orders of magnitude over numerical solvers. These results demonstrate that the architecture establishes a general surrogate modeling framework for spatially structured open quantum dynamics, with immediate relevance to large-scale quantum network simulation, quantum repeater and protocol design, real-time experimental optimization, and scalable device modeling across diverse light-matter platforms."}
{"prompt": "Title: Data-driven solar forecasting enables near-optimal economic decisions\n\nIntroduction The global energy system is moving toward carbon neutrality, with solar photovoltaics (PV) emerging as one of the fastest- growing renewable technologies1‚Äì3. China aims to raise its solar generation penetration to more than 40% by 20504,5. Reaching such an ambitious target will depend both on continued capacity expansion, and on the ability to manage the inherent variability of solar resources at the consumer level6‚Äì9. For industrial consumers, solar forecast quality determines their daily battery operation and grid interactions, and thus drives their long-term decisions to invest in PV projects10‚Äì14. Industrial consumers must plan battery operations days in advance to remain profitable under peak‚Äìvalley on-grid electricity price, which requires long-horizon, high-resolution solar forecasts that numerical weather prediction (NWP) often cannot provide15,16. Recent AI-based weather models such as FourCastNet17, GraphCast18, and Pangu-Weather19 now achieve forecast skills arXiv:2509.06925v1 [physics.geo-ph] 8 Sep 2025 Weather Forecast Temporal Interpolation T0 Solar Radiation Diagnostic 27 Channels Solar Radiation Downscaling Low-Resolution Solar Radiation ‚Ä¶‚Ä¶ 27 Channels High-Resolution Solar Radiation IFS/GFS 73 Channels u v ‚Ä¶ latitudes longitudes q T6 T12 T168 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶‚Ä¶ T0 T6 T5 T1 ‚Ä¶ + ‚Ä¶ + ‚Ä¶ + ‚Ä¶ + ‚Ä¶ ‚Ä¶‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ T0 T6 T5 T1 Low-Resolution Solar Radiation Crop T1 +00min +20min +30min +40min +50min + ‚Ä¶ Lat: 47~19.05, Lon: 97~124.95 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ z ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ Solar Plant and Battery Management Demand Solar Forecast Real-world Variables Actor Critic Battery Management Perfect Information RL RDM t ‚Ä¶ u v ‚Ä¶ q ‚Ä¶ z ‚Ä¶ t ‚Ä¶ u v ‚Ä¶ q ‚Ä¶ z ‚Ä¶ t ‚Ä¶ +10min a) SunCastNet on NVIDIA Earth-2 Platform b) Economic Evaluation Framework Figure 1. SunCastNet solar forecasting pipeline coupled with an RL framework for industrial economic evaluation. (a) SunCastNet Framework: The system begins with IFS/GFS (73 atmospheric variables at 0.25¬∞, 6-hour intervals) , processed by a four-stage sequence(a SunCastNet): (i) Weather forecasting with SFNO, which predicts global circulation using 73 input variables to produce 6-hourly fields at 0.25¬∞ resolution; (ii) Temporal interpolation with ModAFNO, which refines the coarse forecasts from 6-hourly to 1-hourly resolution using two consecutive atmospheric states (2 √ó 73 variables) together with 9 auxiliary fields (total 155 input channels ‚Üí73 output channels); (iii) Solar radiation diagnostics with AFNO, which maps 31 key atmospheric fields to 1-hourly surface solar radiation downwards (SSRD); and (iv) downscaling with CorrDiffSolar, which transforms 57-channel inputs into 0.05¬∞, 10-minute SSRD fields calibrated against dense East Asia‚ÄìPacific observations. (b) Economic Evaluation Framework: These high-resolution forecasts are then embedded in RL‚Äìbased battery management models that integrate solar generation, electricity demand, and price signals to derive optimal charging and discharging strategies. By comparing against perfect-information and robust decision making (RDM) baselines, the framework quantifies the impact of forecast skill on operational regret, infrastructure sizing, and long-term investment returns across industrial sectors. The analysis focuses on China, covering the domain 47‚Äì19.05‚ó¶N, 97‚Äì124.95‚ó¶E. comparable to, or even surpassing, NWP at global 0.25¬∞ and hourly scales20‚Äì25. Many researchers are now drawing findings from low-resolution research to develop high-resolution weather forecasting26‚Äì28, and therefore to address a wide range of downstream management issues. The transition from low- to high-resolution weather forecasting faces significant challenges, including exponentially increasing computational demands, lack of high-resolution observations, and the cumulation of errors29,30. Diverse approaches, ranging from task-specific fine-tuning31, diagnostic modules32,33, and multi-source data integration28, are now being explored. Nevertheless, these advances remain insufficient for industrial consumers, who require forecasts with week-ahead horizons and station-level accuracy34‚Äì36. Here, we introduce SunCastNet (developed on the NVIDIA Earth-2 Platform), a sequential framework that translates recent advances in AI weather forecast into high-resolution (0.05¬∞, 10-min) long-horizon (7-day) solar forecasts and downstream decision support (Fig. 1a). The forecasting component is organized into four successive stages reflecting atmospheric processes37,38: (i) a Spherical Fourier Neural Operator (SFNO) that models global circulation at 0.25¬∞ every 6 hours39, (ii) a Modulated Adaptive Fourier Neural Operator (ModAFNO) that interpolates coarse six-hourly states to hourly variability40, (iii) an AFNO-based diagnostic32 that maps key atmospheric fields directly to surface solar radiation downwards (SSRD), which is essential to photovoltaic power output41 at 0.25¬∞ every hour, and (iv) a CorrDiffSolar module that downscales SSRD", "response": "Solar energy adoption is critical to achieving net-zero emissions. However, it remains difficult for many industrial and commercial actors to decide on whether they should adopt distributed solar-battery systems, which is largely due to the unavailability of fast, low-cost, and high-resolution irradiance forecasts. Here, we present SunCastNet, a lightweight data-driven forecasting system that provides 0.05$^\\circ$, 10-minute resolution predictions of surface solar radiation downwards (SSRD) up to 7 days ahead. SunCastNet, coupled with reinforcement learning (RL) for battery scheduling, reduces operational regret by 76--93\\% compared to robust decision making (RDM). In 25-year investment backtests, it enables up to five of ten high-emitting industrial sectors per region to cross the commercial viability threshold of 12\\% Internal Rate of Return (IRR). These results show that high-resolution, long-horizon solar forecasts can directly translate into measurable economic gains, supporting near-optimal energy operations and accelerating renewable deployment."}
{"prompt": "Title: Dato: A Task-Based Programming Model for Dataflow Accelerators\n\nIntroduction Large language models (LLMs) have driven computational demand to unprecedented levels, yet their performance is increasingly constrained by memory [6, 25, 40]. While peak FLOPs and specialized tensor engines scale rapidly, the ef- fective throughput for LLM training and inference is often bounded by how quickly the tensors can be moved, staged, and transformed. Off-chip DRAM bandwidth improves only marginally and the energy cost per bit access far exceeds that of a FLOP, a limitation widely known as the memory wall [17]. ‚àóEqual Contribution. ‚Ä†Visiting student from Shanghai Jiao Tong University. ‚Ä°Work done at Cornell University. As a result, many kernels fail to reach their arithmetic lim- its, stalling on data movement, layout transformations, and synchronization. One promising architectural response is to minimize off- chip traffic by exploiting on-chip dataflow: streaming data directly between hardware modules so that intermediate results avoid detours through DRAMs. Dataflow accelera- tors such as Google TPU [28], AWS Trainium [16], and AMD Ryzen AI NPU [42] embody this philosophy by coupling com- pute units with scratchpads and lightweight interconnects to sustain pipelines. Even general-purpose GPUs have em- braced similar principles: recent NVIDIA Hopper [36] and Blackwell [38] GPUs introduce dedicated tensor memory accelerators (TMA) to stream tensors asynchronously, over- lapping data transfers with computation. By making data movement a first-class concern through channels and FIFOs, these designs can transform memory-bound workloads into high-throughput pipelines that keep compute units saturated while greatly reducing off-chip traffic. Despite these advances, current programming models struggle to fully unlock the potential of dataflow acceler- ators, which remain trapped in the longstanding trade-off between performance and productivity. Challenge 1: Low-level communication control im- proves performance but hinders productivity. Low-level programming interfaces offer fine-grained hardware control but impose significant burdens on developers. For exam- ple, IRON [23], a frontend for programming AMD NPUs, exposes all FIFO and DMA details to the user‚Äîsimilar in abstraction level to CUDA for GPUs. Even a simple matrix multiplication kernel can require hundreds of lines of code, as IRON essentially serves as a Python wrapper over the MLIR-AIE dialect [53]. Recent schedule languages such as Exo [26, 27] and Allo [7] aim to decouple hardware cus- tomization from algorithm specification, enabling expressive transformations. However, in the dataflow setting, these ap- proaches often exacerbate the problem. For instance, Allo provides a .relay() primitive to model communication, but developers still write array-based code and must manually translate it into FIFOs. This creates a semantic mismatch between array-oriented frontends and the inherently stream- oriented hardware backends. Complex producer-consumer 1 arXiv:2509.06794v1 [cs.PL] 8 Sep 2025 Table 1. Comparison of existing accelerator programming frameworks. Framework Target Input Language Explicit Communication Explicit Sharding Automatic Optimization Type Checking on Dataflow CUTLASS [35] GPU CUDA ‚úì ‚úó ‚úó ‚úó Exo [26] CPU/Xcel Python ‚úó ‚úó ‚úó ‚úó Taskflow [22] CPU/GPU C++ ‚úì ‚úó ‚úì ‚úó Triton [48] GPU Python ‚úó ‚úó ‚úì ‚úó Allo [7] FPGA Python/MLIR ‚úì ‚úó ‚úó ‚úó HIDA [59] FPGA C++ ‚úó ‚úó ‚úì ‚úó TAPA [20] FPGA C++ ‚úì ‚úó ‚úì ‚úó ARIES [62] AIE/NPU Python ‚úó ‚úó ‚úì ‚úó IRON [23] NPU Python/MLIR ‚úì ‚úó ‚úó ‚úó Dato (Ours) FPGA/NPU Python ‚úì ‚úì ‚úì ‚úì topologies become verbose and brittle to express, while com- pilers must infer dataflow intent from array operations‚Äî adding complexity, risking correctness, and missing opti- mization opportunities. In effect, compilers are required to reconstruct the dataflow a programmer intended, rather than enabling programmers to specify it directly. Challenge 2: Tile-based languages hide communi- cation, limiting optimization. Tile-based programming models such as Triton [48] excel at expressing per-tile com- putation with implicit caching but offer little explicit control over inter-tile or inter-kernel communication. Efforts like ARIES [62] adapt Triton-like syntax for dataflow accelerators, yet still enforce fixed communication patterns, restricting de- velopers from fine-grained control. This limitation becomes especially acute in multi-kernel designs, where users have no choice to manage how data flows between kernels. As a re- sult, intermediate results are forced back to off-chip memory before being read again, missing the opportunity to exploit on-chip streaming and resulting in performance degradation. We argue that data communication must be a first- class abstraction in programming models for dataflow ac- celerators. To this end, we propose Dato1, a task-based pro- gramming model that elevates both data communication and sharding to first-class types. In Dato, programmers de- fine tasks connected by stream types and pass in sharded data with lay", "response": "Recent deep learning workloads increasingly push computational demand beyond what current memory systems can sustain, with many kernels stalling on data movement rather than computation. While modern dataflow accelerators incorporate on-chip streaming to mitigate off-chip bandwidth limitations, existing programming models struggle to harness these capabilities effectively. Low-level interfaces provide fine-grained control but impose significant development overhead, whereas high-level tile-based languages abstract away communication details, restricting optimization and forcing compilers to reconstruct the intended dataflow. We present Dato, a Python-embedded, task-based programming model for dataflow accelerators that elevates data communication and sharding to first-class type constructs. Developers write programs as a graph of tasks connected via explicit stream types, with sharded inputs specified using layout types. These tasks are first mapped virtually onto the accelerator's spatial fabric, and the compiler then generates a physical mapping that respects hardware constraints. Experimental results on both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves high performance while significantly reducing the burden of writing optimized code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and delivers a 2.81x speedup on attention kernels compared to a state-of-the-art commercial framework. On the FPGA, Dato surpasses leading frameworks in performance when generating custom systolic arrays, achieving 98% of the theoretical peak performance."}
{"prompt": "Title: UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction\n\n1. Introduction The CheckThat! Lab aims to build computational infrastructure to support human fact-checkers, consisting of a pipeline for verifying the truthfulness of social media text. We participate in Task 2 (English) [1], which addresses a specific challenge in this pipeline: Task Statement Given the text of a social media post, extract the main claim in succinct and concise language suitable for a human fact checker to verify. We anticipated a number of unique challenges in this task and designed our methods to study them empirically. Many of our concerns align with Thorne et al. [2]. Multiple equally relevant claims. Often, multiple extracted claims are topically relevant. However, the task requires focusing on a single fact for manual verification. Analyzing the training and validation data, we found that many gold claims include multiple facts and could be rewritten more succinctly. For example: \"Joe Biden lives in a large estate bought on a senator‚Äôs salary.\" Missing multi-modal content. We found that many gold claims refer to information not available to participants, such as photos or videos included with the original post but removed during pre-processing. While we accept that claims cannot be based on unseen data, we instruct our models to make educated guesses about the topic. Russia vs Ukraine war https://www.huobi.com/en-us/topic/double-invite/register/? invite_code=ije73223&name=BlackWidow&avatar=6&inviter_id=11343840 BREAKING NEWS LIVE Latest stars: Martin Johnson -200 Latest Supporter: Emeka Efobi CLEF 2025 Working Notes, 9 ‚Äì 12 September 2025, Madrid, Spain $ Joe.Wilder@unh.edu (J. Wilder); Nikhil.Kadapala@unh.edu (N. Kadapala); Yanjie.Xu@unh.edu (B. Xu); Mohammed.Alsaadi@unh.edu (M. Alsaadi); Aiden.Parsons@unh.edu (A. Parsons); Mitchell.Rogers@unh.edu (M. Rogers); Palash.Agrawal@unh.edu (P. Agrawal); Adam.Hassick@unh.edu (A. Hassick); dietz@cs.unh.edu (L. Dietz) ¬© 2025 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Faithful extractions. We are especially concerned about extracted claims where LLM hallucinations introduce content not present in the original post. An example of a hallucinated and overly verbose claim follows: Social Media Post: The salary of a U.S. Senator is $174,000 per year. This is Joe Biden‚Äôs house. . . seems legit :) The salary of a U.S. Senator is $174,000 per year. This is Joe Biden‚Äôs house. . . seems legit :) The salary of a U.S. Senator is $174,000 per year. This is Joe Biden‚Äôs house. . . seems legit :) Hallucinated Extracted Claim Joe Biden‚Äôs house, purchased for an amount significantly exceeding the cumulative value of his annual U.S. Senator‚Äôs salary of $174,000, raises questions about potential additional, undisclosed sources of income that may have contributed to the down payment, mortgage payments, property taxes, insurance premiums, and ongoing maintenance costs associated with the property. Our emphasis is on exploring the design space across different LLMs and methods‚Äîfine-tuning and few-shot prompting‚Äîin search of the best trade-off between optimizing the METEOR score and producing claims that are genuinely useful for human fact-checkers. We conduct a broad exploration of methods, prompts, and LLMs, casting a wide net. Our approaches fall into three overarching categories: 1. Fine-tuning approaches, 2. Prompting approaches, and 3. \"Frustratingly easy\" baselines.1 We describe all explored approaches and submit those performing best on the validation set in terms of METEOR. We only use resources provided by the Task 2 organizers and publicly available large language models from Hugging Face and the Together.AI API service. 2. Approaches: Fine-Tuning and Prompting In this section, we describe methods relying on fine-tuning across LLMs of different parameter scales. Our key takeaway: Flan-T5 Large [3] offered the best compromise between raw capability and practical fine-tuning feasibility under our hardware and time constraints. 2.1. Finetuned Flan-T5 Large This approach fine-tuned the Flan-T5 Large [3] model on the CLEF 2025 Task 2 training dataset to align its outputs more closely with the gold-standard claims. Fine-tuning was performed using the huggingface transformers library without advanced techniques such as LoRA or PEFT. Due to resource limitations, billion-parameter models were out of scope. We opted for Flan-T5 Large (783M parameters), which can run locally and is more manageable to train due to its smaller size. A straightforward task-specific prompt was prepended to training examples: 1We did not use baselines provided by the organizer. Please read the following social media post and extract the claim made within it. Normalize the claim by rephrasing it in a clear and concise manner. Post: $text Extracted Claim: The training ran for 10 epochs on an NVIDIA 4060 GPU, taking nearly four days to complete. This approach‚Äôs strength lies in its ability to internalize", "response": "We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower."}
{"prompt": "Title: Imitative Membership Inference Attack\n\nIntroduction Over the past decade, machine learning (ML) has seen re- markable advances, with models such as neural networks increasingly being trained on sensitive datasets. This trend has raised growing concerns about the privacy risks associated with these models. Membership inference attacks (MIAs) [48] have been proposed to measure the degree to which a model leaks information by determining whether some instances were part of its training set. Closely related to Differential Privacy (DP) [13,32], MIAs have become a widely adopted technique for empirical auditing of various trustworthy risks in ML models [39,42,51,56,57], and serve as key components for more sophisticated attacks [4,5,52]. MIAs in the black-box setting [3,50,58,59] typically ex- ploit the model‚Äôs behavioral discrepancy between its training instances (i.e., members) and non-training instances (i.e., non- members) for inference. A widely-used strategy to explore the discrepancy is shadow training [48], which involves training multiple shadow models on datasets drawn from the same distribution as the target model‚Äôs training set. For each query instance, the membership scores generated by the shadow 1Our code is available at https://github.com/zealscott/IMIA. models trained without the instance (shadow out models) represent the out distribution for the instance. In some set- tings, one also has shadow models that are trained with the instance as a member, and can use the scores generated by these shadow in models to compute an in distribution. Given a target model, one tries to tell which distribution the score from the target model fits better. MIAs [3, 9, 19, 54, 60] utilizing shadow training have shown strong attack performance. A critical limitation of existing shadow-based MIAs lies in their substantial computational overhead. Training each shadow model incurs non-trivial overhead, and state-of-the-art attacks like LiRA [3] and PMIA [9] require training hundreds of shadow models to estimate the likelihood ratio for infer- ence. This requirement imposes a substantial computational burden, which reduces the feasibility of using these state-of- the-art MIAs for practical privacy auditing [60]. In addition, it also impedes research reproducibility within the privacy community (see Section 2 for further discussion). We observe that this inefficiency stems from the target- agnostic design of shadow training. That is, the current shadow training process fails to take advantage of know- ing the target model under attack. As a result, the shadow models learn only the general patterns of members and non- members, rather than the specific behavioral discrepancies of the target model. As shown in the top row of Figure 1, this target-agnostic approach causes shadow models to exhibit high predictive variance across instances with varying levels of attack vulnerability. Consequently, existing MIAs need to train a multitude of shadow models to capture this variabil- ity, resulting in both substantial computational overhead and suboptimal performance. To address this challenge, we introduce Imitative Member- ship Inference Attack (IMIA), a novel approach that improves both attack efficiency and effectiveness. At the core of IMIA is imitative training, a new shadow training technique that trains target-informed imitative models to mimic the target model‚Äôs behavior. Specifically, we first train a set of imita- tive out models by applying weighted logits (log of output confidence) matching to the target model‚Äôs outputs, captur- 1 arXiv:2509.06796v1 [cs.CR] 8 Sep 2025 27 0 27 2 18 33 Frequency Easy - Member 27 0 27 2 18 33 Easy - Nonmember 27 0 27 2 18 33 Medium - Member 27 0 27 2 18 33 Medium - Nonmember 27 0 27 2 18 33 Hard - Member 27 0 27 2 18 33 Hard - Nonmember 27 0 27 2 18 33 Frequency Easy - Member 27 0 27 2 18 33 Easy - Nonmember 27 0 27 2 18 33 Medium - Member 27 0 27 2 18 33 Medium - Nonmember 27 0 27 2 18 33 Hard - Member 27 0 27 2 18 33 Hard - Nonmember Shadow Models Imitative Models in models out models target score in models out models target score Figure 1: Distributions of membership scores (i.e., scaled confidence scores, defined in Section 3.1) for six CIFAR-100 instances with varying attack difficulty (easy, medium, hard). A larger overlap between in (trained with the instance) and out (trained without the instance) score distributions indicates greater difficulty in determining membership. The dashed vertical line represents each instance‚Äôs score on the target model. Top row: target-agnostic shadow models show high predictive variance (with long tails and wide distributions) for both members and non-members, resulting in significant overlap that hampers reliable inference, especially for hard-to-attack instances. Bottom row: target-informed imitative models exhibit more stable and well-separated distributions, enabling effective inference across all levels of difficulty. More examples are in Figure 7. Best viewed in color. ing the b", "response": "A Membership Inference Attack (MIA) assesses how much a target machine learning model reveals about its training data by determining whether specific query instances were part of the training set. State-of-the-art MIAs rely on training hundreds of shadow models that are independent of the target model, leading to significant computational overhead. In this paper, we introduce Imitative Membership Inference Attack (IMIA), which employs a novel imitative training technique to strategically construct a small number of target-informed imitative models that closely replicate the target model's behavior for inference. Extensive experimental results demonstrate that IMIA substantially outperforms existing MIAs in various attack settings while only requiring less than 5% of the computational cost of state-of-the-art approaches."}
{"prompt": "Title: Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos\n\n1. INTRODUCTION Sound Event Localization and Detection [1] is a combined task that integrates sound event detection (SED) [2] and sound source localization (SSL) [3]. The goal is to identify active sound events from predefined target classes, track their temporal activity, and estimate their spatial positions. SELD systems are crucial for a wide range of real-world applications, including human-robot interaction [4], security monitoring, and immersive media production [5]. Since 2019, SELD has been a dedicated task in the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge. Over successive editions, the task has evolved to include increasingly complex scenarios, such as moving sound sources [6], ignoring external interfering sounds [7], incorporating visual input to enable multimodal SELD in 360‚ó¶videos [8], and estimating source distance [9]. For the 2025 edition, the challenge has shifted from the traditional 4-channel first-order ambisonics (FOA) and microphone array (MIC) formats to stereo SELD using conventional frontal video content, i.e., leveraging only left and right audio channels and perspective video. This format is better aligned with the requirements of conventional ‚àóThis research was funded by EPSRC-BBC Prosperity Partnership ‚ÄòAI4ME: Future personalised object-based media experiences delivered at scale anywhere‚Äô (EP/V038087/1). For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising. Data supporting this study is available from https: //zenodo.org/records/15559774 media content. In stereo SELD the DOA prediction is limited to the azimuth angles in the range [-90‚ó¶, 90‚ó¶], as elevation is ill-defined when using just two horizontally arranged channels, and distinguishing between front and back sources is inherently ambiguous. Source distance estimation, introduced in the previous edition, remains part of the task, which is therefore often referred to as 3D SELD. Additionally, in the audio-visual track, a new subtask has been introduced: predicting whether sound sources are onscreen or offscreen. SELD is a non-trivial task, as it requires reasoning across spatial, temporal, and semantic dimensions: spatial information gives cues for direction and distance estimation; temporal information marks movements and onsets/offsets of sound source activity; semantic information identifies objects, their relations and likely behaviors. Recent advances in large language models (LLMs) [10], vision- language models (VLMs) [11], and audio-language models (ALMs) [12] demonstrate that language is a powerful lens for enabling semantic understanding and complex reasoning in relation to media content. Building on this, we posit that leveraging language-aligned models can enhance a SELD model‚Äôs semantic reasoning and hence indirectly benefit spatial and temporal reasoning. Traditionally, spatial localization relies on multichannel audio, allowing models to infer source positions through inter-channel time and level differences. This dependence limits the use of large-scale pre-training datasets. While synthetic audio [13] and visual [14] data can partially address this limitation, effective language alignment typically demands extensive pre-training on large-scale, heterogeneous data. Therefore, we extend our SELD architecture by integrating two existing language-aligned models: CLAP [15] for the audio modality and OWL-ViT [16] for the visual modality. Specifically, we extract audio and visual embeddings using their respective pretrained encoders and combine them with SELD embeddings obtained from a CNN-Conformer backbone, a widely adopted architecture in SELD research [17]‚Äì[19]. We argue that the embeddings from CLAP and OWL-ViT are semantically rich and provide complementary information to the task. To fuse these embeddings, we employ an adapted version of the Conformer architecture [20], which we refer to as the Cross-Modal Conformer (CMC). This module is used to integrate intra-modal embeddings from different sources (e.g., the SELD audio encoder and CLAP) and inter-modal embeddings (i.e., the combined audio representation and the visual embedding from OWL-ViT). This architecture is outlined in Sec. 2.1, and an ablation study to demonstrate the individual contributions of the language-aligned models and benchmark against the DCASE Task 3 baseline systems is presented in Sec. 3. To tackle the distance estimation subtask, we partnered common SELD input features with short-term power of the autocorrelation stpACC features [21] in Sec. 2.2. Furthermore, we synthesized and carefully curated large audio and audio-visual datasets used to pre-train our models, described in Sec. 2.3. Final refinements includes a visual arXiv:2509.06598v1 [eess.AS] 8 Sep 2025 Detection and Classification of Acoustic Scenes and Events 2025 30‚Äì31 October 2025, Barcelona, Spain OWL-ViT CLAP Audio Encoder SELD Encoder CNN", "response": "In this study, we address the multimodal task of stereo sound event localization and detection with source distance estimation (3D SELD) in regular video content. 3D SELD is a complex task that combines temporal event classification with spatial localization, requiring reasoning across spatial, temporal, and semantic dimensions. The last is arguably the most challenging to model. Traditional SELD approaches typically rely on multichannel input, limiting their capacity to benefit from large-scale pre-training due to data constraints. To overcome this, we enhance a standard SELD architecture with semantic information by integrating pre-trained, contrastive language-aligned models: CLAP for audio and OWL-ViT for visual inputs. These embeddings are incorporated into a modified Conformer module tailored for multimodal fusion, which we refer to as the Cross-Modal Conformer. We perform an ablation study on the development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the individual contributions of the language-aligned models and benchmark against the DCASE Task 3 baseline systems. Additionally, we detail the curation process of large synthetic audio and audio-visual datasets used for model pre-training. These datasets were further expanded through left-right channel swapping augmentation. Our approach, combining extensive pre-training, model ensembling, and visual post-processing, achieved second rank in the DCASE 2025 Challenge Task 3 (Track B), underscoring the effectiveness of our method. Future work will explore the modality-specific contributions and architectural refinements."}
{"prompt": "Title: Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization\n\nI. INTRODUCTION Large Vision-Language Models (LVLMs) have emerged as a powerful class of multimodal systems capable of under- standing and generating information across visual and textual domains [1], [2]. These models underpin a wide range of applications, from image captioning and visual question an- swering to multimodal dialogue and autonomous agents [3]. Their ability to jointly reason over images and text allows LVLMs to perform complex perception and language tasks that were previously challenging for unimodal models. As a result, they are increasingly being adopted in real-world sce- narios, including education, healthcare, and human-computer interaction, where nuanced understanding of both visual and linguistic context is essential. Unlike their LLM counterparts, where the model processes a text prompt and generates a text response, LVLMs take multimodal inputs, typically combining an image with a text prompt, and produce text-based outputs. This enables LVLMs to reason over both visual and linguistic information, allowing for more context-aware and perceptually grounded responses. Transitioning from LLMs to LVLMs introduces significant complexity due to the inherently multimodal nature of the inputs. While LLMs operate solely on textual data, i.e., taking in a text prompt and producing a corresponding textual output, LVLMs must process and integrate information from multiple modalities, most commonly images paired with text. This added dimension requires the model not only to understand language but also to interpret and align it with visual content, creating a more complex input space and a need for deeper multimodal reasoning. One of the core challenges in this transition lies in enabling the model to effectively fuse and reason over visual and linguistic signals. LVLMs must develop a coherent understand- ing of how visual elements relate to textual descriptions and instructions, which can vary significantly in specificity and abstraction. Achieving this requires architectural adaptations, larger and more diverse training datasets, and often more sophisticated alignment techniques to ensure the outputs are both accurate and contextually appropriate. This makes the de- velopment of LVLMs more resource-intensive and technically demanding compared to their text-only LLM counterparts. While pre-training LVLMs on large-scale datasets provides a strong foundation, further training (post-training) is impor- tant to refine and align their behavior. Instruction learning and preference learning are complementary approaches used to fine-tune LVLMs, each serving a distinct purpose. Instruction learning focuses on teaching models to follow explicit natural language commands by training on instruction-response pairs, i.e., supervised fine-tuning. In contrast, preference learning refines model behavior based on comparative judgments, such as rankings or pairwise preferences. While instruction learning helps the model understand and execute tasks, preference learning aligns its outputs with human preferences and values, especially in cases where multiple valid responses exist. This paper focuses on preference learning approaches for fine-tuning and aligning LVLMs. As for LLMs, Deep Re- inforcement Learning (DRL) has gained traction as a fine- tuning technique that allows LVLMs to learn from feedback arXiv:2509.06759v1 [cs.LG] 8 Sep 2025 Sources of Rewards for DRL By Rule-based Functions By an Existing LVLM(s), i.e., AI Feedback By Training a Reward Model from Human Annotations Sources of Preference Data for DPO By Human Rankings By Machine Rankings Fine-Tuning LVLMs Fig. 1. The structure of this survey paper where we categorize preference learning methods for fine-tuning large vision-language models (LVLMs) into Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO). signals beyond traditional supervised losses. By optimizing policies with respect to reward functions (whether handcrafted, learned, or derived from human preferences), DRL enables more adaptive, goal-oriented, and interpretable fine-tuning of LVLMs. Direct Preference Optimization (DPO) offers an alternative to DRL for fine-tuning LVLMs using preference data [4]. Rather than relying on reward functions or policy rollouts, DPO frames preference learning as a classification problem, i.e., directly optimizing the policy to prefer re- sponses aligned with human judgments. This approach simpli- fies training by avoiding the instability and complexity often associated with DRL algorithms. Moreover, DPO maintains strong performance while offering improved efficiency and more predictable optimization behavior, making it an attractive choice for aligning models with human intent. This survey explores the intersection of DRL and DPO with vision-language modeling, highlighting key methods, chal- lenges, and recent advances in this rapidly evolving research area. Fig. 1 provides a graphical overview of these approaches and serves as th", "response": "Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs."}
{"prompt": "Title: Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks\n\nIntroduction Let us imagine the world through the eyes of a large language model (LLM). The model perceives nothing but text: sequences of tokens generated by an underlying data-generating process [1‚Äì3]. Each author‚Äìor more generally, each effective stylistic source of text in its training data‚Äìcan be interpreted as an external agent generating a sequence of tokens, following a unique autoregressive distribution [4‚Äì6]. The LLM itself, parameterized by PŒ∏, seeks to approximate these distributions. Viewed through an agentic lens, its observation space consists of past tokens (or equivalently, latent projections into a state representation), while its action space is the vocabulary itself. Consequently, the LLM, as an agent, acts by selecting the next token in the sequence, thereby defining a probability distribution over outcomes that coincides with its action distribution. Now, consider a collection of neural networks, or neural agents, each selecting its next token according to its learned parameters. Their weights may differ due to initialization, data order, or architecture design. Aggregating such agents requires a common representation of their outputs. Internal activations are difficult to share or align across models, even within the same architecture, whereas the final logits always reside in a shared space: log-probabilities of the vocabulary. Modern neural networks inherently operate in this logit space, producing scores at the output layer that are transformed into probabilities via softmax. It is therefore natural to perform additive aggregation in log-probability space when combining the actions of multiple neural agents. Recent work in interpretability has pursued the converse perspective. Rather than aggregating multiple networks, one can decompose a single network into an ensemble of subcomponents. Attribution-based 1University of Chicago, Department of Statistics; Correspondence to: sulee@uchicago.edu 2University of Chicago, Department of Statistics and Computer Science 3Independent 1 arXiv:2509.06701v1 [cs.LG] 8 Sep 2025 parameter decomposition methods formalize this intuition by expressing the parameters of a neural network as a sum of simpler components, each contributing additively to the final logits [7, 8]. More precisely, if P1, . . . , Pn denote the distributions induced by these neural subcomponent models, and Œ≤i = 1/n are uniform weights, then the aggregate (original) neural model satisfies log PŒ∏(o) = Pn i=1 Œ≤i log Pi(o) for each shared outcome o ‚ààO. Recall that in logit space, probabilities are recovered through the softmax function. Exponentiating and renormalizing the above for softmax yields PŒ∏(o) ‚àùexp \u0010 X i Œ≤i log Pi(o) \u0011 ‚àù Y i Pi(o)Œ≤i, (1) which is precisely the logarithmic pool of the constituent distributions. Interpreted this way, the trained neural agent does not simply mimic a single data source; rather, it represents the entire collection of subagents by aggregating their behaviors through a geometric mean in probability space. In effect, the model embodies the voice of many agents simultaneously, balancing their relative influence according to Œ≤. This perspective reveals a structural form inherently log-pooling the hypothesized internal subagent components of a neural network. Each observed agent distribution‚Äôs influence is encoded multiplicatively, and the resulting learned policy must cohere into an aggregate distribution. The agent acts in accordance with its logits: higher logits imply higher probability of selecting an outcome, which we interpret as having greater utility for the agent. Therefore, for any agent P, we formally define the epistemic utility of outcome o ‚ààO as U(o) = log P(o). From this viewpoint, the logits correspond directly to utilities, and the aggregation rule (1) implies that the neural agent can be understood as a composition of multiple interacting subagents. Insights from game theory then suggest that such composite agents typically possess a stability criterion to ensure internally consistent or coherent behavior [9‚Äì11]. However, to our knowledge, these stability notions have not been formalized for neural agents. Thus in this paper, we introduce a theoretical framework for analyzing the internal cohesiveness and stability of neural agents. Our approach develops formal definitions grounded in epistemic utility, while drawing conceptual support from active inference [12‚Äì14], mathematical economics [11, 15, 16], and probabilistic modeling [3, 17, 18]. However, beyond the foundational definitions provided in Section 2, our contributions are, to our knowledge, entirely novel and unexplored. A detailed literature review and further intuitive motivations are given in Appendices A and B. Contributions. In light of this context, we summarize our contributions as follows: ‚Ä¢ Formalization of compositional agency: We propose a novel framework for analyzing stability and internal coherence in neural agents. ‚Ä¢ Sharp possibility frontier: We prove str", "response": "We develop a theory of intelligent agency grounded in probabilistic modeling for neural models. Agents are represented as outcome distributions with epistemic utility given by log score, and compositions are defined through weighted logarithmic pooling that strictly improves every member's welfare. We prove that strict unanimity is impossible under linear pooling or in binary outcome spaces, but possible with three or more outcomes. Our framework admits recursive structure via cloning invariance, continuity, and openness, while tilt-based analysis rules out trivial duplication. Finally, we formalize an agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent persona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a manifest-then-suppress Waluigi strategy yields strictly larger first-order misalignment reduction than pure Luigi reinforcement alone. These results clarify how developing a principled mathematical framework for how subagents can coalesce into coherent higher-level entities provides novel implications for alignment in agentic AI systems."}
{"prompt": "Title: Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing\n\nIntroduction There has been considerable interest in machine learning to mimic the hu- man ability of learning new concepts from just a few instances. While formal frameworks for machine learning, such as the language-identification-in-the-limit framework [7] and probably approximately correct (PAC) learning [27], have yet to match the high data efficiency of human learning, few-shot (or one-shot) learning methods [18, 19, 29] are attempting to bridge the gap by using prior knowledge to rapidly generalize to new instances given training on only a small number of samples with supervised information. In this paper, we explore the learnability of policies that generalize in the domain of graph routing, where a one-size-fits-all solution based on local or arXiv:2509.06640v1 [cs.LG] 8 Sep 2025 ii Y. Chen et al. global network states is challenging. Routing using global network states tends to be inefficient in time and communication when the class of graphs has strict scalability limits for network capacity [32] or has significant graph dynamics [8, 10]. Manually designed algorithms and heuristics using local or regional network states often cater to particular network conditions and come with tradeoffs of complexity, scalability, and generalizability across diverse graphs. Many machine learned algorithms in this space incur relatively high computational complexity during training [23], have high overhead at run-time that limits their use in graphs with high dynamics, or are applicable only for small-scale graphs or graphs of limited types (e.g., high-density graphs). Our work focuses attention on answering the following question: Can we design a sample-efficient machine learning algorithm for graph routing based on local information that addresses scalability, generaliz- ability, and complexity issues all at once? We answer this question in the affirmative for the all-pairs near-shortest path (APNSP) problem over the class of uniform random graphs in Euclidean metric spaces. It is well known that uniform random graphs in Euclidean metric spaces can represent the topologies inherent in wireless networks; the policies we learn are thus applicable to real-world wireless networks. Our key insight is that ‚Äîin contrast to pure black-box approaches‚Äî exploiting domain knowledge in various ways ‚Äî input feature selection, policy design, sample selection ‚Äî enables learning of near-optimal, low-complexity routing from only a few samples that generalizes to (almost) all graphs in this class and adapts quickly to network dynamics. Near-optimal, low-complexity routing. To motivate our focus on routing using only local information, we recall that approaches to solving the APNSP problem fall into two categories: global information and local information. Poli- cies using global information encode the entire network state into graph em- beddings [20] and find optimal paths, whereas policies using local information need only node embeddings [9] or the coordinates of the neighbors and the des- tination to predict the next forwarder on the optimal path. The complexity (in time and space) resulting from the latter is inherently better than that of the former. However, typically, the latter comes with the penalty of sub-optimality of the chosen path relative to the optimal (i.e., shortest) path. While this holds for both manual designs and machine learned algorithms, we show that by ex- ploiting domain knowledge with respect to bounded forwarding, it is possible to learn a routing policy that achieves near-optimality in (almost) all graphs in the chosen class. Generalizability. We also develop a domain theory for achieving efficient learn- ing that generalizes over the class of graphs. The theory is based on a condition under which the ranking of neighboring forwarders based on local information matches that based on globally optimal information. We empirically validate the condition by demonstrating a strong correlation between local ranking and global ranking metrics for the class of graphs. The theory then implies the AP- Knowledge-Guided Machine Learning for APNSP iii NSP objective can be realized with high probability by training a deep neural network (DNN) that characterizes the local ranking metric of each neighbor as a potential forwarder. Moreover, it implies the DNN policy can generalize even if it is trained from only a few data samples chosen from a single ‚Äúseed‚Äù graph. The theory also guides our selection of seed graphs as well as corresponding training data and is corroborated by empirical validation of our learned routing solutions. Quick adaptation to network dynamics. By virtue of its generalizability to all graphs in the class, the same DNN can be used without any retraining when the network changes. Moreover, since the DNN uses only local information, route adaptation to dynamics is quick. In fact, the policy is inherently self-stabilizing. In this paper, we develop knowledge-guided learning as follows: We model t", "response": "We propose a simple algorithm that needs only a few data samples from a single graph for learning local routing policies that generalize across a rich class of geometric random graphs in Euclidean metric spaces. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that let each graph node efficiently and scalably route (i.e., forward) packets by considering only the node's state and the state of the neighboring nodes. Our algorithm design exploits network domain knowledge in the selection of input features and design of the policy function for learning an approximately optimal policy. Domain knowledge also provides theoretical assurance that the choice of a ``seed graph'' and its node data sampling suffices for generalizable learning. Remarkably, one of these DNNs we train -- using distance-to-destination as the only input feature -- learns a policy that exactly matches the well-known Greedy Forwarding policy, which forwards packets to the neighbor with the shortest distance to the destination. We also learn a new policy, which we call GreedyTensile routing -- using both distance-to-destination and node stretch as the input features -- that almost always outperforms greedy forwarding. We demonstrate the explainability and ultra-low latency run-time operation of Greedy Tensile routing by symbolically interpreting its DNN in low-complexity terms of two linear actions."}
{"prompt": "Title: On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data\n\nI. INTRODUCTION Generative adversarial networks (GANs) are machine learn- ing frameworks put forth by Goodfellow et al. [1]. A GAN aims to learn an unknown distribution from training data via two competing components, namely the generator and the discriminator. The former tries to mimic the distribu- tion of training data while the latter discriminates between true data and generated data. This framework formulates a minimax problem, where the discriminator aims to maximize the probability of differentiating true data from generated ones, while the generator seeks to minimize this probability [1]. Besides computer vision tasks, applications of GANs to communication systems have also received a lot of attentions. For example, GANs have been applied to autonomous wire- less channel modeling [2] [3] and mmWave MIMO channel estimation [4]. Traditionally, both the generator and discriminator in GAN are approximated by neural networks (NNs) [1], [5]. With- This work was supported in part by NSF under Grant DMS-2109002; in part by the National Science and Technology Council, Taiwan, under Grant 111- 2221-E-002-099-MY2, 111-3114-E-002-001 and MOST 111-2221-E-A49 - 069 -MY3. out the NN restrictions and under an optimal unconstrained discriminator, the minimax problem associated with a GAN becomes the minimization of the Jensen-Shannon divergence (JSD) between the distributions of true and generated data. However, due to the nature of this minimax game, the GAN suffers from several problems including vanishing gradient and mode collapse. Many variants of GAN have been proposed to solve these problems, and one of the most promising variants is the Wasserstein GAN (WGAN) [6] , which replaces the JSD by the Wasserstein distance from the optimal transport theory [7]. The WGAN is differentiable with respect to the generator parameters almost everywhere, which benefits the convergence of stochastic gradient descent (SGD) widely adopted for training NNs [6]. Despite the substantial success of applying WGAN to learn distributions in real applications, there are only a few GAN parameter selection algorithms proved to be theoreti- cally optimal [8], [9], which limits the development of GAN beyond heuristic methods in [1], [5]. This lack of rigorous analysis also restricts the evaluation of GANs‚Äô performance to subjective terms. One exception is [8] where Feizi et al. at- tempted to theoretically understand WGANs on a simple linear quadratic Gaussian (LQG) setting. In this benchmark setting, the synthetic data is generated by a Gaussian distribution, the generator NN is restricted to be linear, and the loss function is quadratic. It is shown in [8, Theorem 1] that for this simple setting, the optimal GAN solution happens to be the principal component analysis (PCA) solution. Regularized versions of GANs are also considered and analyzed [5]. Optimal WGAN solutions under LQG settings, with additional entropic and Sinkhorn regularizers, are also studied [9]. In this paper, we aim to analytically solve WGANs beyond the LQG setting. As described in Sec. II, our setting allows non-Gaussian data distribution and non-linear generators con- structed from sigmoid and ReLU functions, and is therefore more general than [8]‚Äì[10]. Also, we attempt to exactly solve WGANs rather than their regularized versions as in [9]. All of these make our problem much more challenging‚Äîafter all, even for the inner discriminator problem, which is an optimal transport problem, the solution in most cases is numerically approximated but not analytically characterized [7]. To over- come the challenge, we first focus on one-dimensional data and generator in Sec. III-A, where we provide closed-form arXiv:2509.06505v1 [cs.LG] 8 Sep 2025 solutions for optimal generators in one-dimensional WGANs defined in Sec. II. For high-dimensional WGANs, we adopt the sliced WGAN framework and define a revised sliced WGAN with unprojected data distribution in Sec. II. We provide closed-form solutions for asymptotically optimal generators in high-dimensional sliced WGAN in III-B. The proofs are presented in Sec. IV, where we leverage on results in [11] to solve the inner discriminator problem in closed-form, which greatly simplifies the necessary conditions of optimal WGAN parameters. Moreover, our closed-form solutions do not need any training for the discriminator as [12] and hence provide additional benefit for training WGAN with a decentralized system [13]. Empirical studies in Sec. V show that our closed-form one-dimensional WGAN parameters have good convergence behavior with synthetic data under both Gaussian and Laplace distributions. Empirical studies also show that our closed-form high-dimesional sliced WGAN can acheive the same performance as r-PCA solutions, while requiring fewer computational resources. Throughout the paper, we adopt the following notational conventions. The operator diag(¬∑) : Rd√ód ‚ÜíRd extracts the diagonal entries of a d √ó d matrix and returns them", "response": "The generative adversarial network (GAN) aims to approximate an unknown distribution via a parameterized neural network (NN). While GANs have been widely applied in reinforcement and semisupervised learning as well as computer vision tasks, selecting their parameters often needs an exhaustive search and only a few selection methods can be proved to be theoretically optimal. One of the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG) setting, where the NN is linear and the data is Gaussian. In this paper, we focus on the characterization of optimal WGAN parameters beyond the LQG setting. We derive closed-form optimal parameters for one-dimensional WGANs when the NN has non-linear activation functions and the data is non-Gaussian. To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein framework and replace the constraint on marginal distributions of the randomly projected data by a constraint on the joint distribution of the original (unprojected) data. We show that the linear generator can be asymptotically optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our closed-form WGAN parameters have good convergence behavior with data under both Gaussian and Laplace distributions. Also, compared to the r principal component analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve the same performance while requiring less computational resources."}
{"prompt": "Title: Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting\n\nIntroduction The advent of Large Language Models (LLMs) has shifted the paradigm of human-computer inter- action, moving beyond one-shot prompting to more dynamic, multi-turn workflows [Sahoo et al., 2025, Li et al., 2025]. Foundational to this shift is instruction tuning, which aligns models to fol- low human feedback and engage in collaborative tasks [Ouyang et al., 2022]. Central to this new paradigm is the process of iterative refinement, where a user and an AI progressively improve an initial output [Xue et al., 2025]. This approach has become a key of modern LLM applications, with frameworks like SELF-REFINE and Reflexion demonstrating that models can even use self-generated feedback to enhance their outputs, highlighting the immense potential of iterative loops [Madaan et al., 2023, Shinn et al., 2023]. This evolution towards multi-step interaction leverages emergent cognitive capabilities of modern LLMs, which can manifest without explicit prompting [Arnold et al., 2024]. To enhance model reasoning and performance, a significant body of research has focused on developing sophisticated, structured prompting techniques. Seminal approaches like Chain-of-Thought (CoT), which breaks down problems into intermediate steps, have been shown to elicit stronger reasoning [Wei et al., 2022]. This has been extended to more complex methods like Tree-of-Thoughts (ToT), which explores multiple reasoning paths, and ReAct, which synergizes Preprint. arXiv:2509.06770v1 [cs.AI] 8 Sep 2025 reasoning with actions [Yao et al., 2023]. Other methods focus on providing models with specific, domain-relevant knowledge to improve the quality of specialized code generation [Gu et al., 2025] or employ complex search algorithms and multi-agent reflection to reinforce logical steps [Yuan and Xie, 2025]. These structured approaches have proven effective, demonstrating that with the right guidance, LLMs can be powerful and reliable reasoning engines. However, a critical gap exists between these highly-structured, engineered prompting methods and the far more common, ‚Äúnaive‚Äù interaction style where users provide simple, vague feedback. The behavior of LLMs in these unguided, multi-turn loops is poorly understood, with studies showing performance can drop significantly in multi-turn conversations compared to single-turn tasks [Laban et al., 2025]. Recent work suggests this process is fraught with risk; for example, a simple iterative prompt like ‚ÄúAre you sure‚Äù can paradoxically decrease a model‚Äôs truthfulness and increase overconfidence [Krishna et al., 2024]. This aligns with findings that models‚Äô self-correction abilities are often brittle and unreliable [Ji et al., 2023]. This degradation may be exacerbated in long contexts, where models struggle to access information from the ‚Äúmiddle‚Äù of the prompt history [Liu et al., 2024]. This creates a dangerous scenario analogous to ‚Äúmodel collapse‚Äù or a game of ‚Äúbroken telephone,‚Äù where a system feeding on its own output can enter a degenerative cycle [Mohamed et al., 2025]. These gaps motivate three questions that guide our study: How sensitive are iterative refinements to ‚Äúword choice‚Äù and instruction specificity? When does iterative refinement help‚Äîand when does it drift or collapse? If the do collapse, do all models collapse similarly? We address these questions with a controlled, turn-by-turn study. We run 12-turn conversations across ideation, mathematical reasoning, and code, log every turn, and compare two feedback settings: (i) vague prompts using three near-synonyms (‚Äúimprove,‚Äù ‚Äúmake it better,‚Äù ‚Äúrefine‚Äù) and (ii) specific steering along domain axes (novelty vs. practicality for ideation, speed vs. readability for code, elaboration vs. alternate method for math). Our evaluation focuses on dynamics, not just single-shot quality: we track innovation vs. stability across turns, growth in complexity and its plateaus, and semantic drift from the initial intent. We add a compact codebook of common failures (stagnation, over-engineering, flawed anchoring) and use LLM-assisted judgments. We also connect these dynamics to regime shifts suggested by recent work on phase-transition-style effects [Arnold et al., 2024, Nakaishi et al., 2024]. Our results show clear patterns. In ideas and code, when iteration helps, it does so early; in math, late turns can help when the prompt asks for elaboration. After a few turns, vague feedback often plateaus or reduces quality, while targeted steering reliably shifts the intended axis without large side effects. The degradation appears in domain-specific ways: ideation tends to repeat itself, code grows in size without meaningful change, math is fixed by default but can be broken by elaboration to find correct paths late. We quantify these effects with simple, defensible metrics‚Äîincluding Lexical Novelty (LN), growth factor, drift from origin, and turn-to-turn volatility‚Äîand we analyze sensitivity to word choice and instruction specificity across models.", "response": "Large language models (LLMs) are now used in multi-turn workflows, but we still lack a clear way to measure when iteration helps and when it hurts. We present an evaluation framework for iterative refinement that spans ideation, code, and math. Our protocol runs controlled 12-turn conversations per task, utilizing a variety of prompts ranging from vague ``improve it'' feedback to targeted steering, and logs per-turn outputs. We score outcomes with domain-appropriate checks (unit tests for code; answer-equivalence plus reasoning-soundness for math; originality and feasibility for ideation) and track turn-level behavior with three families of metrics: semantic movement across turns, turn-to-turn change, and output size growth. Across models and tasks, gains are domain-dependent: they arrive early in ideas and code, but in math late turns matter when guided by elaboration. After the first few turns, vague feedback often plateaus or reverses correctness, while targeted prompts reliably shift the intended quality axis (novelty vs. feasibility in ideation; speed vs. readability in code; in math, elaboration outperforms exploration and drives late-turn gains). We also observe consistent domain patterns: ideation moves more in meaning across turns, code tends to grow in size with little semantic change, and math starts fixed but can break that path with late, elaborative iteration.Together, the framework and metrics make iteration measurable and comparable across models, and signal when to steer, stop, or switch strategies."}
{"prompt": "Title: Curia: A Multi-Modal Foundation Model for Radiology\n\n1 Introduction Radiology is at the center of many medical special- ties, which rely on radiologists‚Äô interpretation of images from various modalities, including CT, MRI, ultra- sound, and X-ray [1]. The analysis of these images is crucial for detecting and characterizing medical conditions, quantifying disease progression, and mon- itoring treatment efficacy across a broad spectrum of diseases. AI has the potential to enhance radi- ology workflows and improve radiologists‚Äô efficiency, particularly for labor-intensive tasks such as image segmentation, or specialized and/or complex tasks which are prone to inter-reader variability [2, 3]. To date, the dominant paradigm in radiological AI devel- opment has involved training specialized models for individual tasks such as segmentation, abnormality detection (e.g., tumor detection), or pathology classifi- cation. However, this ‚Äúone-task, one-model‚Äù approach is exceptionally resource-intensive, as it necessitates the curation and manual annotation of large, task- specific datasets for each modality and clinical appli- cation [4, 5]. It is potentially one of the bottlenecks in moving AI radiology models into the clinical workflow. Foundation models (FM) represent a significant paradigm shift in the field of AI. More specifically, in 1 arXiv:2509.06830v1 [cs.CV] 8 Sep 2025 Evaluation against radiologists e Pre-training dataset preparation Hospital Clinical routine imaging 2019-2022 PACS 130 TB Data curation anonymizaton DinoV2 Training Curia a Top 20 Body Parts for CT and MR images Size of the pre-training dataset Comparison of Foundation Models on CuriaBench d Fine-tuning Frozen Supervised evaluation Image-level prediction Curia Features Predictor Intracranial Hemorrhage Object-level prediction Curia Features Predictor Liver Volume-level prediction Curia Curia Curia Aggregator Predictor Malignant Lung Nodule c CuriaBench Pathology Anatomy MRI Organ Recognition CT Organ Recognition Neuroimaging Age Estimation Image Registration Prompted Organ Segmentation Musculoskeletal Spinal Canal Stenosis Subarticular Stenosis Foraminal Narro wing ACL Tear Neurodegenerative Alzheimer ‚Äôs Disease Oncology Lung Nodule Malignancy K idney Tumor Malignancy Tumor Localisation K idney Cancer Survival Emergency Abdominal Trauma Stro ke Myocardial Infarction Intracranial Hemorraghe Infectious Pulmonary Infections Tasks Classification Regression Survival Prediction Segmentation Few-shot Cross Modal (CT / MRI) Generalization b Sex Distribution Age Distribution Fig. 1: Overview of Curia. Curia is a radiological foundation model for CT and MRI images, trained with self-supervised learning with the DINOv2 algorithm on 200M images, based on the vision transformer architecture. (a) Pre-training methodology and statistics - All reported numbers correspond to the number of 2D images. PACS = Picture Archiving and Communication System. (b) List of tasks and pathology areas evaluated in the benchmark. We evaluated Curia on classification, regression, survival prediction, and segmentation tasks, and also explore generalization in few-shot and cross- modal settings. (c) Method for supervised evaluations: image-level prediction, object-level prediction, and volume-level prediction. (d) Radar plot of Curia-L‚Äôs performance against MedImageInsight and BioMedCLIP. Metrics are detailed in Fig. 2. To provide robust estimates, we report the mean performance over 1000 bootstrap samples for each task. (e) Performance comparison of Curia-L against resident radiologists. We report the mean performance with 95% confidence intervals, calculated over 1000 bootstrap samples, along with the statistical significance using a paired bootstrap hypothesis test. the domain of natural images, self-supervised models such as DINOv2 [6] and MAE [7] have demonstrated the effectiveness of this approach, often reaching per- formances of supervised models. Leveraging large-scale unlabeled datasets, these models learn fine-grained semantic features that can be effectively transferred to downstream tasks using simple, lightweight classifiers with minimal or no fine-tuning. Adapting these methods to medical imaging is a promising solution to tackle the plethora of radiological use cases across multiple image modalities. By assisting radiologists in detecting and characterizing diseases, 2 these models may help improve patient outcomes and streamline clinical workflows. Ultimately, integrating FMs into radiology offers a path toward enhanced diagnostic precision, innovative research, and person- alized precision medicine [8, 9]. Previous research on FMs in radiology includes models such as Biomed- CLIP [10], BiomedParse [11], and MedImageInsight [12]. These models have been trained on medium-scale datasets (e.g., 15M images for BiomedCLIP), larger than typical supervised training datasets, but consid- erably smaller than those used for FMs in natural language or vision (e.g., 120M images for DINOv2). More critically, their", "response": "AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia."}
{"prompt": "Title: Learning from one graph: transductive learning guarantees via the geometry of small random worlds\n\nIntroduction Graph convolutional networks (GCNs) [31] have rapidly become indispensable in artificial intelli- gence (AI), powering applications from fake news detection[49] and prediction of protein-protein interaction [28] to early diagnosis of cognitive disorders [30] and climate modeling [35]. Beyond these, they enable route planning [62], content recommendation [68], and personalized online mar- ketplaces [61]. Crucially, GCNs can exploit complex graph-based information and the relational structure of data in ways that classical models, such as multi-layer perceptrons (MLPs), cannot, 1All authors contributed equally and are listed in alphabetical order. ‚àóHeinrich Heine University D√ºsseldorf, Mathematics Institute. nils.detering@hhu.de ‚Ä†Kings College London, Department of Mathematics. luca.galimberti@kcl.ac.uk ‚Ä°McMaster University, Department of Mathematics and Statistics. The Vector Institute. kratsioa@mcmaster.ca ¬ßThe London School of Economics and Political Science. g.livieri@lse.ac.uk ¬∂University of Vienna, Faculty of Mathematics. neumana53@univie.at.ac. 1 arXiv:2509.06894v1 [stat.ML] 8 Sep 2025 often achieving superior performance on tasks where graph connectivity is central [9, 18]. Applica- tions of GCNs can be broadly divided into inductive learning (IL) and transductive learning (TL) [59]. In IL, the model learns from multiple, often independent, graph-feature-label triples to make predictions on new, unseen graphs. TL, by contrast, presents a fundamentally different challenge: the learner observes only a single realization of a (possibly random) graph and its node features, along with a subset of node labels, and must infer the remaining labels. Classic TL tasks on graphs include link prediction [67], such as determining whether two individuals are connected in a single social network snapshot, and node classification [31], such as assigning research fields to papers in a partially labeled citation network. Beyond these canonical settings, TL phenomena routinely appear when employing random sub-sampling strategies [27, 47, 56] to scale GCNs to massive real-world graphs [1, 26, 48], highlighting the broad practical relevance and subtle challenges of TL. Acquiring labeled data remains a daunting and persistent obstacle in many real-world appli- cations. Human annotation is not only costly and time-consuming, but in numerous scenarios, labels are simply unavailable. Furthermore, users often lack access to the multiple independent graph-feature-label triples required for standard IL. Instead, they typically work with a single real- ization of a (possibly random) graph and its node features, with labels available for only a subset of nodes ‚Äì i.e. sub-sampling scenario that exemplifies transductive learning. The availability of only a single sample of the graph and node feature matrix contrasts with the standard statistical setting that underpins IL, which relies on multiple independent samples for inference (the law of large numbers or the central limit theorem). Consequently, TL problems are challenging to study in the absence of such tools, and the corresponding statistical literature remains limited ‚Äì often focusing on stylized models [54, 57] or relying on opaque variants of classical statistical objects, e.g. transductive Rademacher complexities [19, 63]. By comparison, IL guarantees for GCNs, for example, benefit from a wealth of classical statistical tools, giving rise to a rich and well-developed theoretical framework [8, 21, 37, 38, 41, 51]. It is apparent that establishing robust TL guarantees for GCNs is of paramount importance. In this work, we advance the field by introducing novel geometric tools that expand the statistician‚Äôs toolbox, focusing on concentration-of-measure techniques that exploit the emergent geometry of large, dense random graphs via innovative low-dimensional metric embedding arguments. Our transductive learning guarantees are both efficient and powerful: they remain effective when the number of labeled nodes N is small, and they attain the optimal non-parametric rate of OpN¬¥1{2q when N is large, highlighting the robustness of our approach across all regimes. 1.1 Contributions Our main contributions fall into two complementary categories. The first consists of transductive learning guarantees for standard regular graph learners, such as GCNs. Equally important, the sec- ond introduces new geometric tools that enrich the statistician‚Äôs toolbox and may have independent applications beyond GCNs. 1.2 Main results We establish concrete, broadly applicable TL guarantees for suitably regular graph learners (The- orems 3.1 and 3.2), treating both the deterministic setting and the common noise setting. In the former, the guarantees hold for any graph without isolated vertices and with arbitrary node fea- tures. (We use ‚Äúnode‚Äù and ‚Äúvertex‚Äù interchangeably.) In the latter, both the graph and the feature 2 matrix are modeled as single random draws, where the graph has diameter", "response": "Since their introduction by Kipf and Welling in $2017$, a primary use of graph convolutional networks is transductive node classification, where missing labels are inferred within a single observed graph and its feature matrix. Despite the widespread use of the network model, the statistical foundations of transductive learning remain limited, as standard inference frameworks typically rely on multiple independent samples rather than a single graph. In this work, we address these gaps by developing new concentration-of-measure tools that leverage the geometric regularities of large graphs via low-dimensional metric embeddings. The emergent regularities are captured using a random graph model; however, the methods remain applicable to deterministic graphs once observed. We establish two principal learning results. The first concerns arbitrary deterministic $k$-vertex graphs, and the second addresses random graphs that share key geometric properties with an Erd\\H{o}s-R\\'{e}nyi graph $\\mathbf{G}=\\mathbf{G}(k,p)$ in the regime $p \\in \\mathcal{O}((\\log (k)/k)^{1/2})$. The first result serves as the basis for and illuminates the second. We then extend these results to the graph convolutional network setting, where additional challenges arise. Lastly, our learning guarantees remain informative even with a few labelled nodes $N$ and achieve the optimal nonparametric rate $\\mathcal{O}(N^{-1/2})$ as $N$ grows."}
{"prompt": "Title: Robustness and accuracy of mean opinion scores with hard and soft outlier detection\n\nI. INTRODUCTION Multimedia system developers and content providers strive to maximize the quality of experience (QoE) in human con- sumption of the digital media they produce. Therefore, many methods have been established and standardized to quantita- tively assess the perceived quality of media. Such assessments are typically carried out in special laboratories that provide controlled conditions during the tests or in crowdsourcing environments. Crowdsourcing has the advantage that it is easier to recruit many participants of specific target groups such as from a certain country or age group, while it is harder to control for the viewing conditions that typically differ between participants. The recruited observers provide the quality values directly, for example, on the 5-level absolute category rating scale (ACR), or indirectly by comparing perceived quality between two or more presented stimuli. From the raw data collected, the experimenter then reconstructs the desired stimulus quality Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) ‚Äì DFG Project ID 251654672 ‚Äì TRR 161. 1github.com/UniStuttgart-VISUS/AdversarialAttacksForOutlierDetection on a linear scale as perceived by the group of observers. For the ACR data corresponding to a particular image, this can simply be the mean opinion score (MOS), which is the average of the ratings. Recently, better statistical methods that account for subject bias, inconsistency and more have been established and standardized (see Section II-B). Reliable subjective image quality assessment hinges on attentive and well-informed participants. However, inattention, fatigue, personal biases, strategic behavior, technical issues, and deviations from prescribed viewing conditions can all lead to inconsistent or unreliable ratings. Furthermore, data quality can be compromised by spammers, data entry errors, and outlier responses. Addressing these potential issues through careful experimental design, robust software, clear instruc- tions, and participant screening or data validation steps are crucial to obtain reliable and meaningful results in subjective visual quality assessments. For crowdsourcing, techniques for identifying unreliable observers are particularly important. A. Performance evaluation of outlier detection methods We use the term outlier detection jointly for methods that detect and handle such unreliable observers and outliers. The handling typically consists of either discarding the data of unreliable observers and outliers (hard outlier detection) or by weighting them according to the degree of their reliability (soft outlier detection). The soft outlier detection methods are integral parts of the scale reconstruction process. Generally, outlier detection identifies observers whose data significantly deviate from the consensus of the data provided by the reliable majority of observers. Several outlier detection methods for QoE studies have been introduced in the liter- ature and some have been standardized by the International Telecommunication Union (ITU). Therefore, there is a need to measure and compare the performance of these and newly proposed outlier detection methods. Recently, the robustness of several quality reconstruction methods, including outlier detection was evaluated by compar- ing quality estimations on clean and noisy datasets, generated, for example, by randomly changing some observer ratings or by adding some spammers; see Section II-C. Each method was applied to both datasets and the resulting estimations were compared using the root mean square difference (RMSD) to assess the impact of noise. A method with a smaller difference between the reconstructions before and after noise injection was considered to be more robust. arXiv:2509.06554v1 [eess.IV] 8 Sep 2025 Fig. 1: The MOS for one stimulus in the ground truth (¬µT ), the dataset sample (¬µS) and the sample with injected noise (¬µN). Two hard outlier detection methods (M1, M2) are compared, giving reconstructed MOS ÀÜ¬µS and ÀÜ¬µN. Method M2 is more robust, i.e., is less affected by noise injection (D2 < D1), but method M1 is better overall in the reconstruction (E1 < E2). B. Limitations of current approaches The above approach seems natural and generally accepted. However, it has two limitations that we address in our con- tribution. The first limitation is that this way of comparing outlier detection methods depends on the choice of synthetic observers. In the following paragraphs, we review two exam- ples of this in the literature. In Altieri et al. [1] (Figs. 4, 5), three soft outlier handling methods, ESQR, ZREC, and SUREAL, were compared on two datasets, one of which was KonIQ-10k [2], using two types of artificial observers. The first type was noise injection in which a given fraction of opinion scores was replaced by random scores from 1 to 5. For the second type, a given number of spammers were added who randomly annotated all stimuli from 1 to 5. T", "response": "In subjective assessment of image and video quality, observers rate or compare selected stimuli. Before calculating the mean opinion scores (MOS) for these stimuli from the ratings, it is recommended to identify and deal with outliers that may have given unreliable ratings. Several methods are available for this purpose, some of which have been standardized. These methods are typically based on statistics and sometimes tested by introducing synthetic ratings from artificial outliers, such as random clickers. However, a reliable and comprehensive approach is lacking for comparative performance analysis of outlier detection methods. To fill this gap, this work proposes and applies an empirical worst-case analysis as a general solution. Our method involves evolutionary optimization of an adversarial black-box attack on outlier detection algorithms, where the adversary maximizes the distortion of scale values with respect to ground truth. We apply our analysis to several hard and soft outlier detection methods for absolute category ratings and show their differing performance in this stress test. In addition, we propose two new outlier detection methods with low complexity and excellent worst-case performance. Software for adversarial attacks and data analysis is available."}
{"prompt": "Title: BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring\n\nI. INTRODUCTION Bioprinting is a transformative technology at the inter- section of tissue engineering, regenerative medicine, and additive manufacturing [1]‚Äì[3]. By precisely depositing cell- laden bioinks in a layer-by-layer manner, bioprinting enables the fabrication of complex three-dimensional (3D) biolog- ical structures, such as vascular networks, organoids, and scaffold-free tissues. As the field advances, maintaining high- resolution spatial fidelity and structural consistency becomes This is a preprint of a paper submitted to ICRA 2026. This work was supported by the University of Galway and funded by Research Ireland 1School of Computer Science, College of Science and Engineering, University of Galway, Galway, H91 TK33, Ireland. 2C ¬¥URAM ‚Äì SFI Research Centre for Medical Devices, University of Galway, Galway, H91 W2TY, Ireland. 3Biomedical Engineering, School of Engineering, College of Science and Engineering, University of Galway, Galway, H91 HX31, Ireland. ‚àóCorresponding Author: karl.mason@universityofgalway.ie ‚Ä† and ‚Ä°These authors contributed equally. critical, not only for functional outcomes, but also for bio- logical viability [4]. Real-time monitoring of bioprinting offers significant promise in improving print quality, reducing failure rates, and enabling adaptive control [5]‚Äì[8]. However, achieving effective in situ monitoring faces two major challenges. First, the dynamic nature of extrusion, including nozzle movement, variable flow rates, and deposition behaviors, requires high spatial and temporal resolution to capture rele- vant features. Second, most bioprinters operate in constrained environments with limited computational resources, making it impractical to deploy conventional deep learning models that rely on high-end GPUs. Semantic segmentation, which classifies each pixel of an image into semantic categories, has emerged as a powerful tool for fine-grained visual analysis. In bioprinting, segment- ing the nozzle, bioink, and background can provide critical feedback to assess extrusion quality, detect anomalies, and enable closed-loop control [9]. While segmentation has been widely adopted in fields such as autonomous driving and medical imaging, its use in bioprinting remains underex- plored, particularly on resource-constrained embedded de- vices where minimizing inference times and model size are critical. To address these challenges, we propose a lightweight semantic segmentation framework optimized for edge de- ployment in bioprinting environments. Our key contributions are as follows: ‚Ä¢ We introduce a new manually annotated dataset of 787 RGB images captured during bioprinting, labeled into three relevant classes: nozzle, bioink, and background. This dataset fills a key gap in the literature by enabling semantic understanding of the bioprinting process. ‚Ä¢ We design a novel BioLite U-Net architecture that uses depthwise separable convolutions to reduce computa- tional complexity without compromising segmentation accuracy for real-time inference on low-power devices. ‚Ä¢ We compare BioLite U-Net against MobileNetV2 and MobileNetV3-based segmentation backbones in terms of mIoU, Dice score, pixel accuracy, and inference latency on a Raspberry Pi 4, demonstrating a favorable trade-off between performance and efficiency. To our knowledge, this is the first study to perform real-time semantic segmentation of bioprinting imagery on embedded hardware. Our work lays the foundation for smart, autonomous bioprinters capable of adaptive control and real- arXiv:2509.06690v1 [cs.CV] 8 Sep 2025 time feedback, ultimately contributing to the broader vision of scalable and precise tissue fabrication. II. RELATED WORK Semantic segmentation has become increasingly important in the monitoring and control of additive manufacturing (AM) processes. Encoder-decoder architectures such as U- Net [10] have been widely adopted for pixel-level under- standing across many domains, from biomedical imaging to industrial inspection. In AM, segmentation models have been used to detect porosity, spatter, melt pool irregularities, and other layer-wise anomalies [11], [12]. DeepLabv3+ [13], which introduced atrous spatial pyramid pooling to capture multi-scale context, has shown strong performance in high- precision segmentation tasks. Despite these advances, segmentation has been underuti- lized in the bioprinting domain. Bioprinting presents unique challenges, such as the optical translucency of bioinks, the deformability of bioinks, and delicate biological constraints that limit imaging quality and consistency. Gugliandolo et al. [14] used infrared imaging to extract bioink filament profiles, but their approach was limited to thermal intensity thresholding. Ashley A Armstrong et al. [15] used a 2D laser displacement scanner to measure the deposited strands of bioink. These approaches provide classification or measure- ment heuristics but lack dense semantic labeling. Our work addresses this gap by proposing", "response": "Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems."}
{"prompt": "Title: Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems\n\n1. Introduction Artificial intelligence (AI)-based recommender systems have become widespread in recruitment [1, 2]. Recommender sys- tems are software applications that use artificial intelligence techniques to analyze data and provide specific suggestions or predictions to users. AI-based systems typically assist in discovering promising talents for development, identifying the most suitable candidates for a job opening, or assigning the right employees to projects based on their skill sets. In human resource management (HRM), these tools promise to accelerate processes, reduce human bias, and ground decisions in objective data [3, 4]. Current trends, such as HR Analytics and People Analytics, integrate AI to offer a broader promise of analytical rigor, predictive opportunities, and prescriptive recommendations for informed decision- making and actions [5], alongside technological modes of control [6]. In recruiting, AI recommender systems directly influence decisions about individuals, making them the sub- ject of regulations, such as the EU AI Act [7] and the GDPR [8]. Moreover, HR systems have faced severe criticism for fairness issues and biased recommendations [9, 10, 11, 12]. To mitigate potential biases and, equally important, to make optimal decisions, HR managers must understand the underlying data models and the mechanisms by which individual recommendations are generated. Explainable AI (XAI) techniques aim to make ‚Äúblack-box‚Äù models in- terpretable when they are opaque‚Äîdue to complexity or proprietary constraints [13, 14]. XAI methods offer interpretable, context-specific expla- nations for model decisions [15, 16]. In recruitment, these RecSys in HR‚Äô25: The 5th Workshop on Recommender Systems for Human Resources, in conjunction with the 19th ACM Conference on Recommender Systems, September 22‚Äì26, 2025, Prague, Czech Republic. *Corresponding author. ‚Ä†These authors contributed equally. $ yannick.kalff@htw-berlin.de (Y. Kalff); katharina.simbeck@htw-berlin.de (K. Simbeck) ¬Ä https://yannickkalff.de (Y. Kalff); https://iug.htw-berlin.de (K. Simbeck) \u001a 0000-0003-1595-175X (Y. Kalff); 0000-0001-6792-461X (K. Simbeck) ¬© 2025 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). explanations can clarify why a candidate‚Äôs application is ranked highly or why specific competencies are flagged during the CV parsing process. This transparency is essen- tial for HR managers who often have non-technical back- grounds and are responsible for legally and ethically sound decisions that comply with anti-discrimination laws. At the same time, from a human resources management per- spective, their decisions must be economically sensible and strategically appropriate for the company. A lack of trans- parency in AI elements or data, combined with unrecog- nized distortions, can lead users to incorrect conclusions. The issue is amplified by providers and developers, as trans- parency and explanations of AI interfaces remain the excep- tion in practice. Lacking transparency often seems to be a deliberate UI design decision (three exemplary dashboards can be found in the appendix Figure 4‚Äì6). However, attaching explanation widgets to a recruitment dashboard does not guarantee impact. For XAI to be effec- tive, HR managers must decode and critically evaluate the provided information [17]. We contend that AI literacy‚Äîa combination of knowledge, skills, and attitudes that enables individuals to understand and assess AI systems [18, 19]‚Äî directly affects both the subjective and objective effective- ness of XAI. Subjectively, AI literacy influences how help- ful, trustworthy, and accessible explanations appear. Ob- jectively, it affects accurate factual understanding that HR managers present when they interpret and act on the infor- mation provided by AI dashboards. To investigate this effect, we conducted an experiment with 410 German-based HR managers, who compared a baseline AI dashboard with versions enriched by three ex- planation styles: important features (a simplified feature im- portance approach), counterfactual explanations, and global model criteria summaries [20, 21]. Drawing on a genuine recruitment ranking tool that exists on the market, we mea- sured participants‚Äô perceived trust, usability, and assessment quality for each existing dashboard variant. Further, the participants assessed statements on the dashboards with correct or false answer possibilities. We address two guiding research questions: RQ1 How do HR managers‚Äô subjective perceptions of a re- 1 Yannick Kalff et al. CEUR Workshop Proceedings 1‚Äì9 cruiting recommender system change when adding explainable AI elements, and does this effect differ across different levels of AI literacy? RQ2 How does HR managers‚Äô objective understanding of a recruiting recommender system change when adding explainable AI elements, and does this effect differ across different levels of AI literacy", "response": "AI-based recommender systems increasingly influence recruitment decisions. Thus, transparency and responsible adoption in Human Resource Management (HRM) are critical. This study examines how HR managers' AI literacy influences their subjective perception and objective understanding of explainable AI (XAI) elements in recruiting recommender dashboards. In an online experiment, 410 German-based HR managers compared baseline dashboards to versions enriched with three XAI styles: important features, counterfactuals, and model criteria. Our results show that the dashboards used in practice do not explain AI results and even keep AI elements opaque. However, while adding XAI features improves subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, it does not increase their objective understanding. It may even reduce accurate understanding, especially with complex explanations. Only overlays of important features significantly aided the interpretations of high-literacy users. Our findings highlight that the benefits of XAI in recruitment depend on users' AI literacy, emphasizing the need for tailored explanation strategies and targeted literacy training in HRM to ensure fair, transparent, and effective adoption of AI."}
{"prompt": "Title: Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities\n\nI. INTRODUCTION The cybersecurity landscape constantly undergoes signifi- cant transformation driven by the cyber attack-defense arms Safayat Bin Hakim and Houbing H. Song are with the Department of Information Systems, University of Maryland, Baltimore County, Baltimore, MD 21250 USA (e-mail: shakim3@umbc.edu; songh@umbc.edu). Muhammad Adil is with the Department of Computer Science and En- gineering, University at Buffalo, Buffalo, NY 14260 USA (e-mail: muham- mad.adil@ieee.org). Alvaro Velasquez is with the Department of Computer Science, Uni- versity of Colorado Boulder, Boulder, CO 80309 USA (e-mail: al- varo.velasquez@colorado.edu). Shouhuai Xu is with the Laboratory for Cybersecurity Dynamics, De- partment of Computer Science, University of Colorado Colorado Springs, Colorado Springs, CO 80918 USA (e-mail: sxu@uccs.edu). race [1]‚Äì[5]. This race calls for revolutions against au- tonomous cyber attacks enabled by AI, such as automated re- connaissance, crafty evasive malware, and orchestrated large- scale campaigns exploiting conventional defense limitations [2], [6]. For instance, autonomous attacks can achieve State- Of-The-Art (SOTA) zero-day exploitation capabilities with substantial cost reductions [7]. Modern threats also evolve across multiple technological dimensions, creating challenges to traditional security ap- proaches [2], [6]. For instance, multi-agent systems, such as VulnBot, demonstrate remarkably higher completion rates in autonomous penetration testing than the baseline approaches [7], [8]. Operational cost reductions make sophisticated at- tacks accessible to resource-constrained threat actors. This development reshapes the cybersecurity threat landscape and demands analysis of both defensive innovations and dual- use implications. Traditional AI approaches used to counter different attacks face three fundamental challenges that limit their effectiveness in modern cybersecurity contexts. 1) Inadequate Grounding: Many commonly used tech- niques are insufficiently grounded in real-world cyber- security concepts and constraints, which limits their applicability [9]‚Äì[11]. Systems demonstrate powerful pattern recognition but lack fundamental understanding of security concepts, struggling to make meaningful connections between outputs and cybersecurity domain knowledge. This leads to brittleness against novel attack vectors and vulnerability to adversarial manipulation [12]‚Äì[14]. 2) Limited Instructibility: Traditional neural approaches prevent systems from adapting behavior appropriately in response to analyst feedback [15]. These systems require extensive retraining to incorporate new knowledge or modify behavior, creating delays in threat response when adversaries rapidly evolve their tactics. This limitation proves particularly problematic in cybersecurity contexts where real-time adaptation to emerging threats is key to defensive effectiveness. 3) Misalignment with Cybersecurity Objectives: AI sys- tems often optimize for metrics that inadequately capture true cybersecurity goals [16]. This leads to solutions that achieve high accuracy on benchmark datasets but fail to serve organizational security needs. Symbolic systems offer explainability and logical consistency crucial for security analysis but prove brittle when confronted with noisy, real-world data. The trade-off between symbolic arXiv:2509.06921v1 [cs.CR] 8 Sep 2025 2 precision and neural adaptability creates persistent chal- lenges that neither paradigm addresses adequately on its own. To address these fundamental limitations, Neuro-Symbolic AI has emerged as a promising paradigm that synergistically integrates neural network‚Äôs pattern recognition capabilities with symbolic reasoning‚Äôs logical foundations. Neuro-Symbolic (NeSy) AI: Addressing Fundamental Re- quirements through Advanced Integration. NeSy AI has emerged as a paradigm that synergistically integrates neural networks‚Äô pattern recognition capabilities with symbolic rea- soning‚Äôs logical foundations [17]‚Äì[19]. This paradigm aims to tackle the aforementioned grounding, instructibility, and alignment challenges encountered by traditional approaches. The paradigm achieves enhanced grounding by combining neural pattern recognition with symbolic knowledge represen- tation [20], [21]. This enables systems to understand cyberse- curity concepts via both statistical and logical perspectives. It maintains robustness against adversarial manipulation through explicit logical constraints. Improved instructibility emerges through integration mech- anisms whereby security analysts provide feedback to update both neural and symbolic components. This enables rapid adaptation to evolving threats without requiring extensive re- training cycles. Recent studies show how symbolic knowledge bases can be dynamically updated based on analyst expertise while neural components adapt to new data patterns [22]. Objective alignment is achieved through explicit encoding of cybersecurity pri", "response": "Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit fundamental limitations: inadequate conceptual grounding leading to non-robustness against novel attacks; limited instructibility impeding analyst-guided adaptation; and misalignment with cybersecurity objectives. Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize cybersecurity AI. However, there is no systematic understanding of this emerging approach. These hybrid systems address critical cybersecurity challenges by combining neural pattern recognition with symbolic reasoning, enabling enhanced threat understanding while introducing concerning autonomous offensive capabilities that reshape threat landscapes. In this survey, we systematically characterize this field by analyzing 127 publications spanning 2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A) framework to evaluate these systems, focusing on both cyber defense and cyber offense across network security, malware analysis, and cyber operations. Our analysis shows advantages of multi-agent NeSy architectures and identifies critical implementation challenges including standardization gaps, computational complexity, and human-AI collaboration requirements that constrain deployment. We show that causal reasoning integration is the most transformative advancement, enabling proactive defense beyond correlation-based approaches. Our findings highlight dual-use implications where autonomous systems demonstrate substantial capabilities in zero-day exploitation while achieving significant cost reductions, altering threat dynamics. We provide insights and future research directions, emphasizing the urgent need for community-driven standardization frameworks and responsible development practices that ensure advancement serves defensive cybersecurity objectives while maintaining societal alignment."}
{"prompt": "Title: Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training\n\n1. Introduction Transformer-based large language models (LLMs) have rapidly become foundational to progress across multiple domains, ranging from natural language understanding and reasoning to vision, robotics, and are regarded as a path towards artificial general intelligence. Current LLM designs almost universally rely on isotropic architectures - all layers have the same hidden dimensions, number of attention heads, and feed-forward widths. Al- though this design simplifies implementation and scaling, such uniform parameter allocation may not reflect the most efficient use of model capacity. In practice, different lay- ers of an LLM are known to play different functional roles, with earlier layers capturing local syntactic patterns and later layers responsible for high-level abstraction and reasoning (Jin et al., 2025) (Chen et al., 2024) (Skean et al., 2025). Therefore, it is reasonable to hypothesize that different lay- ers within the network may benefit from varying levels of computational capacity. A non-uniform, or heterogeneous, distribution of parameters across layers could lead to more efficient utilization of the model‚Äôs capacity, enhancing per- formance, all within the constraints of a fixed parameter budget. With this motivation, Mehta et al. (2024) introduced Layer- Wise Scaling (LWS) in OpenELM, achieving superior per- formance with half the pre-training tokens compared to sim- ilar models, attributing much of this gain to LWS. It assigns fewer parameters to initial layers and linearly increases it toward deeper layers. Despite these promising claims, the specific contribution of Layer-Wise Scaling remains am- biguous due to the absence of targeted ablation studies that isolate its effects from other architectural modifications. Another component adopted from Mehta et al. (2024) is Grouped Query Attention (GQA), which is tightly coupled with the LWS implementation in that work. Since GQA contributes to efficiency improvements and was used in the original LWS paper, we apply it consistently across all of our experiments. A parallel line of research has emerged around the improve- ment of inference efficiency through parameter pruning tech- niques, which aim to reduce model size and computational cost by removing redundant weights from already trained LLMs (Zhu et al., 2024). Initially, the pruning methods ap- plied isotropic sparsity patterns, treating all layers uniformly (Wang et al., 2024). However, more recent approaches have introduced a diverse range of strategies and algorithms that analyze and quantify the relative importance of parameters throughout the network and pruning layers accordingly. (Jin et al., 2025; Chen et al., 2024; Skean et al., 2025; He et al., 2024) In this work, we first conduct controlled experiments to isolate and evaluate the effect of (vanilla) Layer-Wise Scal- ing. Secondly, inspired by the findings from the pruning literature regarding the importance of certain layers, we in- vestigate whether initiating pre-training with architectures resembling the pruned models can enhance performance. Specifically, we explore three novel architectural variations: (i) applying LWS while maintaining maximal size in the first and last layers - Framed LWS; (ii) a reverse framed LWS strategy, beginning with larger initial layers that grad- ually reduce in size; and (iii) a crown-shaped configuration, reflecting empirical evidence suggesting central layers are of greatest importance, used together with framing. 1 arXiv:2509.06518v1 [cs.CL] 8 Sep 2025 Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training Our experiments are conducted on a 180M-parameter model, built upon the OLMo-core repository, and trained on a cor- pus of 5 billion tokens. Although the experiments are based on a 190M parameter model, this scale is increasingly rec- ognized as a reference setting to explore architectural effi- ciency under realistic computing constraints (Mehta et al., 2024). Small LLM models serve as reliable testbeds for evaluating generalization of architectural innovations (Hoff- mann et al., 2022a). Prior scaling studies have shown that techniques yielding efficiency gains in small models often extrapolate to larger scales when governed by principled design rules. Consequently, understanding the isolated ef- fect of layer-wise scaling at this size provides evidence for its potential application in larger models, where sample ef- ficiency translates directly into reduced training costs and environmental impact (Kaplan et al., 2020a). The code for our experiments is available open-source at https://github.com/baroian/OLMo-custom. In Section 2 we begin by reviewing related work on hetero- geneous architectures and motivating our four LWS profiles. In Section 3, we formalize the Layer-Wise Scaling frame- work and detail its four variants. Section 3 also outlines the model architectures, training setup, and datasets. Sec- tion 4 presents the results, while Section 5 conclud", "response": "Transformer-based language models traditionally use uniform (isotropic) layer sizes, yet they ignore the diverse functional roles that different depths can play and their computational capacity needs. Building on Layer-Wise Scaling (LWS) and pruning literature, we introduce three new LWS variants - Framed, Reverse, and Crown - that redistribute FFN widths and attention heads via two or three-point linear interpolation in the pre-training stage. We present the first systematic ablation of LWS and its variants, on a fixed budget of 180M parameters, trained on 5B tokens. All models converge to similar losses and achieve better performance compared to an equal-cost isotropic baseline, without a substantial decrease in training throughput. This work represents an initial step into the design space of layer-wise architectures for pre-training, but future work should scale experiments to orders of magnitude more tokens and parameters to fully assess their potential."}
{"prompt": "Title: Green Learning for STAR-RIS mmWave Systems with Implicit CSI\n\nI. INTRODUCTION ReconÔ¨Ågurable intelligent surfaces (RISs) are a key enabler for addressing the severe path loss, limited diffraction, and blockage sensitivity of millimeter-wave (mmWave) communi- cation in next-generation wireless systems [1]. By enabling programmable control of the propagation environment through nearly-passive meta-surfaces, RISs support passive beamform- ing, virtual line-of-sight links, and energy-efÔ¨Åcient coverage extension in MIMO systems [2]‚Äì[4]. However, conventional This work was supported in part by the Academia Sinica (AS) under Grant 235g Postdoctoral Scholar Program, the Sixth Generation Communication and Sensing Research Center funded by the Higher Education SPROUT Project, the Ministry of Education of Taiwan, and the National Science and Technology Council (NSTC) of Taiwan under Grant 113-2221-E-110-059-MY3, 113-2218- E-110-008, 113-2218-E-110-009, and 113-2926-I-001-502-G. Prof. Walid Saad was supported by the U.S. National Science Foundation (NSF) under Grant CNS-2225511. RISs are limited to one-sided reÔ¨Çection, which restricts full- space coverage, especially in 3D network deployments with users on both sides. To overcome this, the concept of simulta- neously transmitting and reÔ¨Çecting RIS (STAR-RIS) has been proposed [5], allowing each element to independently modulate both reÔ¨Çected and transmitted signals. This enables two-sided coverage, enhances spatial design Ô¨Çexibility, and promotes energy-efÔ¨Åcient, full-space wireless networking. To implement simultaneous transmission and reÔ¨Çection in practice, three operating protocols for STAR-RIS can be con- sidered: Energy splitting (ES), mode switching (MS), and time switching (TS) [1], [5]. In the ES protocol, each element simultaneously handles both transmission and reÔ¨Çection by al- locating a controllable power ratio between the two directions. While this mode provides the highest spatial Ô¨Çexibility and beamforming granularity, it also introduces a large number of coupled optimization variables, increasing system complexity. In contrast, the MS protocol partitions the RIS elements into two disjoint sets, where each element is dedicated to either transmission or reÔ¨Çection at a given time, thereby reduc- ing control complexity at the expense of some beamforming Ô¨Çexibility. The TS protocol further simpliÔ¨Åes implementation by alternating the entire surface between transmission and reÔ¨Çection phases over time, which decouples the optimization of the two functions but requires strict time synchronization. To improve content delivery efÔ¨Åciency and support massive IoT and URLLC in 6G communication systems [1], broad- casting transmission has gained increasing attention for multi- user scenarios where identical information must be delivered to multiple receivers [6]. In autonomous driving networks, for example, shared information such as road conditions needs to be distributed to a Ô¨Çeet of vehicles that may dynamically enter or leave the coverage area. Unlike unicast, which repeatedly transmits the same data stream to each user, broadcasting deliv- ers a common signal to multiple users simultaneously, reducing redundant transmissions, lowering power consumption, and improving spectral efÔ¨Åciency [7]. Motivated by these beneÔ¨Åts, this work considers a STAR-RIS-assisted MIMO broadcasting system operating under the ES protocol, providing a practical solution for low-power, full-space communication in 6G net- works. Recent studies have investigated precoding strategies for STAR-RIS-assisted MIMO systems using optimization-based and deep learning (DL)-based methods [8]‚Äì[11]. Classical arXiv:2509.06820v1 [eess.SP] 8 Sep 2025 optimization techniques typically rely on full CSI and adopt alternating optimization to jointly design the precoder and transmission/reÔ¨Çection coefÔ¨Åcients (TRCs), as exempliÔ¨Åed by the block coordinate descent (BCD) approach in [8]. Building on the above optimization-based approaches, to overcome the limitations of conventional methods that rely on full CSI, DL-based solutions have been proposed. For instance, hybrid deep reinforcement learning has been used to handle discrete TRC selection under coupled constraints [9], and convolutional neural networks (CNNs) have been ap- plied to approximate STAR-RIS conÔ¨Ågurations in short-packet simultaneous wireless information and power transfer non- orthogonal multiple access (SWIPT-NOMA) systems [10]. However, these methods still rely on large-scale deep neural networks (DNNs), demand full or high-Ô¨Ådelity CSI labels for training, and require substantial inference resources at runtime. Beyond full CSI designs, some recent efforts have attempted to relax CSI dependency. For example, the authors in [11] proposed a statistical CSI-based design that integrates STAR- RIS with movable antennas, jointly optimizing beamforming, TRCs, and antenna positions using fractional programming. While this approach partially relaxes the CSI assumption, it still depends on iterative solvers and doe", "response": "In this paper, a green learning (GL)-based precoding framework is proposed for simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems. Motivated by the growing emphasis on environmental sustainability in future 6G networks, this work adopts a broadcasting transmission architecture for scenarios where multiple users share identical information, improving spectral efficiency and reducing redundant transmissions and power consumption. Different from conventional optimization methods, such as block coordinate descent (BCD) that require perfect channel state information (CSI) and iterative computation, the proposed GL framework operates directly on received uplink pilot signals without explicit CSI estimation. Unlike deep learning (DL) approaches that require CSI-based labels for training, the proposed GL approach also avoids deep neural networks and backpropagation, leading to a more lightweight design. Although the proposed GL framework is trained with supervision generated by BCD under full CSI, inference is performed in a fully CSI-free manner. The proposed GL integrates subspace approximation with adjusted bias (Saab), relevant feature test (RFT)-based supervised feature selection, and eXtreme gradient boosting (XGBoost)-based decision learning to jointly predict the STAR-RIS coefficients and transmit precoder. Simulation results show that the proposed GL approach achieves competitive spectral efficiency compared to BCD and DL-based models, while reducing floating-point operations (FLOPs) by over four orders of magnitude. These advantages make the proposed GL approach highly suitable for real-time deployment in energy- and hardware-constrained broadcasting scenarios."}
{"prompt": "Title: Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments\n\nIntroduction To operate in natural human environments like homes and kitchens, robots must navigate safely in fast-changing, partially observable settings. This demands an intuitive understanding of robot‚Äôs own physical presence within the world. At the core of this capability is collision-free motion generation. Motion generation can operate beneath high-level policy layers such as VLMs or behavior cloning agents, ensuring that the robot‚Äôs actions remain both safe and physically feasible. To achieve this, robots have traditionally relied on motion planning approaches. Search-based methods, such as A* [1] and AIT* [2], are capable of finding globally optimum solutions, but they assume complete knowledge of the environment and static conditions. Due to their long execution times, these planners are typically run offline to generate fixed open-loop trajectories that the robot executes, limiting their performance in avoiding dynamic obstacles. Reactive controller-based approaches [3, 4, 5, 6], such as Riemannian Motion Policies (RMP) [4] and Geometric Fabrics [5], offer reactive collision avoidance in dynamic scenes. However, these approaches lack global scene awareness and often become trapped in local minima in complex environments [7]. An alternative is to formulate motion generation as a visuo-motor neural policy that maps raw visual observations directly to actions. Unlike open-loop planners, a learned visuo-motor policy continuously processes live sensory inputs‚Äîsuch as point clouds‚Äîto adapt its behavior on the fly without requiring known models of the scene. This real-time closed loop adaptability is crucial in scenarios where the goal becomes temporarily obstructed by dynamic obstacles or during sudden environmental changes. Generating such visuo-motor neural policies requires a robust training methodology. Several works have proposed using traditional motion planners to produce ground truth trajectories to serve as supervision for training [8, 7, 9, 10]. Unfortunately, prior methods such as MœÄNet [7] have only demonstrated limited success in simple settings and often fail to generalize to unseen environments, constraining their broader applicability. More recent efforts, like NeuralMP [9], attempt to address these generalization issues by introducing test-time optimization to correct for policy inaccuracies. While this improves accuracy, it requires runtime search before execution, sacrificing the reactivity necessary for fast-changing environments. Our method, Deep Reactive Policy (DRP), is a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly from point cloud sensory input. At the core of DRP is IMPACT (Imitating Motion Planning with Action-Chunking Transformer), a transformer-based neural motion policy. First, we pretrain IMPACT on 10 million motion trajectories generated in diverse simulation environments leveraging cuRobo [11], a state-of- the-art GPU-accelerated motion planner. Next, we finetune IMPACT with an iterative student-teacher method to improve the policy‚Äôs static obstacle avoidance capabilities. Finally, we integrate a locally- reactive goal-proposal module, DCP-RMP, with IMPACT to further enhance dynamic reactivity. Altogether, we call our approach Deep Reactive Policy (DRP), enabling the policy to generalize from learned experiences and develop an intuitive understanding of the changing environment. Our core contributions are: ‚Ä¢ We scale up motion data generation to train IMPACT, a novel end-to-end transformer-based neural motion policy conditioned on point cloud observations. ‚Ä¢ We further improve IMPACT‚Äôs obstacle avoidance performance via finetuning with iterative student-teacher distillation. ‚Ä¢ We enhance IMPACT‚Äôs dynamic obstacle avoidance performance via a locally reactive goal- proposal module, DCP-RMP. We evaluate DRP on both simulation and real-world environments, featuring complex obstacle arrangements and dynamic obstacles. DRP consistently outperforms previous state-of-the-art motion planning methods, as detailed in Section 4. 2 2 Related Work 2.1 Global Planning Methods Global planners generate collision-free trajectories by exploring the full state space, offering asymp- totic completeness and optimality. Search-based methods like A* [1] and its variants [12, 13, 14] guarantee optimality under admissible heuristics but scale poorly in high-dimensional continuous domains due to discretization. Sampling-based planners such as PRM [15], RRT [16], and their ex- tensions [17, 18, 19, 20, 2] improve scalability and efficiency through continuous-space sampling and informed exploration. Trajectory optimization methods [21, 22, 23] refine trajectories via continuous optimization but are sensitive to initialization and local minima. Recent work, cuRobo, advances trajectory optimization with GPU parallelization. However, it still plans from scratch for each new problem and struggles with noise and partial obse", "response": "Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. Video results and code available at https://deep-reactive-policy.com"}
{"prompt": "Title: Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination\n\nIntroduction Multi‚Äìtask learning (MTL) improves statistical efficiency and predictive accuracy by sharing information across related tasks, instead of learning each task in isolation (Caruana, 1997; Baxter, 2000; Evgeniou and Pontil, 2004; Ando and Zhang, 2005). Classical MTL couples ‚àóhuang.yian@columbia.edu ‚Ä†yang.feng@nyu.edu ‚Ä°zying@stat.columbia.edu 1 arXiv:2509.06575v1 [stat.ML] 8 Sep 2025 task predictors via shared regularizers, task clustering, or learned task relationships (Argyriou et al., 2008; Ji and Ye, 2009; Jacob et al., 2008; Kumar and Daum¬¥eIII, 2012; Xue et al., 2007). In parallel, representation learning provides transferable, low-dimensional structure that can dramatically reduce sample complexity across tasks (Bengio et al., 2013; LeCun et al., 2015). These threads meet in representation-based MTL, which assumes task parameters lie approximately in a shared low-rank subspace, enabling joint estimation of a common representation and per‚Äìtask coefficients (Argyriou et al., 2008; Maurer et al., 2016; Zhang and Yang, 2017; Crawshaw, 2020). Despite the provable gains over single-task learning delivered by representation-based MTL under ideal scenarios (Maurer et al., 2016; Tripuraneni et al., 2020; Thekumparampil et al., 2021; Saunshi et al., 2021), in practice, however, modern pipelines aggregate large, heterogeneous task collections harvested automatically. As a result, outlier tasks, contami- nated tasks, or even tasks subject to adversary attack may account for non-trivial proportion among all tasks in the MTL.a For example, in the application of computer vision and autonomous driving, multi-task networks jointly predict depth, surface, and instance segmentation from shared backbones (Sener and Koltun, 2018; Kendall et al., 2018; Standley et al., 2020). At scale, tasks harvested from different cities, sensors, or weather regimes can be weakly related, corrupted (e.g., due to miscalibrated LiDAR, systematic label noise, or even adversarially attacked). Such task-level anomalies act as outliers that can distort the shared representation if not robustly handled. In genomics and regulatory modeling, multi-assay prediction across cell types/assays naturally forms an MTL setting (e.g., multi-label regulatory prediction) (Zhou and Troyanskaya, 2015; Kelley et al., 2016). Cross-lab batch effects and mislabeled or low-quality cohorts can yield contaminated tasks (Johnson et al., 2007; Leek et al., 2010). A robust shared representation is essential to avoid negative transfer from corrupted assays or atypical cell types. In healthcare scenarios, survival models are often trained across multiple centers with heterogeneous coding practices, small cohorts, or population shifts (Ranganath et al., 2016; Katzman et al., 2018; Lee et al., 2018). Erroneous or atypical sites behave like outlier or contaminated tasks. Robust representation learning is needed to stabilize hazard estimation while preserving site-specific adaptations. These practical scenarios raises persistent challenges and requirements to guard against negative transfer when some tasks are weakly related or harmful (Pan and Yang, 2010; Weiss et al., 2016; Rosenstein et al., 2005; Standley et al., 2020). In the literature, however, robustness to contaminated tasks that violate the shared-structure assumption is less explored. Du et al. (2020); Tripuraneni et al. (2020, 2021); Niu et al. (2024); Thekumparampil et al. (2021) assume the absence of contaminated tasks in the MTL, with exactly the same shared representation. Chua et al. (2021); Duan and Wang (2023) follow the assumption of absence of outliers, while allowing similar but not exactly the same shared representation among tasks. Tian et al. (2023) relaxes the stringent assumption about outliers, proposing an algorithm that allows some small contamination proportion. Some sparsity and decomposition formulations are considered with the aim of detecting and isolating outlier tasks while sharing features among inliers (Jalali et al., 2010; Gong et al., 2012), but they require solving computationally aIn this paper, we do not distinguish between outlier tasks, contaminated tasks and adversarily attacked tasks, given their similarity. We shall use these words interchangeably in the paper. 2 complicated optimization, and only tolerate some small proportion of outliers. In this paper, we study representation MTL with unknown and potentially large contamina- tion proportion where the contaminated data can follow arbitrary distribution, while allowing heterogeneity of representations among tasks. We propose a robust and adaptive spectral method (RAS). Specifically, RAS adopts a data-driven singular-value threshold calibrated to the perturbation level, retaining directions whose signal exceeds the heterogeneity and noise floor. A final biased regularization step anchors task estimates to the learned subspace while preserving per‚Äìtask adaptivity. RAS requires neither the knowledge of intrinsic dime", "response": "Representation-based multi-task learning (MTL) improves efficiency by learning a shared structure across tasks, but its practical application is often hindered by contamination, outliers, or adversarial tasks. Most existing methods and theories assume a clean or near-clean setting, failing when contamination is significant. This paper tackles representation MTL with an unknown and potentially large contamination proportion, while also allowing for heterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral method (RAS) that can distill the shared inlier representation effectively and efficiently, while requiring no prior knowledge of the contamination level or the true representation dimension. Theoretically, we provide non-asymptotic error bounds for both the learned representation and the per-task parameters. These bounds adapt to inlier task similarity and outlier structure, and guarantee that RAS performs at least as well as single-task learning, thus preventing negative transfer. We also extend our framework to transfer learning with corresponding theoretical guarantees for the target task. Extensive experiments confirm our theory, showcasing the robustness and adaptivity of RAS, and its superior performance in regimes with up to 80\\% task contamination."}
