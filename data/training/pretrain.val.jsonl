{"text": "Title: A machine-learned expression for the excess Gibbs energy Authors: Marco Hoffmann, Thomas Specht, Quirin G√∂ttl, Jakob Burger, Stephan Mandt, Hans Hasse, Fabian Jirasek Categories: cs.LG, cs.CE Published: 2025-09-08 09:47:03+00:00 Abstract: The excess Gibbs energy plays a central role in chemical engineering and chemistry, providing a basis for modeling the thermodynamic properties of liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures solely from the molecular structures of their components is a long-standing challenge. In this work, we address this challenge by integrating physical laws as hard constraints within a flexible neural network. The resulting model, HANNA, was trained end-to-end on an extensive experimental dataset for binary mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent predictions. A novel surrogate solver developed in this work enabled the inclusion of liquid-liquid equilibrium data in the training process. Furthermore, a geometric projection method was applied to enable robust extrapolations to multi-component mixtures, without requiring additional parameters. We demonstrate that HANNA delivers excellent predictions, clearly outperforming state-of-the-art benchmark methods in accuracy and scope. The trained model and corresponding code are openly available, and an interactive interface is provided on our website, MLPROP."}
{"text": "Title: Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties Authors: Tanujit Chakraborty, Donia Besher, Madhurima Panja, Shovon Sengupta Categories: econ.EM, cs.LG, stat.AP, stat.ML Published: 2025-09-08 13:49:48+00:00 Abstract: Accurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil, Russia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that conventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics, including global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price growth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the need for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of external drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average (NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of neural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model, including asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques. We quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results across six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine learning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market participants navigating volatile financial conditions. The \\texttt{narfima} \\textbf{R} package provides an implementation of our approach. Paper excerpt: 1. Introduction Exchange rates significantly influence macroeconomic outcomes, shaping trade balances, capital flows, inflationary pressures, and financial stability, thereby becoming a central focus for policymakers, central banks, and market participants in an increasingly interconnected global economy (Eichengreen, 1998; Hausmann et al., 1999; Stoica and Ihnatov, 2016). Their role in assessing countries‚Äô financial stability has long been established (Taylor, 2001), with accurate forecasts proving indispensable for guiding monetary policy, designing capital controls, and implementing macro-prudential measures (Wieland and Wolters, 2013; Lubik and Schorfheide, 2007). For commodity-dependent economies, where exchange rate fluctuations directly impact inflation forecasts and broader economic stability, reliable projections are particularly vital (Rossi, 2013). Within this context, spot exchange rates, re- flecting current market prices for immediate currency delivery, are especially important as they directly influence trade prices, international capital flows, external debt obligations, and monetary policy decisions (Pilbeam and Langeland, 2015). Unlike aggre- gate measures such as the real effective exchange rate, spot rates respond swiftly to short-term market conditions, policy shifts, and uncertainty shocks, with their continuous trading facilitating rapid assimilation of new information (Bartsch, 2019). This makes spot markets an ideal setting for analyzing the transmission of oil price shocks and policy uncertainty to exchange rate dynamics across multiple horizons. Given the rising global prominence of the BRIC economies1, collectively accounting for 37.3% of world GDP (PPP) and a rapidly growing share of global trade (O‚ÄôNeill, 2011; Hopewell, 2017; Sengupta et al., 2025; Nasir et al., 2018), understanding and accurately forecasting spot exchange rates in these economies has become increasingly critical for macroeconomic planning, financial stability, and policy coordination. 1 As of January 1, 2024, the BRICS has expanded to include eleven members (Brazil, Russia, India, China, South Africa, Argentina, Egypt, Ethiopia, Iran, Saudi Arabia, and the United Arab Emirates). This expansion has increased the collective share of global GDP. Further details can be found at: https://www.weforum.org/stories/2024/11/brics-summit-geopolitics-bloc-international/. 1 arXiv:2509.06697v1 [econ.EM] 8 Sep 2025 2 Chakraborty et al. Although emerging economies such as the BRIC nations have become increasingly influential in the global economy, they remain particularly vulnerable to global shocks, often experiencing rapid and severe currency fluctuations than developed nations. This increased susceptibility stems from greater exposure to external shocks, fragile financial markets, and risk of sudden reversals in capital flows, a phenomenon termed ‚Äúflight-to-quality‚Äù (Bernanke et al., 1994; Calvo and Talvi, 2005). In response to these challenges, extensive literature has emphasized the importance of incorporating various forms of uncertainty measures, such as economic policy uncertainty (EPU) and geopolitical risks (GPR), into exchange rate forecasting frameworks (Kumar et al., 2024; Salisu et al., 2022). From a theoretical perspective, when domestic uncertainty exceeds foreign uncertainty, domestic investors tend to invest in foreign currency assets, triggering exchange rate movements (Balcilar et al., 2016). Moreover, economic uncertainties affect expectations about costs and returns, which in turn influence both supply and demand in currency markets (Benigno et al., 2012). Motivated by these theoretical foundations, recent empirical studies have focused on modeling the interplay between EPU and exchange rate dynamics. For instance, Zhou et al., 2020 demonstrated the enhanced forecasting performance of Generalised Autoregressive Conditional Heteroskedasticity - Mixed Data Sampling (GARCH-MIDAS) models incorporating Sino-US EPU in predicting Chinese exchange rate volatility, while Benigno et al., 2012 explored how monetary, inflation, and productivity uncer- tainties influence real exchange rates. Further studies have consistently confirmed the robust predictive power of EPU in emerging markets across short and long horizons (Colombo, 2013; Sin, 2015; Juhro and Phan, 2018; Abid, 2020). Alongside EPU, other financial uncertainty indices such as the US Equity Market Volatility (EMV) and US Monetary Policy Uncertainty (MPU) measure distinct aspects of the economic environment that potentially impact currency markets. Empirical evidence from Mueller et al., 2017 indicates that US MPU significantly affects currency risk premia, with emerging market currencies exhibiting greater sensi- tivity due to ‚Äúreach for yield‚Äù behavior. Additionally, Istrefi and Mouabbi, 2018 documents an inverse relationship between US MPU and economic activity, where increased US MPU typically prompts dollar appreciation driven by safe-haven demand d"}
{"text": "Title: Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference Authors: Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, Yansong Tang Categories: cs.AI, cs.LG Published: 2025-09-08 17:54:08+00:00 Abstract: Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x. Paper excerpt: 1. Introduction Online reinforcement learning (Online-RL) [5, 23, 24, 31] methods that perform a direct gradient update through dif- ferentiable rewards have demonstrated substantial potential to align diffusion models with human preferences. Com- pared to policy-based approaches [4, 7, 8, 18, 32], these methods use analytical gradients rather than policy gradi- ents, allowing more efficient fitting of reward preferences. However, they face two serious challenges. First, they re- strict optimization to only a few diffusion steps, making them more susceptible to reward hacking, a phenomenon where models achieve high reward scores for low-quality images [5, 6, 14, 18, 22, 32]. Second, they lack an on- line mechanism to adjust rewards and require costly offline preparations before RL to tune for desired aesthetic quali- ties such as photorealism or precise lighting effects. The first limitation stems from the conventional pro- cess of aligning the generation progress with a reward model. Existing methods typically backpropagate gradi- ents through a standard multistep sampler [11, 27], such as DDIM. However, these frameworks are not only com- putationally expensive but also prone to severe optimiza- tion instability, such as gradient explosion. This issue be- comes particularly acute when backpropagating gradients through the long computational graphs of early diffusion timesteps, forcing these methods to restrict optimization to the later stages of the trajectory. Nevertheless, this nar- row focus on late-stage timesteps makes the model prone to overfitting the reward, as demonstrated in our experi- ment (see Fig. 7). This overfitting manifests as reward hack- ing, leading models to exploit known biases in popular re- ward models. For instance, HPSv2 [30] develops a pref- erence for reddish tones, PickScore [12] for purple images and ImageReward [31] for overexposed regions. Previous work [3, 15] has also found that these models tend to prefer smoothed images with low-detail. To address this limita- tion, our method first injects predefined noise into the clean image, enabling the model to directly interpolate back to the original from any given timestep. The second challenge is the absence of mechanisms for online reward adjustment to accommodate the evolv- ing needs of real-world scenarios. To achieve superior visual quality, both the research community and industry often perform offline adjustments before RL. For exam- ple, contemporaneous works such as ICTHP [3] and Flux.1 Krea [15] have shown that existing reward models tend to favor images with low aesthetic complexity. ICTHP ad- dresses this issue by collecting a large, high-quality dataset to fine-tune the reward model, while other works such as DRaFT [5] and DanceGRPO [32] search for suitable re- ward systems to modulate image attributes such as bright- ness and saturation. In contrast, we propose treating re- wards as text-conditional signals, enabling online adjust- ment through prompt augmentation without the need for additional data. To further mitigate reward hacking, we reg- ularize the reward signal by using the relative difference be- tween conditional reward pairs, defined by predefined pos- itive and negative keywords applied to the same sample, as the objective function. This approach effectively filters out information irrelevant to semantic guidance. Consequently, we introduce a reinforcement learning framework, Seman- tic Relative Preference Optimization (SRPO), built upon Direct-Align. In our experiments, we first leverage SRPO to adjust standard reward models to two critical but often over- looked aspects: image realism and texture detail. Next, we rigorously compare SRPO with several state-of-the- art online RL-based methods on FLUX.1.dev, including ReFL [31], DRaFT [5], DanceGRPO [32], across a diverse set of evaluation metrics such as Aesthetic predictor 2.5 [1], Pickscore [12], ImageReward [31], GenEval [9], and hu- man assessments. Remarkably, our approach demonstrates a substantial improvement in human evaluation metrics. Specifically, compared to the baseline FLUX.1.dev [13] model, our method achieves an approximate 3.7-fold in- crease in perceived realism and a 3.1-fold improvement in aesthetic quality. Finally, we emphasize the efficiency of our approach. By applying SRPO to the FLUX.1.dev and training for only 10 minutes on HPDv2 dataset [30], our method enables the model to surpass the performance of the latest version of FLUX.1.Krea [15] on the HPDv2 bench- mark. In summary, the key contributions are as follows: ‚Ä¢ Mitigating Reward Hacking: The proposed framework effectively mitigates reward hacking. Specifically, it re- moves the limitation of previous methods that could only train on the later steps of the diffusion process. Further- more, we introduce a Semantic Relative Preference mech- anism, which regularizes the reward signal by evaluating each sample with both positive and negative prompt con- ditional preference."}
{"text": "Title: MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents Authors: Pengxiang Zhao, Guangyi Liu, Yaozhen Liang, Weiqing He, Zhengxi Lu, Yuehao Huang, Yaxuan Guo, Kexin Zhang, Hao Wang, Liang Liu, Yong Liu Categories: cs.AI Published: 2025-09-08 09:43:48+00:00 Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents. Paper excerpt: Introduction The rise of Large Language Models (LLMs) (et al. 2024; Zhao et al. 2023; Minaee et al. 2024; Chang et al. 2024; Xu et al. 2024) is driving the development of Graphical User Interface (GUI) Agents, enabling them to operate di- verse digital platforms such as computers, web browsers, and smartphones (Liu et al. 2025b; Tang et al. 2025; Liu et al. 2025a). Early research on mobile agents primarily focused on replicating human-like flexibility through GUI- only interaction (Lee et al. 2025; Gou et al. 2024; Cheng et al. 2024; You et al. 2024; Zhang et al. 2024; Lu et al. 2025). This approach grants agents the generality to oper- ate on any application, but it often overlooks the significant efficiency advantages offered by more direct methods. *These authors contributed equally. ‚Ä†Project Lead. ‚Ä°Corresponding authors. Figure 1: Workflow of GUI Only vs. GUI-Shortcut Hy- brid Agent. Shortcuts improve agent execution efficiency by bypassing GUI operations. As GUI agents are increasingly applied to complex tasks, enhancing their efficiency has become a central research focus. In this context, hybrid paradigms have emerged as a promising solution, demonstrating effectiveness across a wide range of platforms (Wang et al. 2025b; Zheng et al. 2025; Zhang et al. 2025a,b; Jiang et al. 2025; Lee et al. 2023). This approach combines the speed and reliability of ‚Äúshortcuts‚Äù such as Application Programming Interface (API) calls, deep links, and Robotic Process Automation (RPA) scripts (Bridle and McCreath 2006; Guerreiro, Gam- boa, and Jorge 2008; Kennedy and Everett 2011; Tripathi 2018; Ling, Gao, and Wang 2020; Agostinelli et al. 2022), with the flexibility of GUI operations. As Fig. 1 illustrates, a hybrid agent can bypass tedious, multi-step GUI operations by invoking a single, efficient shortcut, drastically reducing operational complexity and time. However, despite these advancements, a framework for systematically evaluating and benchmarking hybrid agents is still lacking (Le et al. 2020; Roffarello, Purohit, and Puro- hit 2024; Zhang et al. 2025a,b). This gap leaves the full po- tential of GUI-Shortcut agents far from being thoroughly ex- plored and assessed. While this is a general challenge, the mobile domain represents a critical and demanding environ- ment for progress. Therefore, we present MAS-Bench, the first benchmark explicitly designed to evaluate GUI-shortcut hybrid mo- bile agents. The tasks within MAS-Bench are solvable en- arXiv:2509.06477v1 [cs.AI] 8 Sep 2025 tirely through GUI interactions, but integrating shortcuts can improve efficiency. MAS-Bench evaluates agents‚Äô ability to make optimal decisions between different operational modes to enhance performance. Our experiments on MAS-Bench show that hybrid agents achieve up to a 64.1% success rate, a significant improvement over the 44.6% achieved by their GUI-only counterparts, and execute tasks with over 40% greater efficiency. Furthermore, we introduce a novel framework to eval- uate agents‚Äô capacity to generate new shortcuts from in- teraction. This framework integrates agent-generated short- cuts into a standard baseline agent and measures subsequent task performance. Our findings reveal a performance gap: while our predefined shortcuts prove highly reliable (100% success rate), agent-generated shortcuts lag in robustness and efficiency. Dynamic shortcuts demonstrate significant potential. Despite their task completion rate of 38% (base- line 43%), they offer the highest efficiency, highlighting them as a promising direction for future research into robust shortcut generation. Our contributions are threefold: ‚Ä¢ We introduce MAS-Bench, the first benchmark for systematically evaluating GUI-shortcut hybrid mobile agents. It comprises 139 complex tasks spanning 11 real- world applications, supported by a knowledge base of 88 predefined shortcuts and 7 distinct evaluation metrics. ‚Ä¢ We establish baselines with several agents on MAS- Bench, demonstrating that those leveraging GUI-shortcut hybrid operations achieve significantly higher success rates and efficiency than their GUI-only counterparts. ‚Ä¢ We propose the first framework to evaluate an agent‚Äôs ability to generate shortcuts from interaction. Our exper- iments reveal a substantial gap between agent-generated shortcuts and predefined ones in efficiency and robust- ness, highlighting a key area for future research. 2 Related Work Mobile Task Automation. Mobile task automation evolves from methods based on predefined scripts to more intelligent and adaptive agents driven by LLMs (Kirubakaran and Karthikeyani 2013; Amalfi- tano et al. 2014; Linares-V¬¥asquez, Moran, and Poshyvanyk 2017; Kong et al. 2018; Zhao, Harrison, and Yu 2024). Traditional approaches, such as API calls, deep links, and Robotic Process Automation (RPA) scripts, offer direct execution paths but suffer from significant limitations, including invalidation from app updates, and a sensitivity to UI changes that hinders thei"}
{"text": "Title: Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models Authors: Guan-Yu Chen, Xi Yang Categories: stat.ML, cs.LG, cs.NA, math.NA Published: 2025-09-08 16:23:58+00:00 Abstract: We propose a novel randomized framework for the estimation problem of large-scale linear statistical models, namely Sequential Least-Squares Estimators with Fast Randomized Sketching (SLSE-FRS), which integrates Sketch-and-Solve and Iterative-Sketching methods for the first time. By iteratively constructing and solving sketched least-squares (LS) subproblems with increasing sketch sizes to achieve better precisions, SLSE-FRS gradually refines the estimators of the true parameter vector, ultimately producing high-precision estimators. We analyze the convergence properties of SLSE-FRS, and provide its efficient implementation. Numerical experiments show that SLSE-FRS outperforms the state-of-the-art methods, namely the Preconditioned Conjugate Gradient (PCG) method, and the Iterative Double Sketching (IDS) method. Paper excerpt: Introduction Linear regression analysis is one of the most classic and fundamental methods used to describe the relationships between variables. Suppose that there exists a standard linear relationship between the response vector Y ‚ààRN and the feature matrix X ‚ààRN√ód with the sample size N and the feature size d as follows Y = XŒ≤ + Œ∂, where Œ≤ ‚ààRd is the unknown true parameter vector to be estimated and Œ∂ ‚ààRN represents the random noise vector with zero mean and a variance matrix œÉ2IN. To learn the unknown parameter Œ≤, we consider the ordinary least-squares (OLS) estimator ÀÜŒ≤, ÀÜŒ≤ = arg min Œ≤‚ààRd f(Œ≤; X, Y ), (1) with f(Œ≤; X, Y ) = 1 2‚à•Y ‚àíXŒ≤‚à•2 2. Throughout the paper, we assume that X has full column rank, then the OLS estimator can be explicitly formulated as ÀÜŒ≤ = (X‚ä§X)‚àí1X‚ä§Y. Due to its numerous well-established and favorable statistical properties (Chatterjee and Hadi, 2009), the OLS estimator is widely adopted to estimate the parameter Œ≤ in practice. However, for large-scale problems with N ‚â´d, the direct computational complexity O(Nd2) to obtain ÀÜŒ≤ becomes prohibitive. To address this challenge, numerous randomized algorithms based on sketching methods have been developed to obtain an approximation of the OLS estimator efficiently. The first classical randomized line to reduce the computational cost, known as Sketch-and-Solve (Drineas et al., 2011; Sarlos, 2006), is using sketching matrix S ‚ààRm√óN with m ‚â™N to construct the sketched data (SX, SY ) of the original data (X, Y ). Rather than solving problem (1) for the OLS ‚àóFirst author: Guan-Yu Chen (chenguanyuu@nuaa.edu.cn); Corresponding author: Xi Yang (yangxi@nuaa.edu.cn). ‚Ä†School of Mathematics, Nanjing University of Aeronautics and Astronautics, Nanjing 211106, China. 1 arXiv:2509.06856v1 [stat.ML] 8 Sep 2025 estimator from large-scale data (X, Y ), one can solve the following smaller sketched LS problem to obtain the sketched LS estimator ÀúŒ≤ as an approximation, ÀúŒ≤ = arg min Œ≤‚ààRd f(Œ≤; SX, SY ), with f(Œ≤; SX, SY ) = 1 2‚à•SY ‚àíSXŒ≤‚à•2 2. Then the direct methods can be called to compute the sketched estimator ÀúŒ≤ within O(md2) time. As the suboptimality illustrated in (Pilanci and Wainwright, 2016), any Sketch-and-Solve methods based on only observing a single pair of sketched data (SX, SY ), unless the sketch size m ‚â•N, necessarily has a substantially larger error than the OLS estimator. In other words, with a small sketch size m, the Sketch-and-Solve methods result in estimators with relatively low precision. The second widely adopted line is Iterative Sketching, which involves repeatedly sketching the problem and iteratively refining the estimator. Recently, an Iterative Sketching method has attracted significant attention, the Iterative Hessian Sketch (IHS) (Pilanci and Wainwright, 2016) and its variants. IHS is an effective and efficient iterative sketching method for large-scale LS problems, which uses refreshed sketched Hessian matrix Ht = X‚ä§S‚ä§ t StX to approximate the Hessian matrix H = X‚ä§X of (1). The update formula can be expressed as: Œ≤t+1 = Œ≤t ‚àíH‚àí1 t ‚àáf(Œ≤t; X, Y ), where the sketching matrices S0, . . . , St, . . . are independent and identically distributed (i.i.d.) of size m √ó N, with m ‚â™N and ‚àáf(Œ≤; X, Y ) := X‚ä§(XŒ≤ ‚àíY ). The theoretical results in (Derezi≈Ñski and Mahoney, 2024) guarantee that, with high probability, the prediction error can decay at a constant rate and the output can serve as a high-precision estimator for Œ≤. In (Ozaslan et al., 2019), the convergence rate of IHS is significantly improved by incorporating a momentum term and using a fixed sketching matrix ÀÜS ‚ààRm√óN to approximate the Hessian matrix, denoted as ÀÜH = X‚ä§ÀÜS‚ä§ÀÜSX. This approach avoids the repeated construction of the inverse of the Hessian sketch H‚àí1 t . The update formula can be represented as: Œ≤t+1 = Œ≤t ‚àí¬µ ÀÜH‚àí1‚àáf(Œ≤t; X, Y ) + Œ∑(Œ≤t ‚àíŒ≤t‚àí1), Based on Marchenko-Pastur law, Ozaslan et al. (2019) investigated the optimal step sizes ¬µ and Œ∑, achieving a data-independent convergence rate (d/m)1/2. See more related work in (Tang et al., 2017; Lacotte et al., 2021; Lacotte and Pilanci, 2020, 2021; Na et al., 2023; Epperly, 2024) Inspired by the results in (Dobriban and Liu, 2019), we noticed that the asymptotic precision of the sketched LS estimator of the Sketch-and-Solve method can be explicitly formulated by a function of the sketch size m, the sample size N and the feature size d. For a fixed sketching matrix (e.g., Gaussian or SRHT), increasing m improves the precision of the estimator but also raises the computational costs. This theory appears to suggest that we can only strike a balance between improving accuracy and reducing computational complexity. In fact, it provides us with an opportunity to enhance the precision while simultaneously decreasing the computational costs. We suggest applying the Sketch-and-Solve method multiple times with a carefully constructed sequence of sketched LS subproblems with increasing sketch sizes. The precision of the estimators can be"}
{"text": "Title: Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards Authors: Noel Codella, Sam Preston, Hao Qiu, Leonardo Schettini, Wen-wai Yim, Mert √ñz, Shrey Jain, Matthew P. Lungren, Thomas Osborne Categories: cs.LG, cs.AI Published: 2025-09-08 12:15:53+00:00 Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology specialists collaboratively assess complex patient cases to determine optimal treatment strategies. A central element of this process is the patient summary, typically compiled by a medical oncologist, radiation oncologist, or surgeon, or their trained medical assistant, who distills heterogeneous medical records into a concise narrative to facilitate discussion. This manual approach is often labor-intensive, subjective, and prone to omissions of critical information. To address these limitations, we introduce the Healthcare Agent Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that coordinates a multi-agent clinical workflow to generate accurate and comprehensive patient summaries for MTBs. Evaluating predicted patient summaries against ground truth presents additional challenges due to stylistic variation, ordering, synonym usage, and phrasing differences, which complicate the measurement of both succinctness and completeness. To overcome these evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework designed to assess the comprehensiveness and succinctness of generated summaries. Using a benchmark dataset derived from de-identified tumor board discussions, we applied TBFact to evaluate our Patient History agent. Results show that the agent captured 94% of high-importance information (including partial entailments) and achieved a TBFact recall of 0.84 under strict entailment criteria. We further demonstrate that TBFact enables a data-free evaluation framework that institutions can deploy locally without sharing sensitive clinical data. Together, HAO and TBFact establish a robust foundation for delivering reliable and scalable support to MTBs. Paper excerpt: Introduction Molecular Tumor Boards (MTBs) are among healthcare‚Äôs most complex collaborative workflows, where radiologists, pathologists, oncologists, geneticists, and other specialists align on patient-specific cancer treatment strategies. The backbone of these meetings is a comprehensive yet concise patient summary that distills heterogeneous records (clinical notes, imaging, pathology, genomics) into a coherent timeline that orients discussion and decision-making. However, manual preparation of these summaries imposes a significant time burden: radiologists and pathologists report mean preparation times of 81.7 and 144.0 minutes respectively [1]. Quality is also variable; the study notes that the ‚àóEqual contribution. ‚Ä†Correspondence: hlsfrontierteam@microsoft.com Preprint. arXiv:2509.06602v1 [cs.LG] 8 Sep 2025 inclusion of comorbidities and patient perspectives often falls below average standards, and the quality of case history and pathological information remains inconsistent [1]. While general-purpose large language models have demonstrated impressive capabilities across many domains, they face critical limitations in high-stakes healthcare settings. Precision is critical: even minor hallucinations or inconsistencies can compromise patient safety and decision quality. Effective decision-making also requires multi-modal integration‚Äîcorrelating imaging, pathology, genomics, and structured Electronic Health Record (EHR) data‚Äîmuch of which lies outside the scope of public training corpora. Finally, transparency and traceability are essential: clinicians must understand how conclusions are formed and be able to audit intermediate steps. To address these challenges, we introduce the Healthcare Agent Orchestrator, a modular, LLM- driven multi-agent system that mirrors the collaborative structure of real tumor boards. Rather than relying on a single monolithic model, HAO coordinates specialized agents‚Äîeach focused on a domain such as patient history, radiology, pathology, staging, or clinical guidelines‚Äîwhile maintaining coherence and explainability through an orchestrator. This design enables grounded reasoning across heterogeneous data sources and aligns with the multidisciplinary nature of oncology care, allowing HAO to expand into other use-cases. Evaluating the quality of generated patient summaries introduces its own challenges. Common simi- larity metrics that emphasize surface overlap are insensitive to clinically meaningful differences: two summaries may use different phrasing, ordering, or synonyms while conveying the same facts‚Äîor conversely, appear similar while omitting critical details or introducing unsupported claims. We there- fore implement TBFact, an evaluation framework that operates at the level of clinical factual claims, and assess bidirectional entailment between candidate and reference to quantify both completeness (coverage of reference facts) and succinctness (a precision-oriented proxy that penalizes unsupported or extraneous statements). 2 Healthcare Agent Orchestrator (HAO) HAO coordinates role-specialized agents through a facilitator (the orchestrator) that manages turn- taking, shared memory, and verification checkpoints. Agents such as PatientHistory, Radiology, or ClinicalTrials focus on complementary functions aligned to MTB workflows. The orchestrator plans and moderates the interaction, dynamically selecting only the agents needed for a case and recording an auditable trail of intermediate results. The interaction model follows a structured group- chat abstraction implemented with Semantic Kernel and Magentic-One, extended for healthcare complexity, and uses Model Context Protocol (MCP) for secure, two-way tool/data connectivity. Teams-native collaboration experience The user experience is embedded in Microsoft Teams, where clinicians and developers converse with the orchestrator or directly @-mention agents inside a channel or group chat. This reduces workflow switching and lets multi-human, multi-agent conversations unfold in the same thread used for case coordination. Outputs (e.g., patient timelines or draft MTB briefs) are distributable across Microsoft 365 apps (Word, PowerPoint) for rapid handoff to tumor board packets and follow-ups. Design goals and orchestration rationale HAO is designed around three principles: (i) precision under specialization, by engaging only the most relevant agents for a case; (ii) traceability, through shared memory, inline citations, and auditable intermediate artifacts; and (iii) safety-by-design, via domain-aware verification checkpoints and constrained tool routing. These choices aim to reduce error propagation and over-orchestration noise while preserving the benefits of division of labor across domains (history, radiology, pathology, trials). This flexibility supports diverse use cases‚Äîfrom rapid single-agent timelines to multi-agent workflows for complex cases‚Äîwhile maintaining transparency and alignment with institutional govern"}
{"text": "Title: Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding Authors: Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min, Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen, Zhi-Hong Deng Categories: cs.LG Published: 2025-09-08 17:36:21+00:00 Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks. Paper excerpt: INTRODUCTION Recent large language models (LLMs) such as OpenAI-o1 (OpenAI et al., 2024), DeepSeek- R1 (DeepSeek-AI et al., 2025a), and Kimi-K2 (Team et al., 2025) have made remarkable break- throughs in reasoning ability, benefiting from long chain-of-thought that incorporates self-reflection and revision. This capability can be realized through pure reinforcement learning with verifiable rewards (RLVR), which forgoes memorizing annotated reasoning processes and instead exploits the model‚Äôs inherent capabilities by strengthening correct exploratory behaviors (Chu et al., 2025). At present, RLVR has become the common practice for building high-performance reasoning models. ‚àóThe first two authors contribute equally. ‚Ä†Corresponding authors. 1 arXiv:2509.06923v1 [cs.LG] 8 Sep 2025 Preprint, Working in Progress ‚Ä¶ Recall that a set is uncountable if ‚Ä¶ any non-empty set that is power set-like has an uncountable subset not in bijection with the original set ... Therefore, any uncountable set implies such a subset. <answer> True </answer> ‚Ä¶ The existence of ùúîùúî1 demonstrates that not all uncountable sets have an uncountable subset of strictly smaller cardinality. <answer> False </answer> Policy LLM Understanding the Statement An uncountable set is one whose cardinality ... Analyzing ùùéùùéùüèùüèas a Counterexample Any uncountable subset ùëÜùëÜ‚äÜùúîùúî1 is cofinal ‚Ä¶ Hint Problem Determine whether the following statement is true or false: every uncountable set has an uncountable subset that is not in bijection with the original set? Figure 1: Comparison between the direct rollout (blue) and hinted rollout (purple). The hint consists of the first few steps from an annotated solution. Adding a hint can simplify the problem and guide the LLM toward completing correct solutions. However, on-policy exploration inherently constrains the learning efficiency, exhibiting strong data dependency (Gao et al., 2025; Dou et al., 2025; Schmied et al., 2025; Yu et al., 2025; Zhang et al., 2025b; Sun et al., 2025). RLVR is driven by the rewards from extensive online sampled rollouts, which collapse to zero when the training problems are too difficult for LLMs to produce a correct response. Conversely, overly simple problems yield nearly all correct rewards, producing minor advantage value. It remains unclear what problem difficulty maximizes learning efficiency and how to curate such data. Moreover, as recent studies (Gandhi et al., 2025; Yue et al., 2025; Zhao et al., 2025) have found, RLVR merely amplifies existing behaviors rather than fostering novel cognitive capabilities, thereby limiting the achievable performance to that of the base model. Supervised fine- tuning (SFT) (K¬®opf et al., 2023) is a naive way to improve the ability of LLMs before RL with expert data. However, existing works (Zhang et al., 2025c; Chen et al., 2025) show that directly using SFT-then-RL is not an effective way, which even underperforms pure RL. To overcome these limitations, recent works have attempted to integrate SFT into the RL frame- work, enabling synergistic learning when SFT-then-RL is ineffective. LUFFY (Yan et al., 2025) and SRFT (Fu et al., 2025) incorporate parallel off-policy guidance into the rollout set, allowing simulta- neous exploration and imitation. UFT (Liu et al., 2025a), TAPO (Wu et al., 2025), StepHint (Zhang et al., 2025a), Hint-GRPO (Huang et al., 2025a), and Prefix-RFT (Huang et al., 2025b) append an off-policy hint prefix to each problem to reduce exploration difficulty. While these methods allow for a certain degree of problem difficulty modulation, they have a salient shortcoming: off-policy guid- ance is applied statically and indiscriminately across problems and hint levels, causing most training samples‚Äô difficulty to mismatch the model‚Äôs evolving capability. Consequently, for those challeng- ing problems, mild off-policy guidance does not effectively resolve the low efficiency of on-policy exploration, whereas for those easy problems, excessive off-policy intervention may impede the LLM from developing its own reasoning patterns. This observation raises a critical question: What is the appropriate problem difficulty under off-policy guidance, and how can the difficulty be dynamically adjusted in accordance with the model‚Äôs evolving capability? In this paper, we propose SEELE: reSponsive rEasoning Evolution via capabiLity-adaptivE hint scaffolding, a theoretically-grounded supervision-aided RVLR approach that keeps high learning efficiency throughout the whole training stage via dynamically adjusting the proportion of the off- policy prefixes. SEELE explicitly formalizes that an appropriately difficult problem should yield a rollout accuracy of approximately 50% through a theoretical analysis of the loss descent magni- tude. By appending an instance-specific, dynamically determined hint after the original problem (Figure 1), SEELE is able to control the problem difficulty within the ‚Äúsweet spot‚Äù. Unlike previous hint-based me"}
{"text": "Title: Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning Authors: William Xu, Yiwei Lu, Yihan Wang, Matthew Y. R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu Categories: cs.LG, stat.ML Published: 2025-09-08 17:14:55+00:00 Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to their ease of deployment and high success rates. These attacks aim to manipulate the prediction for a single test sample in classification models. Unlike indiscriminate attacks that aim to decrease overall test performance, targeted attacks present a unique threat to individual test instances. This threat model raises a fundamental question: what factors make certain test samples more susceptible to successful poisoning than others? We investigate how attack difficulty varies across different test instances and identify key characteristics that influence vulnerability. This paper introduces three predictive criteria for targeted data poisoning difficulty: ergodic prediction accuracy (analyzed through clean training dynamics), poison distance, and poison budget. Our experimental results demonstrate that these metrics effectively predict the varying difficulty of real-world targeted poisoning attacks across diverse scenarios, offering practitioners valuable insights for vulnerability assessment and understanding data poisoning attacks. Paper excerpt: Introduction In the past decade, machine learning (ML) models have achieved great success in various domains, largely due to the vast amount of training data available on the internet. However, this reliance on massive training datasets not only increases computational costs but also introduces significant security vulnerabilities during the data collection process [KNL+20; SZS+14]. Adversaries can exploit these vulnerabilities through data poisoning attacks which deliberately inject malicious samples into training data either actively or passively [CJC+24; GBB+20; LYY20; SHKR22; Wak16]. These attacks are particularly concerning because they can compromise model integrity at its foundation, affecting all downstream applications and users of the poisoned model [GTX+23]. Targeted data poisoning attacks represent a specialized form of this threat, where attackers aim to manipulate model behavior for specific test instances while maintaining normal performance on all other inputs [e.g., AMW+21; GFH+21; GL20; SHN+18; ZHL+19]. We primarily focus on classification models (and briefly discuss generative models in Section D), where the objective is to misclassify a particular sample ‚àóAuthors WX and YL have equal contribution. Authors YW, MYRY, ZL are listed in contribution order. Authors GK and YY are listed in alphabetical order. ‚Ä†Cheriton School of Computer Science, University of Waterloo. w262xu@uwaterloo.ca. ‚Ä°University of Ottawa. yiwei.lu@uottawa.ca. ¬ßCheriton School of Computer Science, University of Waterloo. yihan.wang@uwaterloo.ca. ¬∂Carnegie Mellon University. myang4@cs.cmu.edu. ‚ÄñGoogle. robertliu@google.com. ‚àó‚àóCheriton School of Computer Science, University of Waterloo and Vector Institute. g@csail.mit.edu. Supported by a Canada CIFAR AI Chair, an NSERC Discovery Grant, and an Ontario Early Researcher Award. ‚Ä†‚Ä†Cheriton School of Computer Science, University of Waterloo and Vector Institute. yaoliang.yu@uwaterloo.ca. 1 arXiv:2509.06896v1 [cs.LG] 8 Sep 2025 to a predetermined class while maintaining correct predictions for all other inputs. Such attacks are difficult to detect as they leave little evidence in overall model performance metrics. Current evaluations of targeted attack threats typically rely on randomly selected test samples to report overall attack success rates [e.g., AMW+21; GFH+21] as an average assessment. However, our observations reveal substantial variation in attack effectiveness across different test instances, with no clear understanding of what characteristics drive these disparities. This paper addresses this critical knowledge gap by investi- gating two key research questions: (1) what factors determine why a certain test sample is more vulnerable to targeted poisoning attacks than others? and (2) can we develop reliable metrics to predict the difficulty of poisoning a specific instance? We address the first question by identifying three critical factors that influence poisoning difficulty: the inherent classification difficulty during clean training, the distance in model parameter space required to achieve poisoning, and the attacker‚Äôs resource constraints measured by poison budget. To quantify these factors, we introduce three corresponding metrics that naturally predict poisoning difficulty: (1) ergodic prediction accuracy (EPA) derived from clean training dynamics, (2) poisoning distance Œ¥, and (3) poisoning budget lower bound œÑ. Importantly, all our proposed metrics can be calculated using only clean training data and processes, without requiring the simulation of actual poisoning attacks‚Äîmaking them practical and accessible tools for defenders to assess vulnerability and prioritize protection efforts. Our experimental results confirm that all three proposed factors strongly correlate with real-world attack performance. The metrics we developed effectively predict poisoning difficulty across various test instances, capturing different dimensions and levels of vulnerability. Specifically, we find that ergodic prediction ac- curacy (EPA) serves as a powerful indicator for distinguishing between easy-to-poison and hard-to-poison test samples. Meanwhile, poisoning distance Œ¥ and budget lower bound œÑ provide more fine-grained pre- dictions for specific poison classes from complementary perspectives. Together, these three metrics form a comprehensive framework that enables defenders to assess poisoning vulnerability for any given test sample. In summary, our work makes three distinct contributions: (1) We identify classification difficulty during clean training, parameter-space poisoning distance, and poison budget as the key factors determining targeted poisoning vulnerability; (2) We introduce three corresponding metrics: ergodic prediction accuracy (EPA), poisoning distance, and budget lower bound, all calculable using only clean training processes without any expensive attack and retraining; (3) Our experiments demonstrate the effectiveness of these metrics and introduce a fr"}
{"text": "Title: MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization Authors: Niki Kotecha, Ehecatl Antonio del Rio Chanona Categories: cs.AI Published: 2025-09-08 09:51:24+00:00 Abstract: In supply chain management, decision-making often involves balancing multiple conflicting objectives, such as cost reduction, service level improvement, and environmental sustainability. Traditional multi-objective optimization methods, such as linear programming and evolutionary algorithms, struggle to adapt in real-time to the dynamic nature of supply chains. In this paper, we propose an approach that combines Reinforcement Learning (RL) and Multi-Objective Evolutionary Algorithms (MOEAs) to address these challenges for dynamic multi-objective optimization under uncertainty. Our method leverages MOEAs to search the parameter space of policy neural networks, generating a Pareto front of policies. This provides decision-makers with a diverse population of policies that can be dynamically switched based on the current system objectives, ensuring flexibility and adaptability in real-time decision-making. We also introduce Conditional Value-at-Risk (CVaR) to incorporate risk-sensitive decision-making, enhancing resilience in uncertain environments. We demonstrate the effectiveness of our approach through case studies, showcasing its ability to respond to supply chain dynamics and outperforming state-of-the-art methods in an inventory management case study. The proposed strategy not only improves decision-making efficiency but also offers a more robust framework for managing uncertainty and optimizing performance in supply chains. Paper excerpt: Introduction In recent years, the field of Process Systems Engineering (PSE) has experienced a significant shift towards advanced optimization techniques that are capable of handling the inherent complexities of modern systems, whether in process control (Li√±√°n and Ricardez-Sandoval, 2025), sustainable supply chains (Qiu et al., 2024; Grossmann and Guill√©n- Gos√°lbez, 2010; Rangel-Martinez et al., 2021), or other industrial operations (Szatm√°ri et al., 2024). A key challenge in PSE is the need to optimize multiple, often conflicting objectives simultaneously, such as minimizing costs, maximizing throughput, ensuring quality, and reducing environmental impact. This complexity has become particularly evident as the development of sustainable practices has emerged as a key strategic focus for many organizations, driven by the increase in regulatory requirements and pressure from consumers to adopt environmentally friendly practices. As industries strive towards making their value chains more sustainable, the integration of these principles into every aspect of their operations, particularly in decision-making, becomes essential. The historical roots of multi-objective optimization trace back to the work of Vilfredo Pareto in the late 19th century (Pareto, 1935). Pareto efficiency, or Pareto optimality, refers to a situation where no objective can be improved without worsening another. In the context of multi-objective optimization, Pareto-optimal solutions represent a set of decisions where the trade-offs between competing objectives are balanced in the most efficient way. This foundational concept forms the basis of the modern approach to multi-objective optimization, where the goal is to generate a set of Pareto-optimal solutions for decision-makers to choose from, based on their specific priorities (Gunantara, 2018). While traditional MOO method, including Linear Programming (LP) (√ñzceylan and Paksoy, 2013) and Evolutionary Algorithms (EAs) (Liao et al., 2011), are well-established, the increasing complexity of real-world process systems requires dynamic and adaptable solutions. This has led to the emergence of Dynamic Multi-Objective Optimization (DMOO), which seeks to optimize systems that evolve over time in response to changing environmental, operational, and economic conditions. DMOO methods adapt to evolving conditions, re-optimizing solutions as objectives shift, which is particularly important for process systems engineering where decisions must be continuously adapted to accommodate variations in process dynamics, disturbances, and uncertain operating conditions which require fast and flexible decision-making. In such cases, the Pareto set must be recomputed as the environment changes. However, in fast-paced environments, the computational overhead involved in repeatedly solving DMOO problems can hinder fast, adaptive, real-time decision-making. This motivates the use of multi-objective reinforcement learning (MORL), which builds on RL principles to address multi-objective decision-making. Recent advances in MORL have demonstrated its ability to solve multi-objective decision-making problems in a series of applications in process systems engineering domains (Li et al., 2022), inventory management (Qiu et al., 2024), robotics control (Xu et al., 2020) and energy management (Wu et al., 2023). 2 MORSE: Multi-Objective RL for Supply Chain Optimization 1.1 Related Work Traditional Multi-Objective Optimization (MOO) approaches focus on finding a set of optimal solutions, known as the Pareto optimal, that balance trade-offs between different objectives such as cost, service level and environmental emissions. The collection of all Pareto optimal solutions forms the Pareto Front, which provides decision-makers with a range of non-dominated solutions, allowing them to choose the most appropriate solution based on their system requirements and preference. Methods such as Linear Programming (LP) (√ñzceylan and Paksoy, 2013), Goal Programming (Dutta and Kumar, 2015), and Evolutionary Algorithms (EAs) (Liao et al., 2011), including Non-dominated Sorting Genetic Algorithm II (NSGA-II) (Hnaien et al., 2010), have been widely used to approximate the Pareto front effectively. These approaches focus on two key criteria: (1) proximity - ensuring solutions closely approximate the true Pareto front, and (2) diversity - ensuring solutions span a wide range of trade-offs across the objective space. MOO has been widely applied in inventory management, integrating numerous objectives such as cost efficiency, service levels, and sustainable practices (Aslam and Amos, 2010). For example, Agrawal et al. (2020) explored a deterministic multi-objective optimization process that targeted multi-item inventory management, emphasizing the balance between reducing shortages and controlling costs through multi-criteria decision-making techniques. Recent literature has also explored the integration of sustainability principles into inven"}
{"text": "Title: Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing Authors: Zheqi Lv, Wenqiao Zhang, Kairui Fu, Qi Tian, Shengyu Zhang, Jiajie Su, Jingyuan Chen, Kun Kuang, Fei Wu Categories: cs.LG, cs.CV, cs.DC, cs.IR Published: 2025-09-08 11:06:50+00:00 Abstract: The on-device real-time data distribution shift on devices challenges the generalization of lightweight on-device models. This critical issue is often overlooked in current research, which predominantly relies on data-intensive and computationally expensive fine-tuning approaches. To tackle this, we introduce Persona, a novel personalized method using a prototype-based, backpropagation-free parameter editing framework to enhance model generalization without post-deployment retraining. Persona employs a neural adapter in the cloud to generate a parameter editing matrix based on real-time device data. This matrix adeptly adapts on-device models to the prevailing data distributions, efficiently clustering them into prototype models. The prototypes are dynamically refined via the parameter editing matrix, facilitating efficient evolution. Furthermore, the integration of cross-layer knowledge transfer ensures consistent and context-aware multi-layer parameter changes and prototype assignment. Extensive experiments on vision task and recommendation task on multiple datasets confirm Persona's effectiveness and generality. Paper excerpt: Introduction Deep neural networks often learn from vast global data collected from devices to create cloud-based models [10, 13, 32, 39, 44]. This cloud-centric approach could introduce latencies between on-device data/request generation and the delivery of prediction results, lead- ing to missed opportunities for device participation [64]. To mitigate this issue, it is common to deploy static, cloud-pretrained models directly on devices, as illustrated in Figure 1(a)[17, 24]. Neverthe- less, these static models often struggle to adapt to dynamically changing local environments, such as altering perspectives in au- tonomous vehicles or evolving user preferences in recommender systems. This inflexibility potentially undermines the efficacy of real-time decision-making and degrades user experiences[64] [2]. Consequently, there is an increasing necessity for investigating real-time generalization, i.e., models which can dynamically gen- eralize to reflect ongoing changes and address on-device real-time data distribution shift. A straightforward solution is to drive on-device generalization in- volves instant fine-tuning, as illustrated in Figure 1(b). Recognizing the challenge of sparse labeled data on many devices, which might precipitate overfitting when directly applying fine-tuning, recent studies have explored methods to synthesize or extract distribution- specific data samples from heterogenous sources, such as cloud storage or other devices [4, 23, 29, 61, 66]. Despite these advance- ments, a significant challenge remains: these methods typically arXiv:2509.06552v1 [cs.LG] 8 Sep 2025 MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland Zheqi Lv et al. Static Model Output Fine-tuned Model Device Data ‚Ä¶ ‚Ä¶ ùëí! \"! ùëí! \"\" Real-time Data ùë°! ùë°\" (a) On-device Static Model Static Model Dynamic Model Data Embedding Parameter Editing Device Data ‚Ä¶ ‚Ä¶ ùëí# $! ùëí# $\" Real-time Data Parameter Editing Matrix Prototype Model Primary Adaptive Model Parameter Editor (c) Device-cloud Prototype-based Parameter Editing Framework Effectiveness Efficiency Add Latency NDCG@5 Device Data Global Data (b) On-device Retraining-based Methods Train Deploy Static Model Device (d) Comparison Global Data Train Deploy Cloud Distribution Shift Input !ùë¶ Input Output !ùë¶ Retrain Well Generalized Poor Generalized Device Data Parameter Editor Parameter Editing Input Output !ùë¶ Prototype Model Deploy Device Primary Adaptive Model (Shared Layers) Same Model ùë°# ùë°$ ùë°% ‚Ä¶ ‚Ä¶ Real-time Multimodal Sequence Data ùë°# ùë°$ ùë°% ‚Ä¶ Image Tabular On-device Real-time Data Legend Figure 1: (a) describes the on-device static model. (b) describes traditional paradigm which can be used to solve the on-device real- time data distribution shift. (c) illustrates our Persona. In (a), (b), and (c), the term \"Device Data\" refers to real-time sequential data from multiple modalities collected on the device across various scenarios, such as image modality sequences (e.g., facial expression recognition) and tabular modality sequences (e.g., user behavior modeling). (d) is the comparison of baselines and Persona (Time Consumption: 1s (Persona) ‚â™10s (Fine-tuning)), NDCG@5: 0.3022 (Persona) > 0.2482 (Fine-tuning)). demand extensive computational resources, and could hardly meet the requirements of real-time generation, i.e., to swiftly adjust to new distributions and deliver prompt responses. In this paper, our goal is to continuously provide real-time, gen- eralized deep learning models, balancing adaptability and computa- tional efficiency. To this end, as shown in the Figure 1(c), we propose a Prototype-based Parameter Editing Framework (abbr. Persona), to address the aforementioned on-device real-time data distribution shift. At its core, Persona incorporates a Parameter Editor, which ingeniously maps real-time device data into Parameter Editing Ma- trix. The matrix transforms a shared, unified deep learning model into a adaptive model to better serve for the current on-device data distribution. Remarkably, this process necessitates once inference of Parameter Editors, fueling efficient and responsive generalization. Despite the global optimality of the universally learned deep learning model, it may not uniformly cater to the disparate needs of individual devices. Our empirical study, as elucidated in Section 4.3, lends credence to our proposition: the Parameter Editor‚Äôs ef- ficacy is proportional to the proximity of generalization needs to the shared deep learning model. In this regard, Persona adopts the prototype-editor paradigm. The initial Parameter Editing Matrix facilitate the clustering of devices and the construction of multiple shared Prototype Models, each corresponding to a uniquely crafted editor. These editors perform the twofold task of generalizing device models through the application of the generated Parameter Editing Matrix, and enhancing the dynamic assignment of device models to Prototype Models. The efficacy of our framework hinges on the delicate equilibri"}
