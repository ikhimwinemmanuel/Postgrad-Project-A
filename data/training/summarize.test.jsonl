{"prompt": "Title: Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing\n\nIntroduction Deep neural networks often learn from vast global data collected from devices to create cloud-based models [10, 13, 32, 39, 44]. This cloud-centric approach could introduce latencies between on-device data/request generation and the delivery of prediction results, lead- ing to missed opportunities for device participation [64]. To mitigate this issue, it is common to deploy static, cloud-pretrained models directly on devices, as illustrated in Figure 1(a)[17, 24]. Neverthe- less, these static models often struggle to adapt to dynamically changing local environments, such as altering perspectives in au- tonomous vehicles or evolving user preferences in recommender systems. This inflexibility potentially undermines the efficacy of real-time decision-making and degrades user experiences[64] [2]. Consequently, there is an increasing necessity for investigating real-time generalization, i.e., models which can dynamically gen- eralize to reflect ongoing changes and address on-device real-time data distribution shift. A straightforward solution is to drive on-device generalization in- volves instant fine-tuning, as illustrated in Figure 1(b). Recognizing the challenge of sparse labeled data on many devices, which might precipitate overfitting when directly applying fine-tuning, recent studies have explored methods to synthesize or extract distribution- specific data samples from heterogenous sources, such as cloud storage or other devices [4, 23, 29, 61, 66]. Despite these advance- ments, a significant challenge remains: these methods typically arXiv:2509.06552v1 [cs.LG] 8 Sep 2025 MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland Zheqi Lv et al. Static Model Output Fine-tuned Model Device Data ‚Ä¶ ‚Ä¶ ùëí! \"! ùëí! \"\" Real-time Data ùë°! ùë°\" (a) On-device Static Model Static Model Dynamic Model Data Embedding Parameter Editing Device Data ‚Ä¶ ‚Ä¶ ùëí# $! ùëí# $\" Real-time Data Parameter Editing Matrix Prototype Model Primary Adaptive Model Parameter Editor (c) Device-cloud Prototype-based Parameter Editing Framework Effectiveness Efficiency Add Latency NDCG@5 Device Data Global Data (b) On-device Retraining-based Methods Train Deploy Static Model Device (d) Comparison Global Data Train Deploy Cloud Distribution Shift Input !ùë¶ Input Output !ùë¶ Retrain Well Generalized Poor Generalized Device Data Parameter Editor Parameter Editing Input Output !ùë¶ Prototype Model Deploy Device Primary Adaptive Model (Shared Layers) Same Model ùë°# ùë°$ ùë°% ‚Ä¶ ‚Ä¶ Real-time Multimodal Sequence Data ùë°# ùë°$ ùë°% ‚Ä¶ Image Tabular On-device Real-time Data Legend Figure 1: (a) describes the on-device static model. (b) describes traditional paradigm which can be used to solve the on-device real- time data distribution shift. (c) illustrates our Persona. In (a), (b), and (c), the term \"Device Data\" refers to real-time sequential data from multiple modalities collected on the device across various scenarios, such as image modality sequences (e.g., facial expression recognition) and tabular modality sequences (e.g., user behavior modeling). (d) is the comparison of baselines and Persona (Time Consumption: 1s (Persona) ‚â™10s (Fine-tuning)), NDCG@5: 0.3022 (Persona) > 0.2482 (Fine-tuning)). demand extensive computational resources, and could hardly meet the requirements of real-time generation, i.e., to swiftly adjust to new distributions and deliver prompt responses. In this paper, our goal is to continuously provide real-time, gen- eralized deep learning models, balancing adaptability and computa- tional efficiency. To this end, as shown in the Figure 1(c), we propose a Prototype-based Parameter Editing Framework (abbr. Persona), to address the aforementioned on-device real-time data distribution shift. At its core, Persona incorporates a Parameter Editor, which ingeniously maps real-time device data into Parameter Editing Ma- trix. The matrix transforms a shared, unified deep learning model into a adaptive model to better serve for the current on-device data distribution. Remarkably, this process necessitates once inference of Parameter Editors, fueling efficient and responsive generalization. Despite the global optimality of the universally learned deep learning model, it may not uniformly cater to the disparate needs of individual devices. Our empirical study, as elucidated in Section 4.3, lends credence to our proposition: the Parameter Editor‚Äôs ef- ficacy is proportional to the proximity of generalization needs to the shared deep learning model. In this regard, Persona adopts the prototype-editor paradigm. The initial Parameter Editing Matrix facilitate the clustering of devices and the construction of multiple shared Prototype Models, each corresponding to a uniquely crafted editor. These editors perform the twofold task of generalizing device models through the application of the generated Parameter Editing Matrix, and enhancing the dynamic assignment of device models to Prototype Models. The efficacy of our framework hinges on the delicate equilibri", "response": "The on-device real-time data distribution shift on devices challenges the generalization of lightweight on-device models. This critical issue is often overlooked in current research, which predominantly relies on data-intensive and computationally expensive fine-tuning approaches. To tackle this, we introduce Persona, a novel personalized method using a prototype-based, backpropagation-free parameter editing framework to enhance model generalization without post-deployment retraining. Persona employs a neural adapter in the cloud to generate a parameter editing matrix based on real-time device data. This matrix adeptly adapts on-device models to the prevailing data distributions, efficiently clustering them into prototype models. The prototypes are dynamically refined via the parameter editing matrix, facilitating efficient evolution. Furthermore, the integration of cross-layer knowledge transfer ensures consistent and context-aware multi-layer parameter changes and prototype assignment. Extensive experiments on vision task and recommendation task on multiple datasets confirm Persona's effectiveness and generality."}
{"prompt": "Title: Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice\n\nIntroduction Rheumatoid arthritis (RA) is a chronic autoimmune disease characterized by inflammation that predominantly targets small joints but can advance to larger joints and other organs, leading to significant complications like joint deformity, bone erosion, and weakening of tendons and ligaments [1]. In the United States, RA affects roughly 1.5 million individuals, equivalent to 0.6% of the adult population, while globally, the prevalence approximates 1% [2]. These statistics emphasize the substantial impact of RA on both national and international levels, highlighting the significance of developing innovative methodologies for its detection and evaluation. The complications of RA encompass not only its direct impact on the confines of joints but also its potential to extend its effects to larger joints, such as the knees and hips, which can become involved as the disease progresses. Furthermore, systemic embodiments may appear, leading to complications affecting organs like the heart, lungs, and blood vessels. Thus, the consequences of RA span a diverse range of clinical presentations, warranting a comprehensive approach to its diagnosis and management. Recent advances in cellular profiling techniques like single-cell transcriptomics and spatial transcriptomics have led to the discovery of novel pathogenic cell types in RA joint tissues, highlighting the significant heterogeneity in cellular composition among RA patients. These discoveries provide exciting opportunities for precision medicine approaches to RA treatment[3]. Precision medicine in RA involves using genomic profiling, biomarker testing, and imaging techniques to customize treatment plans for each patient[4]. Joint damage is a hallmark of RA that can lead to deformities and loss of function, and radiographic imaging is an essential tool for disease assessment[5]. The total Sharp/van der Heijde score (TSS) is a widely used clinical scoring method that can objectively quantify joint damage and aid in tracking disease progression [6]. This method evaluates erosion and joint space narrowing (JSN) in 16 joints of each hand and six joints of each foot. Erosion and JSN are rated on a scale from 0 to 5 and 0 to 4, respectively. Although the TSS is a valuable tool in precision medicine approaches to RA treatment scoring systems, it is time-consuming and subject to inter- and intra-reader variability, potentially limiting its clinical effectiveness [7, 8]. Computerized analysis, like deep convolutional neural networks (CNNs), has gradually become prevalent for RA [9] leading to increased interest in their application for automated TSS and the launch of the RA2- DREAM challenge [10]. Although there is growing research interest in automated joint identification and TSS assessment [11, 12], it has not yet been widely adopted in clinical settings due to challenges in achieving reproducible and generalized workflows and results across different radiographic images. These challenges arise from variations in image quality, differences in imaging equipment and protocols at various medical centers, and changes in imaging methods, such as including both hands in a single frame or using bilateral views of hands. Additionally, technical issues like improper angling of the image receiver or axis further complicate the process. Identifying joints in X-ray images with deformative hands and severe erosion, where joints may disappear, results in a variable number of joints for each patient, making it challenging for computerized object recognition models, like the difficulties faced by human observers. Consequently, these cases are often excluded from previous research, limiting the generalizability and reproducibility of these models for clinical use. To address these issues, we developed a standardized ARTSS framework that considers the most plausible variables affecting the replicability of our findings. We trained and tested our models on publicly available data and employed custom data augmentation techniques to balance the dataset, particularly for cases with TSS > 100. Additionally, we devised a novel approach to handle cases with varying numbers of joints, especially in patients with severe joint erosion, ensuring the inclusion of relevant patient data often excluded in prior studies. Finally, we established a reliable pipeline to enhance the generalizability of our findings. The study utilized advanced deep learning architectures, specifically U-net and YOLOv7[13], for hand segmentation and joint identification. We also employed several well-established pre-trained models, including VGG16 [14], VGG19, EfficientNet [15], ResNet [16], and DenseNet [17], as the backbone for predicting radiographic scores from patient data, leveraging their extensive knowledge acquired through large datasets and complex pre-training objectives. Moreover, we developed a custom Vision Transformer (ViT) [18] model for the", "response": "Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming and subjective. This study introduces an Automated Radiographic Sharp Scoring (ARTSS) framework that leverages deep learning to analyze full-hand X-ray images, aiming to reduce inter- and intra-observer variability. The research uniquely accommodates patients with joint disappearance and variable-length image sequences. We developed ARTSS using data from 970 patients, structured into four stages: I) Image pre-processing and re-orientation using ResNet50, II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201, EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS from two radiologists was used as the ground truth. Model training employed 3-fold cross-validation, with each fold consisting of 452 training and 227 validation samples, and external testing included 291 unseen subjects. Our joint identification model achieved 99% accuracy. The best-performing model, ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results demonstrate the potential of deep learning to automate RA scoring, which can significantly enhance clinical practice. Our approach addresses the challenge of joint disappearance and variable joint numbers, offers timesaving benefits, reduces inter- and intra-reader variability, improves radiologist accuracy, and aids rheumatologists in making more informed decisions."}
{"prompt": "Title: mmBERT: A Modern Multilingual Encoder with Annealed Language Learning\n\nIntroduction Encoder-only language models (LMs) were developed during the the early years of scaling language model pre-training, including models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). These models were followed by multilingual variants such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (also known as XLM-R) (Conneau et al., 2019). However, these models cannot generate text, and thus have fallen out of popularity in favor of larger decoder-only language models (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023; Dubey et al., 2024). Despite this, encoder-only models still are frequently used for natural language understanding (NLU) tasks, including classification, clustering, and retrieval. For many years, these older models were still the best encoder-only models available. However, there has been a recent revival of encoder-only model pretraining (Nussbaum et al., 2024; Portes et al., 2023; Warner et al., 2024; Boizard et al., 2025), bringing modern pre-training techniques for decoder-only language models to encoder-only models. There have also been new analyses showing that encoder-only models are significantly better for classification/retrieval than decoder-only models for a given size, even beating decoders an order-of-magnitude larger (Weller et al., 2025; Gisserot-Boukhlef et al., 2025). In this encoder-only model revival there has been a conspicuous lack of large-scale multilinguality. Although it is over six years old, XLM-R is still SOTA ‚Äì especially surprising when you consider the fast-paced nature of the LM field. Thus, we aim to provide a more recent improved version. * Authors contributed equally Models, data, and code are available at https://github.com/jhu-clsp/mmBERT arXiv:2509.06888v1 [cs.CL] 8 Sep 2025 We do this by pre-training our new model suite, MMBERT, on 3T tokens of multilingual text using an architecture inspired from ModernBERT (Warner et al., 2024). We also propose novel contributions to the pre-training recipe: (1) an inverse mask schedule learning rate (high ‚áílow), (2) an annealing language schedule (more biased ‚áímore uniform), and (3) increasing the number of languages at each training phase (60‚Üí110‚Üí1833), allowing for maximal impact of the smaller amount of data. MMBERT improves over XLM-R across the board, and even beats models like OpenAI‚Äôs o3 (OpenAI, 2025) and Google‚Äôs Gemini 2.5 Pro (Comanici et al., 2025) on low-resource languages. We show that including the low-resource languages in the decay phase enables rapid learning, boosting performance on these languages roughly 2x despite only using 100B tokens. Overall, MMBERT is the first model to show significant improvements over XLM-R for massively multilingual data while also introducing new techniques for multilingual LM pre-training that apply to both encoders and decoders. 2 Related Work Encoder-only Models Encoder-only models were the predominant language model in the early LM days, with ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019) being early examples of scaling language models up to the trillion token data range. Encoder-only models are still the predominant models used for classification and retrieval tasks when inference speed is important. However, decoder-only models have been scaled to the trillion parameter scale whereas encoder models typically remain less than 1 billion parameters. The revival of encoder-only LM development was spurred by works such as MosiacBERT showing a BERT-equivalent model could be trained in under 24 hours (Portes et al., 2023; Nussbaum et al., 2024). More recently, ModernBERT (Warner et al., 2024) further scaled these recipes and showed that you could greatly improve performance. Since then there have been several more: EuroBERT focusing on 15 languages (Boizard et al., 2025), NeoBERT (Le Breton et al., 2025) on English, and Ettin showing paired open-data encoder and decoder recipes (Weller et al., 2025). However, none of these more recent models have scaled to more than 15 languages. In the massively multilingual setting, there are very few available models: mBERT with 104 languages (Devlin et al., 2019), XLM-R with 100 languages (Conneau et al., 2019), and mGTE with 74 languages (Zhang et al., 2024). Works such as multilingual DistilBERT (Sanh et al., 2019) and multilingual MiniLM (Wang et al., 2020) have distilled from mBERT and XLM-R respectively to derive smaller variants. The more recent mGTE showed slightly improved performance over XLM-R while allowing for longer contexts, while mBERT is generally not used due to the improvements of XLM-R. Despite XLM-R‚Äôs release date, it has aged very well: its design was well ahead of its era through its use of 6T training tokens, more than any other encoder-only model has ever been trained on, even to this day (including our MMBERT). However, data quality has significantly increased since 2019, allowing us to achieve higher scores with only half of the toke", "response": "Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages."}
{"prompt": "Title: Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder\n\nIntroduction Cardiotocography (CTG) is the primary tool for fetal monitoring, recording fetal heart rate (FHR) and uterine activity (UA) to support antepartum assessment and guide intrapartum care. Clinicians use CTGs to detect fetal distress and trigger interventions aimed at preventing adverse pregnancy outcomes such as neonatal acidosis or hypoxia [1, 2]. However, CTG inter- pretation has remained highly subjective, with considerable inter- and intra-observer variability [3, 4]. Even experienced obstetricians frequently disagree on CTG trace classification, and the low specificity of CTG interpretation can lead to unnecessary interventions (including caesarean deliveries) without a corresponding reduction in poor neonatal outcomes [1]. These limitations have long motivated efforts to develop automated, objective approaches to CTG interpretation. Computerised techniques have been developed to support CTG analysis, aiming to im- prove detection and prediction of fetal compromise. Traditional approaches have used hand- engineered features from the FHR and UA signals such as baseline, variability, accelerations and 1 arXiv:2509.06540v1 [cs.LG] 8 Sep 2025 decelerations. These reflect clinical parameters defined in guidelines like FIGO [5] and NICE [6]. Classical machine learning models classify fetal status from these pre-processed features, often using curated datasets with expert-defined labels (e.g., umbilical artery pH or Apgar score) [7, 8, 9]. While effective in controlled settings, they depend on manual feature selection and show inconsistent performance across datasets. More recently, deep learning methods have been applied to raw CTG signals, with convolutional and recurrent neural networks achieving state-of-the-art results [10]. Their clinical adoption is hindered by poor interpretability, limited data and variable generalisability. The ‚Äúblack box‚Äù nature of these models raises concerns about trust and explainability, highlighting the need for methods that combine predictive power with interpretability. Deep generative models, such as supervised variational autoencoders (VAEs), offer a potential solution by enabling outcome prediction alongside structured representation of learned features. VAEs have been successful at distinguishing CTG segments labeled as ‚Äúsuspicious‚Äù, ‚Äúpatho- logical‚Äù or ‚Äúnormal‚Äù by majority voting of a panel of three expert clinicians, achieving an area under the receiver operating characteristic curve of 0.94‚Äì0.96 when distinguishing ‚Äúnormal‚Äù from ‚Äúpathological‚Äù segments [11, 12]. Similar models have also been effectively applied to model single- and multi-beat ECG signals, enabling the generation of realistic signals via la- tent variables that capture meaningful physiological features [13, 14, 15, 16]. Motivated by the potential of these models, this study explores a supervised VAE for predicting fetal outcomes from FHR segments. The approach combines the feature-learning capability of deep neural networks with the structured, interpretable latent space of probabilistic generative models. In the absence of expert-labeled segments, FHR data from the extensive OxMat CTG dataset are labeled according to pregnancy outcomes [17]. Latent space traversals, partial least squares regression and analysis of coefficients of determination are then applied to perform a novel investigation in to the interpretability of the model. 2 Method 2.1 Data Selection and Splitting The OxMat dataset comprises 208,115 antepartum cardiotocography (CTG) records collected from 1990 until 2024. It includes extensive demographic and medical data for both mothers and babies at the antepartum, intrapartum and postpartum stages. For this study, two subsets were derived: a normal pregnancy outcome (NPO) group and an adverse pregnancy outcome (APO) group, based on stringent inclusion criteria, as detailed in Figure 1 and validated in a previous study [7]. All eligible CTG segments meeting inclusion criteria were used and no formal sample size calculation was performed, as the study leveraged the complete available dataset. Only CTGs recorded between 27 and 36 weeks gestation are included within the two datasets (to avoid late-pregnancy influences) and babies in the APO group were all born preterm. CTGs were split in to five-minute segments, overlapping by two and a half minutes with one another. The number of CTGs and FHR segments extracted are detailed in Figure 2. 2.2 Data Pre-processing FHR segments with more than 25% missing data were excluded. Remaining missing values were assigned a placeholder value, while segments between 3.75 and 5 minutes in duration were padded at the end using a separate padding value. Legacy recordings using the older ‚Äúepoch‚Äù format (3.75 s per sample) were converted to 4 Hz via up-sampling and signal smoothing. A basic noise reduction algorithm was applied to FHR signals to suppress unnatural outlier values, spikes, maternal heart rate contamination and other common CTG artifac", "response": "Objective: To develop and interpret a supervised variational autoencoder (VAE) model for classifying cardiotocography (CTG) signals based on pregnancy outcomes, addressing interpretability limits of current deep learning approaches. Methods: The OxMat CTG dataset was used to train a VAE on five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes. The model was optimised for signal reconstruction and outcome prediction, incorporating Kullback-Leibler divergence and total correlation (TC) constraints to structure the latent space. Performance was evaluated using area under the receiver operating characteristic curve (AUROC) and mean squared error (MSE). Interpretability was assessed using coefficient of determination, latent traversals and unsupervised component analyses. Results: The model achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level, where predicted scores were aggregated. Relaxing TC constraints improved both reconstruction and classification. Latent analysis showed that baseline-related features (e.g., FHR baseline, baseline shift) were well represented and aligned with model scores, while metrics like short- and long-term variability were less strongly encoded. Traversals revealed clear signal changes for baseline features, while other properties were entangled or subtle. Unsupervised decompositions corroborated these patterns. Findings: This work demonstrates that supervised VAEs can achieve competitive fetal outcome prediction while partially encoding clinically meaningful CTG features. The irregular, multi-timescale nature of FHR signals poses challenges for disentangling physiological components, distinguishing CTG from more periodic signals such as ECG. Although full interpretability was not achieved, the model supports clinically useful outcome prediction and provides a basis for future interpretable, generative models."}
{"prompt": "Title: Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor\n\n1. Introduction The importance of effective bioprocess control cannot be overstated. Un- like conventional chemical processes, where the reactor is the primary unit of control, bioprocesses rely on living cells as the actual manufacturing units. These cells are complex, autonomous systems with internal regulatory mech- anisms and are heterogeneously distributed within the bioreactor. This intro- duces significant challenges, as the micro-scale dynamics of individual cells cannot be directly manipulated through macro-scale control variables (Luo et al., 2021). Maintaining stable environmental conditions‚Äîsuch as nutrient levels, pH, temperature, and dissolved oxygen‚Äîis essential to support cel- lular proliferation and productivity, yet these variables are in constant flux requiring the implementation of advanced control systems (Liu et al., 2024). Microalgae-based systems exemplify these challenges. Microalgae are photosynthetic microorganisms capable of thriving under adverse conditions by converting solar energy and carbon-based compounds such as CO2 into biomass, while releasing oxygen (Tarafdar et al., 2023). Their growth de- pends on the availability of essential nutrients like carbon, nitrogen, and phosphorus. CO2 is typically supplied via injection, serving both as a carbon source and a pH buffer‚Äîmaking pH control one of the most critical system variables. Nitrogen and phosphorus can be supplied directly or derived from the medium, particularly when wastewater is used. These biological and op- erational complexities underscore the need for advanced, automated control strategies to ensure process stability and regulatory compliance (Luo et al., 2021; Wang et al., 2022; Guzm√°n et al., 2025). Focusing on pH, this variable stands out as one of the most critical, as it directly influences the solubility and availability of both CO2 and nutri- 2 ents, significantly affecting the metabolism of microalgae (Juneja et al., 2013; Nordio et al., 2023). Photosynthesis itself induces constant pH fluctuations, further complicating its control. Typically, on/off control systems are em- ployed (Rodr√≠guez-Torres et al., 2021), which fail to capture the system‚Äôs dynamic behavior and external disturbances. Additionally, simple Propor- tional, Integral, and Derivative (PID) controllers with fixed parameters have been proposed as well (Fern√°ndez et al., 2010; Isiramen et al., 2022), yet their performance is often insufficient due to the nonlinearities, disturbances, and time-varying dynamics inherent to the process. The reliance on such control strategies largely stems from the challenges associated with developing ac- curate models that fully represent the complex process dynamics (Guzm√°n et al., 2021; Guzm√°n et al., 2025). Given the modeling challenges inherent in these types of processes, robust and adaptive control techniques have arisen in the literature. Several studies have addressed this issue from different perspectives. For instance, a ro- bust adaptative feedforward tracking control was presented by Schaum et al. (2017). Feudjio Letchindjio et al. (2021) explored the use of Extremum Seek- ing Control (ESC) to drive the productivity of a continuous photobiorreactor (PBR) to optimal or suboptimal setpoints. Amaro et al. (2023) introduced an adaptative fuzzy strategy for the nominal model of a generalized Model Predictive Control (MPC) in charge of regulating the pH. In the work by Caparroz et al. (2024), an adaptive model based on a regression tree was presented, capable of predicting pH under varying operational conditions. This tree was subsequently used for the development and implementation of an adaptive control strategy based on a PID controller (H√§gglund and Guzm√°n, 2024). In another study, Caparroz et al. (2023) explored the use of a Model Reference Adaptive Control (MRAC) strategy for pH regulation. This strategy was later hybridized with a PID controller (Caparroz et al., 2025a). Moreover, in Caparroz et al. (2025b), a new relay-based autotun- ing technique based on PID controllers that applies the relay signal to the setpoint was recently designed and experimentally validated for pH control in semi-industrial open PBR. However, such adaptive strategies still face challenges related to the need for nominal models and their limited general- ization capability in the face of highly nonlinear and time-varying dynamics, as typically observed in PBR operating over extended periods. In light of the limitations of adaptive approaches, data-driven learning techniques have emerged as a more flexible alternative. One of the first efforts in this direction was presented by Pataro et al. (2023), who developed 3 an MPC strategy combined with an oracle function that learns online from data to adjust the uncertainties of the nominal model used internally by the MPC. This strategy yielded good results in terms of adaptability and control across dynamic variations induced by different culture media. Nonetheless, despite i", "response": "The inherent complexity of living cells as production units creates major challenges for maintaining stable and optimal bioprocess conditions, especially in open Photobioreactors (PBRs) exposed to fluctuating environments. To address this, we propose a Reinforcement Learning (RL) control approach, combined with Behavior Cloning (BC), for pH regulation in open PBR systems. This represents, to the best of our knowledge, the first application of an RL-based control strategy to such a nonlinear and disturbance-prone bioprocess. Our method begins with an offline training stage in which the RL agent learns from trajectories generated by a nominal Proportional-Integral-Derivative (PID) controller, without direct interaction with the real system. This is followed by a daily online fine-tuning phase, enabling adaptation to evolving process dynamics and stronger rejection of fast, transient disturbances. This hybrid offline-online strategy allows deployment of an adaptive control policy capable of handling the inherent nonlinearities and external perturbations in open PBRs. Simulation studies highlight the advantages of our method: the Integral of Absolute Error (IAE) was reduced by 8% compared to PID control and by 5% relative to standard off-policy RL. Moreover, control effort decreased substantially-by 54% compared to PID and 7% compared to standard RL-an important factor for minimizing operational costs. Finally, an 8-day experimental validation under varying environmental conditions confirmed the robustness and reliability of the proposed approach. Overall, this work demonstrates the potential of RL-based methods for bioprocess control and paves the way for their broader application to other nonlinear, disturbance-prone systems."}
{"prompt": "Title: Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs\n\nI. INTRODUCTION M ACHINE Learning (ML) models have become the leading approach in Network Intrusion Detection Sys- tems (NIDS). These models have achieved State-of-the- art (SOTA) performance [1] and, unlike traditional methods, do not require experts to painstakingly identify patterns, known as signatures, or develop classification rules for known intrusions. However, ML models traditionally require large labelled datasets, containing many examples of each class the system aims to classify, to be effective. Unfortunately, in network intrusion detection acquiring such datasets is often non- trivial, necessitating that experts identify and label numerous instances of malicious network flows. To exacerbate this issue, traffic taken from other networks, either real or simulated, does not generalise well to new networks [2]. This leaves ML-based NIDS non-implementable in newly established networks where there are no labelled instances of malicious traffic. Furthermore, fully trained ML classifiers struggle to detect novel ‚Äúzero-day‚Äù intrusions for which there are no labelled instances. One solution to this problem is to exploit the abundance of benign network flows through the application of anomaly detec- tion algorithms, such as Support Vector Machines (SVMs) [3] and autoencoders [4]. These algorithms learn the distribution of benign network traffic during training and flag non-conforming traffic as malicious during inference. While widely researched, the lack of malicious samples during training results in traditional anomaly detectors exhibiting false positive rates that are too high for practical deployment. Recently, Self-Supervised Learning (SSL) has emerged as a promising approach for NIDS, enabling models to learn meaningful latent representations from unlabelled data [5]‚Äì[9]. This label independence allows it to be applied to NIDS to learn meaningful representations of network traffic from only benign flows [10]. Contrastive learning is one such method, where models are trained by minimising a distance metric between similar (positive) pairs of samples while simultane- ously maximising it between dissimilar (negative) pairs. This is typically achieved by generating augmented views of the same sample, which are treated as positive pairs, resulting in a latent representation which is robust to the chosen perturbations. Several works have successfully leveraged contrastive SSL to improve intrusion detection performance [6]‚Äì[8], however, existing SSL approaches learn a distinct distribution for each sample and its augmented views in latent space. This limits their ability to model benign traffic wholistically and introduces challenges in distinguishing between benign and malicious traffic effectively. This work proposes Contrastive Learning using Augmented Negative Pairs (CLAN), which presents a change in paradigm: instead of treating augmented samples as positive pairs, they are treated as negative. It is shown that this change results in the model learning a fundamentally different latent representation of the data: while existing approaches learn a distinct latent distribution for each sample; CLAN instead learns a single distribution of benign traffic. Not only does this allow for more efficient inference, but by learning the distribution of benign traffic wholistically CLAN achieves improved performance when both deployed as an anomaly detector or fine-tuned on a limited dataset to perform multi-class classification. The core contributions of this work can be summarised as follows: ¬© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI: https://doi.org/10.1109/CSR64739.2025.11129979 Link to IEEE Xplore: https://ieeexplore.ieee.org/document/11129979 arXiv:2509.06550v1 [cs.LG] 8 Sep 2025 1) A contrastive SSL framework is proposed for NIDS. In contrast to traditional approaches, where samples and their augmented versions are treated as positive pairs, this work instead aims to model the class level distribution of benign traffic by viewing augmented samples as belonging to another, potentially malicious, distribution. Thus, they are treated as negative pairs. 2) The framework is extended to perform binary clas- sification without fine-tuning allowing it to function as an anomaly detector. Experimental results show the proposed approach outperforms existing anomaly detection and SSL algorithms. 3) It is shown that the priors learned by pretraining can be exploited for supervised classification, allowing the proposed model to be performative when fine-tuned on a limited quantity of labelled samples. It is experimentally shown to outperform existing SSL approaches in this setti", "response": "Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models."}
{"prompt": "Title: Concolic Testing on Individual Fairness of Neural Network Models\n\nIntroduction Deep Neural Networks (DNNs) are increasingly deployed across diverse applica- tions, from autonomous vehicles to medical diagnostics. While these models often achieve remarkable performance, their deployment in high-stakes scenarios raises sig- nificant concerns about trustworthiness and fairness. In critical domains such as criminal justice, employment, and financial services, the algorithmic fairness of DNNs has come under intense scrutiny [1, 2, 3]. Notable examples include racial bias observed in the COMPAS recidivism prediction model [4] and gender bias evident in Amazon‚Äôs recruiting model [5]. This heightened awareness underscores the critical importance of addressing fairness issues of neural networks. Various forms of discrimination have been acknowledged, including group discrim- ination [6] and individual discrimination [7]. Discrimination is typically delineated with respect to a set of Protected Attributes (PAs), such as race and gender, in contrast to the Non-Protected Attributes (NPAs). Group fairness [6] advocates for impartial treatment among protected groups (i.e., the sub-population defined by a protected attribute) to elimi- nate group discrimination. While group fairness is relatively easy to measure and has clear policy implications, it can sometimes mask individual-level disparities or lead to reverse discrimination. In contrast, individual fairness [7] dictates that similar individuals should be treated similarly regardless of their membership in protected groups. In the context of ‚àóCorresponding author ‚Ä† Chih-Duo Hong is supported by the NSTC, Taiwan, under grant number 112-2222-E004-001-MY3. Fang Yu is supported by the NSTC under grant numbers 113-2221-E-004-010-MY2 and 113-2634-F-004-001-MBK. 1 arXiv:2509.06864v1 [cs.LG] 8 Sep 2025 2 Ming-I Huang, Chih-Duo Hong, and Fang Yu machine learning, this means that two inputs differing solely in their PAs should lead to identical model outcomes. Extensive efforts have been directed towards enhancing indi- vidual fairness at the model level. One prevalent approach is fairness testing, which aims to generate efficient test suites before deployment [8, 9, 10, 11, 12]. These discrimination tests can be employed to quantify discrimination or can be utilized for model retraining to alleviate unfairness. Despite these efforts, verifying fairness properties in complex DNNs remains challenging due to their non-linear nature. This paper presents PyFair, a novel framework for evaluating individual fairness of DNNs using the concolic testing tool PyCT [13, 14]. Our research addresses the critical gap in formal fairness guarantees for real-world DNN applications by extending automatic testing techniques to fairness verification. Given a DNN model, PyFair employs concolic execution to generate fairness-specific path constraints for the model. By systematically exploring these constraints, test inputs that are most relevant to fairness assessment can be identified. Although PyCT may be used alongside random sampling to evaluate potential discrimination in a DNN, the absence of detected discrimination generally does not guarantee overall model fairness. PyFair strengthens the capabilities of PyCT, enabling a more thorough and systematic exploration of the model‚Äôs behavior beyond sampling. Thanks to our employment of SMT (Satisfiability Modulo Theories) solvers [15], this exploration is complete when the neural network can be faithfully encoded in an SMT theory.1 We assess the efficacy of PyFair by evaluating network models studied in the literature [16, 10, 11]. Our experiments demonstrate that PyFair can effectively identify discrimina- tory instances in most models, often outperforming existing constraint-based testing tools like Fairify [16]. We also test models improved by bias mitigation techniques such as ADF [10] and EIDIG [11], showing that PyFair can still detect unfairness in these ‚Äúfairer‚Äù models. Furthermore, we evaluate PyFair‚Äôs capability to verify fairness in artificially constructed fair models, revealing both its potential and limitations in handling complex architectures. These comprehensive experiments showcase PyFair‚Äôs effectiveness in both discriminatory instance detection and fairness verification. Overall, this work advances the state-of-the-art in neural network fairness verification through a rigorous, systematic approach to identifying discriminatory instances. While our approach delivers stronger guarantees than random sampling or gradient-based testing methods [8, 9, 10, 11, 12], it requires greater computational resources and faces scalability challenges when dealing with complex network architectures. This tradeoff between rigorous verification and computational efficiency represents an important consideration for practitioners choosing between different fairness testing approaches based on their specific requirements for completeness versus scalability. 2. Related Work Fairness testing. Recent research has foc", "response": "This paper introduces PyFair, a formal framework for evaluating and verifying individual fairness of Deep Neural Networks (DNNs). By adapting the concolic testing tool PyCT, we generate fairness-specific path constraints to systematically explore DNN behaviors. Our key innovation is a dual network architecture that enables comprehensive fairness assessments and provides completeness guarantees for certain network types. We evaluate PyFair on 25 benchmark models, including those enhanced by existing bias mitigation techniques. Results demonstrate PyFair's efficacy in detecting discriminatory instances and verifying fairness, while also revealing scalability challenges for complex models. This work advances algorithmic fairness in critical domains by offering a rigorous, systematic method for fairness testing and verification of pre-trained DNNs."}
{"prompt": "Title: DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT\n\nIntroduction The rise of large models and agents is driving the Internet of Things (IoT) towards a new era of advanced autonomous in- telligence, which in turn places unprecedented demands on the quality of its underlying data (Guo et al. 2024; Aouedi et al. 2024). In the smart home scenario, this deep integra- tion of Artificial Intelligence (AI) and IoT is transforming living spaces into vast and complex sensing networks (Huda et al. 2024). The high-dimensional, multivariate time-series data streams continuously generated by these networks form the very backbone of advanced automated services, such as intelligent energy management (Nikpour et al. 2025), proac- tive security (Rehman et al. 2024), and personalized scene recommendations (Xiao et al. 2023). However, the performance and reliability of all such in- telligent services hinge entirely upon the credibility of the data that drives them. Consequently, the issue of data cred- ibility‚Äîthat is, whether the data accurately reflects the true Copyright ¬© 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Kitchen (door open) Nk Nl Nk Nl Nk Nl Nk Nl Kitchen Kitchen Kitchen Kitchen Living room Living room Living room Living room Static Graph Dynamic Graph Kitchen (door close) Light t Associated Causal Temp Spatial level Temporal level Figure 1: Illustration of the core challenges. state of the physical world‚Äîhas become a core bottleneck impeding the advancement of intelligent systems toward greater autonomy and reliability. While Spatio-Temporal Graph Neural Networks (STGNNs) have achieved tremen- dous success in modeling such dependencies, particularly in structured data scenarios like traffic flow forecasting (Ju et al. 2024; Kong, Guo, and Liu 2024), their direct appli- cation to the smart home context reveals two fundamental limitations stemming from the unique dynamic and event- driven nature of the environment: 1. Static Assumption of Spatial Dependencies: The ma- jority of existing methods rely on a pre-defined, static graph topology to capture spatial correlations (Tang et al. 2023; Pazho et al. 2023). In a smart home, however, the physi- cal relationships between sensors are dynamic. For instance, opening or closing a window fundamentally alters the corre- lation strength between indoor and outdoor temperature sen- sors. Even models that employ adaptive adjacency matrices (Chen et al. 2023; Sun et al. 2025) often learn graph struc- tures that evolve too slowly to capture the abrupt topologi- cal changes triggered by discrete events. This assumption of a static or slowly-changing graph fundamentally limits the model‚Äôs capacity to represent real-world physical dynamics. 2. Causal Confusion in Temporal Dependencies: In the complex environment of a smart home, a single human ac- tivity can trigger simultaneous changes across multiple sen- sors, creating inter-dependencies between variables in mul- tivariate time series. While existing deep learning models are good at capturing these co-occurrence patterns, they of- ten struggle to distinguish true causal chains from spurious arXiv:2509.06483v1 [cs.LG] 8 Sep 2025 correlations (Gong et al. 2024). This ‚Äúcorrelation-causation confusion‚Äù is a critical flaw; a model that learns a spurious link between, for instance, a coffee machine and a toaster (which are often used together in the morning) may fail when only one is used. This disregard for the underlying causal mechanisms compromises the model‚Äôs robustness, making it liable to misinterpret valid but less common se- quences of events as anomalies. Recent efforts have begun to tackle these issues from separate angles. To capture dynamic spatial dependencies, existing research (Geng et al. 2024; Liu and Zhang 2024) have moved beyond static graphs by learning topologies that evolve based on data correlations. On the temporal front, an- other line of work (Fu, Pan, and Zhang 2024; Gong et al. 2024) has focused on explicit causal discovery, attempting to first learn a causal graph from data and then use it as a prior to guide the model‚Äôs predictions. However, the dy- namic graph models learn abstract correlations that are fre- quently ungrounded from the discrete, physical events that truly govern the environment. Meanwhile, approaches based on explicit causal discovery often rely on strong statisti- cal assumptions and decouple the causal learning from the end-to-end representation learning process. A critical gap thus persists: the lack of a unified framework that marries physically-grounded, event-driven dynamic graphs with ro- bust causal reasoning embedded within the architecture. To address these challenges and bridge this gap, this pa- per proposes a novel framework named the Dynamic Causal Spatio-Temporal Graph Network (DyC-STG), designed as an end-to-end solution with deep reasoning capabilities for assessing data credibility in smart homes. Specifically, we design an event-driven dynamic graph constru", "response": "The wide spreading of Internet of Things (IoT) sensors generates vast spatio-temporal data streams, but ensuring data credibility is a critical yet unsolved challenge for applications like smart homes. While spatio-temporal graph (STG) models are a leading paradigm for such data, they often fall short in dynamic, human-centric environments due to two fundamental limitations: (1) their reliance on static graph topologies, which fail to capture physical, event-driven dynamics, and (2) their tendency to confuse spurious correlations with true causality, undermining robustness in human-centric environments. To address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network (DyC-STG), a novel framework designed for real-time data credibility analysis in IoT. Our framework features two synergistic contributions: an event-driven dynamic graph module that adapts the graph topology in real-time to reflect physical state changes, and a causal reasoning module to distill causally-aware representations by strictly enforcing temporal precedence. To facilitate the research in this domain we release two new real-world datasets. Comprehensive experiments show that DyC-STG establishes a new state-of-the-art, outperforming the strongest baselines by 1.4 percentage points and achieving an F1-Score of up to 0.930."}
{"prompt": "Title: On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''\n\nIntroduction Fairness in machine learning is increasing in importance as the applications of machine learning keep growing. One of the applications of machine learning is in the medical domain (Garg & Mago, 2021), where preventing biases is especially important, as this directly impacts patients health. Vision-language (VL) models are a type of model increasingly being used in the medical domain (Zhao et al., 2024). However, they have also been found to contain biases (Fraser & Kiritchenko, 2024; Lee et al., 2023; Luo et al., 2024), which introduces the need for the development of unbiased and explainable models. Several methods have been proposed to reduce biases of machine learning models (Caton & Haas, 2024; Seth et al., 2023), such as adversarial learning (Berg et al., 2022; Beutel et al., 2017) and modified loss functions (Quadrianto & Sharmanska, 2017; Kamishima et al., 2012; Prost et al., 2019). The latter approach is an in-processing method and can be used while fine-tuning pre-trained VL models for downstream tasks. For example, Quadrianto & Sharmanska (2017) use the maximum mean discrepancy (MMD; Gretton et al., 2012) as a regularization term for privileged learning to encourage the distributions of groups to be close. ‚àóEqual contribution. 1 arXiv:2509.06535v1 [cs.CV] 8 Sep 2025 To mitigate biases in a post-hoc manner, Luo et al. (2024) proposed FairCLIP, which uses a regularization function to improve group fairness for CLIP (Radford et al., 2021), and study the fairness of the model on an extra data set. In this work, we first provide a detailed outline of the specific claims made by Luo et al. (2024) and reproduce the experiments presented in their original paper. We then propose FairCLIP+, a generalized FairCLIP objective designed to improve group fairness for multiple sensitive attributes at the same time. Additionally, a more detailed analysis of FairCLIP was performed to study the relation between the regularization objective and the performance of the model. Following this, we extend the evaluation of the FairCLIP method by applying it to a new data set, providing insight into the generalizability. 2 Scope of reproducibility In order to address biases in CLIP-based models, Luo et al. (2024) proposed FairCLIP, a method that uses an optimal transport-based regularization function to improve the fairness of CLIP, particularly in the medical domain. This approach aims to minimize the distance between the population distribution and the group distribution of a sensitive attribute. The focus of our work is on reproducing the following claims made by Luo et al. (2024): 1. CLIP shows significant biases towards 1) Asian, 2) male, 3) non-Hispanic and 4) Spanish-speaking individuals on the Harvard-FairVLMed dataset, but finetuning on the data can alleviate the biases, improving fairness and performance. 2. Fine-tuning CLIP using the FairCLIP objective on the Harvard-FairVLMed data set improves both performance and fairness of zero-shot glaucoma classification across various subgroups in the Harvard-FairVLMed data set. 3 Methodology 3.1 Model descriptions CLIP CLIP (Radford et al., 2021) is a contrastive vision-language model consisting of a language model and a vision transformer, and can be used to compute a similarity score between an image and a text. The similarity scores can be utilized in downstream tasks, such as captioning or classification. Experiments were carried out on CLIP with the RS50, ViT-B/16 or ViT-L/14 architecture, which have 102M, 150M and 428M parameters, respectively. All CLIP-based models were initialized using the official checkpoints. BLIP-2 BLIP-2 (Li et al., 2023b) is another vision-language model, which combines a frozen vision encoder and a frozen language model using a Querying transformer (Q-Former) model. The Q-former learns the interaction between the vision encoder and the language model. Following Luo et al. (2024), a frozen CLIP vision encoder was used for BLIP-2. FairCLIP The FairCLIP model is a pre-trained CLIP model that is fine-tuned with the standard CLIP loss and a regularizer for group fairness. Group fairness requires that groups are treated equally, which in particular should hold for sensitive attributes. Let A denote the set of sensitive attributes and let A ‚ààA be a sensitive attribute; A = {male, female} for example. Suppose we have a batch {(x(i) I , x(i) T , a(i))}n i=1 containing image features x(i) I , text features x(i) T and a sensitive group label a(i) ‚ààA. Each sensitive group should have the same underlying distribution D(xI,xT ,a|a=Œ±) of similarity scores {‚ü®x(i) I , x(i) T ‚ü©}n i=1, where ‚ü®¬∑, ¬∑‚ü© is the Euclidean inner product. That is, the distribution of diagonal elements of IT ‚ä§for an image feature matrix I and text feature matrix T . Computing the actual distributions is intractable; the distributions are thus estimated using a batch DB and a batch DBa given a sensitive group a. The closeness of probability distributions can be measured us", "response": "We investigated the reproducibility of FairCLIP, proposed by Luo et al. (2024), for improving the group fairness of CLIP (Radford et al., 2021) by minimizing image-text similarity score disparities across sensitive groups using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was reproduced to primarily investigate the research findings for FairCLIP. The model description by Luo et al. (2024) was found to differ from the original implementation. Therefore, a new implementation, A-FairCLIP, is introduced to examine specific design choices. Furthermore, FairCLIP+ is proposed to extend the FairCLIP objective to include multiple attributes. Additionally, the impact of the distance minimization on FairCLIP's fairness and performance was explored. In alignment with the original authors, CLIP was found to be biased towards certain demographics when applied to zero-shot glaucoma classification using medical scans and clinical notes from the Harvard-FairVLMed dataset. However, the experimental results on two datasets do not support their claim that FairCLIP improves the performance and fairness of CLIP. Although the regularization objective reduces Sinkhorn distances, both the official implementation and the aligned implementation, A-FairCLIP, were not found to improve performance nor fairness in zero-shot glaucoma classification."}
{"prompt": "Title: Reward function compression facilitates goal-dependent reinforcement learning\n\nIntroduction Reinforcement learning is a powerful process by which agents learn to make decisions based on experienced rewards and punishments, which has seen broad applications across neuroscience, psychology, and machine learning [1]. The study of animal reinforcement learning (RL) has predominantly focused on two outcome categories: primary rewards, which satisfy basic physiological needs (e.g., food, water), and secondary rewards, which derive their value through learned associations with primary rewards (tokens, money, numeric points [2]). This focus has advanced our understanding of RL behavior and its neural basis in dopaminergic signaling [3]. However, primary and secondary rewards do not adequately capture the complexity of human reward processing [4], especially in contexts involving abstract goals and deliberate planning [5]. For example, people often find activities rewarding in the absence, if not at the expense, of external incentives [6]. Even when the benefits of engaging in an activity are clear, people can attribute value to its successful completion a priori, without needing to directly associate new outcomes with established rewards. Indeed, humans can flexibly imbue novel outcomes with value and use them to guide learning through a process that recruits subcortical reward circuits [7, 8]. Together, these findings suggest that humans use a third class of rewards: flexible, goal-dependent outcomes that derive value from current objectives rather than learned associations [9]. Goal-dependent reinforcement learning implies the construction of a goal-dependent reward function, i.e., a mapping between an outcome space and scalar rewards. While individuals show remarkable flexibility in creating ad hoc reward functions according to current goals, the underlying process is often inefficient [7]. This is illustrated, for example, by the discrepancy between the high frequency at which people set New Year‚Äôs resolutions and the low rate at which they arXiv:2509.06810v1 [q-bio.NC] 8 Sep 2025 Reward function compression facilitates goal-dependent reinforcement learning PREPRINT, 2025 Agent Working memory Long-term memory Environment Goal-dependent valuation g(t) Task info RL learner Policy r(t) W(t) a(t-1) s(t) o(t) A At first, RL relies on WM for outcome valuation Agent Working memory Long-term memory Environment Goal-dependent valuation g(t) Task info RL learner Policy r(t) W(t) a(t-1) s(t) o(t) Reward function B Eventually, RL exploits a compressed reward function Figure 1: We break down goal-dependent learning into several parts. Combined with other learning components (not shown), an RL learning algorithm informs value estimates W on a given trial t, which are entered into a policy. The agent selects an action a through a joint policy, to which the environment responds by outputting outcome information o and a new state s. A working memory (WM) component organizes and maintains task-relevant information (here, broadly intended and dubbed the ‚Äútask information‚Äù), used to support the reinforcement learning (RL) process. A) Initially, value attribution to o is operated by WM, which maintains the current goal and supports its matching the the experienced outcome to generate a reward r used by the RL agent. WM also holds a trace of recently experienced goals. B) If goals share enough similar features, a compressed reward function can be created, which replaces active goal maintenance and can, over time, be transferred to long-term memory storage. Eventually, the compressed reward function can be directly employed to assign value to experienced outcomes, sparing resources for other WM tasks. complete them [10]. However, the precise mechanisms by which humans align their value system to current goals ‚Äì and which aspects make such alignment hard to maintain ‚Äì remain uncharted. Identifying the key players involved in goal-dependent learning and their respective weaknesses could inform behavioral strategies that help people achieve their objectives in both personal and educational settings. Setting up custom reward functions likely involves the interplay of multiple cognitive systems [7]. The existence of interactions between RL and executive functions is now well-established [11, 12]. In particular, WM can act as a source of information for RL, affording faster or more efficient learning [13, 14]. WM has also been proposed to support the verbalization of critical task components [15, 16] and the representation of states, actions, and rewards over which RL operates [17]. However, much less is known about how exchanges between RL and WM support goal-dependent reward functions, and how they might be limited. Building on existing work [7], we designed a task where participants had to learn which responses led to desired outcomes, but using abstract images labeled as ‚Äúgoal‚Äù/‚Äúnongoal‚Äù as feedback instead of standard numeric points (Figure 2). We replicated our finding that people can indeed imbue abstract, n", "response": "Reinforcement learning agents learn from rewards, but humans can uniquely assign value to novel, abstract outcomes in a goal-dependent manner. However, this flexibility is cognitively costly, making learning less efficient. Here, we propose that goal-dependent learning is initially supported by a capacity-limited working memory system. With consistent experience, learners create a \"compressed\" reward function (a simplified rule defining the goal) which is then transferred to long-term memory and applied automatically upon receiving feedback. This process frees up working memory resources, boosting learning efficiency. We test this theory across six experiments. Consistent with our predictions, our findings demonstrate that learning is parametrically impaired by the size of the goal space, but improves when the goal space structure allows for compression. We also find faster reward processing to correlate with better learning performance, supporting the idea that as goal valuation becomes more automatic, more resources are available for learning. We leverage computational modeling to support this interpretation. Our work suggests that efficient goal-directed learning relies on compressing complex goal information into a stable reward function, shedding light on the cognitive mechanisms of human motivation. These findings generate new insights into the neuroscience of intrinsic motivation and could help improve behavioral techniques that support people in achieving their goals."}
{"prompt": "Title: Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning\n\nI. INTRODUCTION A S digital platforms like Netflix, Disney+, and YouTube expand [1], ensuring that younger audiences access only age-appropriate content has become increasingly critical [2]. The MPAA rating system [3], with classifications such as G, PG, PG-13, and R, helps families make informed viewing Dipta Neogi, Nourash Azmine Chowdhury, Muhammad Rafsan Kabir, and Mohammad Ashrafuzzaman Khan are with the Department of Elec- trical and Computer Engineering at North South University, Dhaka, 1229, Bangladesh. Corresponding author: Muhammad Rafsan Kabir (E-mail: muhammad.kabir@northsouth.edu) choices [4], protecting young viewers from exposure to inap- propriate content [5]. However, the sheer volume of daily up- loads [6] makes manual rating methods impractical. Streaming platforms require automated classification systems to ensure compliance with MPAA guidelines and global regulations while maintaining consistency and accuracy. These systems help prevent regulatory issues and public backlash, ultimately enhancing viewer safety and experience. Recent advancements in machine learning and deep learning have enabled automated systems to predict MPAA ratings with remarkable accuracy. Masha et al. [7] proposed multimodal approaches integrating textual data from movie trailers, while Ha et al. [8] combined film scripts with static visual features, highlighting the benefits of diverse data modalities. However, these studies primarily focus on textual and static visual data, overlooking the complexity of dynamic video representations, which inherently capture temporal and contextual cues. Uzza- man et al. [9] and Uddin et al. [10] developed ConvLSTM and LRCN-based models for human activity recognition, achieving high accuracy on UCF50 and HMDB51 datasets. Despite progress in video classification, these approaches neglect fac- tors like environmental noise and video quality, which impact real-world performance. This underscores the need for more efficient, scalable solutions adaptable to diverse and dynamic environments. To address the aforementioned gaps, we first curated a custom video dataset featuring 323 diverse samples across four MPAA rating classes (G, PG, PG-13, and R). We trained and evaluated existing video classification models, such as ResNet3D-50 [11] and LRCN (LSTM + CNN) [12] in contrastive learning framework, on the curated dataset for classifying MPAA ratings. Then, for further robust video clas- sification, we incorporated an additional attention mechanism to the existing LRCN backbone, introducing a novel hybrid backbone in contrastive learning for video classification. This architecture is designed to capture both the visual details (spatial features) and the flow of the video over time (temporal features). The attention mechanism helps the model focus on the most important frames in a sequence, enabling it to detect subtle differences, such as those distinguishing a PG- 13 video from an R-rated one. In our study, we employ three distinct attention mechanisms: self-attention [13], co-attention [14], and Bahdanau attention [15]. This framework allows the model to learn robust, context-aware, and finely tuned video representations. To ensure accurate differentiation between MPAA rating classes, various methods are applied during arXiv:2509.06826v1 [cs.CV] 8 Sep 2025 2 the training process to minimize confusion, particularly for videos that may fall on the boundary between two categories. The model is designed to be both efficient and lightweight, making it highly suitable for use on memory-constrained devices without compromising performance. Moreover, we developed a web application to demonstrate the real-world usability of our proposed model architecture for MPAA rating classification from videos. Contributions: The key contributions are as follows: ‚Ä¢ We curated a custom video dataset from diverse inter- net sources, comprising 323 video samples across four MPAA rating classes (G, PG, PG-13, R), with durations ranging from 11 to 25 seconds in various formats to ensure heterogeneity and robustness. ‚Ä¢ A novel hybrid model architecture is proposed within a contrastive learning framework that incorporates an LRCN backbone, consisting of CNN and LSTM, with an attention mechanism to improve adaptability, tempo- ral coherence, and representation granularity for video classification. ‚Ä¢ Advanced attention mechanisms‚Äîself-attention, co- attention, and Bahdanau attention‚Äîare integrated to se- lectively emphasize contextually significant features, en- abling interpretable and context-aware embeddings for managing complex cross-frame dependencies. ‚Ä¢ Several contrastive loss functions, including NT-logistic, NT-Xent, and Margin Triplet, are employed to optimize embedding quality during pre-training and fine-tuning, with gradient clipping ensuring stability and convergence during training. ‚Ä¢ We evaluated the performance of our proposed model architecture along with existing architectures within the contras", "response": "The rapid growth of visual content consumption across platforms necessitates automated video classification for age-suitability standards like the MPAA rating system (G, PG, PG-13, R). Traditional methods struggle with large labeled data requirements, poor generalization, and inefficient feature learning. To address these challenges, we employ contrastive learning for improved discrimination and adaptability, exploring three frameworks: Instance Discrimination, Contextual Contrastive Learning, and Multi-View Contrastive Learning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a Bahdanau attention mechanism, achieving state-of-the-art performance in the Contextual Contrastive Learning framework, with 88% accuracy and an F1 score of 0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling, and attention mechanisms for dynamic frame prioritization, the model excels in fine-grained borderline distinctions, such as differentiating PG-13 and R-rated content. We evaluate the model's performance across various contrastive loss functions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating the robustness of our proposed architecture. To ensure practical application, the model is deployed as a web application for real-time MPAA rating classification, offering an efficient solution for automated content compliance across streaming platforms."}
